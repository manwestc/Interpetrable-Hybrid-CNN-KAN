{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d5668d4-717f-4e0a-a900-f1b70dc8ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the root directory\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Add the root directory to the Python path\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dace8b6-da36-4ed7-94f9-e7bfde57e588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from TINTOlib.tinto import TINTO\n",
    "from kan import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import traceback\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "import traceback\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "#from torch.optim import LBFGS\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787a631d-2f1e-4b05-845e-bb878447f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 381\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b71d2a5-d2ad-4b72-9869-8fdae3a98141",
   "metadata": {},
   "source": [
    "# BEST: ACC = 0.998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1580c162-4015-4cda-8492-b51350055b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=\"data/wall-robot-navigation\"\n",
    "x_col=[\"V1\", \"V2\", \"V3\", \"V4\", \"V5\", \"V6\", \"V7\", \"V8\",\"V9\",\"V10\",\"V11\", \"V12\", \"V13\", \n",
    "       \"V14\", \"V15\", \"V16\", \"V17\", \"V18\", \"V19\",\"V20\",\"V21\", \"V22\", \"V23\", \"V24\"]\n",
    "target_col=[\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b03fb57-d803-49b8-8e18-3e3fb5bfe3d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5054e7cf-2a69-4b0a-97a5-02e10264038e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load Dataset and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0d46e15-1287-4f1e-ba6a-cc05b3c8915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean(npy_filename, y_filename, x_col, target_col):\n",
    "    \"\"\"\n",
    "    Load the feature array (npy_filename) and label array (y_filename),\n",
    "    drop rows in the feature array that contain any NaNs, and apply\n",
    "    the same mask to the label array.\n",
    "    \"\"\"\n",
    "    # Load numpy arrays\n",
    "    X = np.load(os.path.join(folder, npy_filename))\n",
    "    y = np.load(os.path.join(folder, y_filename))\n",
    "    \n",
    "    # Ensure the number of rows matches between X and y\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"The number of rows in {} and {} do not match.\".format(npy_filename, y_filename))\n",
    "    \n",
    "    # Create a boolean mask for rows that do NOT have any NaN values in X\n",
    "    valid_rows = ~np.isnan(X).any(axis=1)\n",
    "\n",
    "    # Filter both arrays using the valid_rows mask\n",
    "    X_clean = X[valid_rows]\n",
    "    y_clean = y[valid_rows]\n",
    "    \n",
    "    # Convert arrays to DataFrames\n",
    "    df_X = pd.DataFrame(X_clean)\n",
    "    df_y = pd.DataFrame(y_clean)\n",
    "    df_X.columns = x_col\n",
    "    df_y.columns = target_col\n",
    "    return df_X, df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6443278-0647-432b-86d7-6883cf1aea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(X_train, y_train, X_test, y_test, X_val, y_val, image_model, problem_type, batch_size=32):\n",
    "    # Add target column to input for IGTD\n",
    "    X_train_full = X_train.copy()\n",
    "    X_train_full[\"target\"] = y_train.values\n",
    "\n",
    "    X_val_full = X_val.copy()\n",
    "    X_val_full[\"target\"] = y_val.values\n",
    "\n",
    "    X_test_full = X_test.copy()\n",
    "    X_test_full[\"target\"] = y_test.values\n",
    "\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(f'{images_folder}/train'):\n",
    "        #print(X_train_full)\n",
    "        image_model.fit_transform(X_train_full, f'{images_folder}/train')\n",
    "        image_model.saveHyperparameters(f'{images_folder}/model.pkl')\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    # Load image paths\n",
    "    imgs_train = pd.read_csv(os.path.join(f'{images_folder}/train', f'{problem_type}.csv'))\n",
    "    imgs_train[\"images\"] = images_folder + \"/train/\" + imgs_train[\"images\"]\n",
    "\n",
    "    if not os.path.exists(f'{images_folder}/val'):\n",
    "        image_model.transform(X_val_full, f'{images_folder}/val')\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    imgs_val = pd.read_csv(os.path.join(f'{images_folder}/val', f'{problem_type}.csv'))\n",
    "    imgs_val[\"images\"] = images_folder + \"/val/\" + imgs_val[\"images\"]\n",
    "\n",
    "    if not os.path.exists(f'{images_folder}/test'):\n",
    "        image_model.transform(X_test_full, f'{images_folder}/test')\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    imgs_test = pd.read_csv(os.path.join(f'{images_folder}/test', f'{problem_type}.csv'))\n",
    "    imgs_test[\"images\"] = images_folder + \"/test/\" + imgs_test[\"images\"]\n",
    "\n",
    "    # Image data\n",
    "    X_train_img = np.array([cv2.imread(img) for img in imgs_train[\"images\"]])\n",
    "    X_val_img = np.array([cv2.imread(img) for img in imgs_val[\"images\"]])\n",
    "    X_test_img = np.array([cv2.imread(img) for img in imgs_test[\"images\"]])\n",
    "\n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale numerical data\n",
    "    X_train_num = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_val_num = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n",
    "    X_test_num = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "    attributes = len(X_train_num.columns)\n",
    "    height, width, channels = X_train_img[0].shape\n",
    "    imgs_shape = (channels, height, width)\n",
    "\n",
    "    print(\"Images shape: \", imgs_shape)\n",
    "    print(\"Attributes: \", attributes)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_num_tensor = torch.as_tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor = torch.as_tensor(X_val_num.values, dtype=torch.float32)\n",
    "    X_test_num_tensor = torch.as_tensor(X_test_num.values, dtype=torch.float32)\n",
    "    X_train_img_tensor = torch.as_tensor(X_train_img, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n",
    "    X_val_img_tensor = torch.as_tensor(X_val_img, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n",
    "    X_test_img_tensor = torch.as_tensor(X_test_img, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n",
    "    y_train_tensor = torch.as_tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_val_tensor = torch.as_tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_test_tensor = torch.as_tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_num_tensor, X_train_img_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_num_tensor, X_val_img_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_num_tensor, X_test_img_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes, imgs_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c2d115-8c10-4670-afb9-88009569c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_coordinate_and_xcol(coordinate, x_col):\n",
    "    \"\"\"\n",
    "    Given a coordinate (tuple of row, col arrays) and x_col feature list,\n",
    "    return completed coordinate and x_col including empty positions.\n",
    "\n",
    "    Empty positions are filled with labels: 'Ex1', 'Ex2', ...\n",
    "    \"\"\"\n",
    "\n",
    "    row_coords, col_coords = coordinate\n",
    "    max_row = row_coords.max()\n",
    "    max_col = col_coords.max()\n",
    "    max_c = max(max_row, max_col)\n",
    "    # All possible coordinate slots\n",
    "    full_coords = set((r, c) for r in range(max_col + 1) for c in range(max_col + 1))\n",
    "    current_coords = set(zip(row_coords, col_coords))\n",
    "    missing_coords = sorted(full_coords - current_coords)\n",
    "\n",
    "    # Create updated coordinate arrays\n",
    "    new_row_coords = list(row_coords)\n",
    "    new_col_coords = list(col_coords)\n",
    "    new_x_col = list(x_col)\n",
    "\n",
    "    for idx, (r, c) in enumerate(missing_coords):\n",
    "        new_row_coords.append(r)\n",
    "        new_col_coords.append(c)\n",
    "        new_x_col.append(f\"Ex{idx+1}\")\n",
    "\n",
    "    completed_coordinate = (np.array(new_row_coords), np.array(new_col_coords))\n",
    "    return completed_coordinate, new_x_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c7ac21-b8f8-4de9-b756-9da684567bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_mapping(x_col, coordinate, scale=(4,4)):\n",
    "    grid = np.full(scale, \"\", dtype=object)\n",
    "    rows, cols = coordinate\n",
    "    for i, (r, c) in enumerate(zip(rows, cols)):\n",
    "        if i < len(x_col):\n",
    "            grid[r, c] = x_col[i]\n",
    "        else:\n",
    "            grid[r, c] = \"?\"\n",
    "    \n",
    "    plt.figure(figsize=(scale[1] * 2, scale[0] * 2))\n",
    "    for i in range(scale[0]):\n",
    "        for j in range(scale[1]):\n",
    "            plt.text(j, i, grid[i, j], ha='center', va='center', fontsize=10,\n",
    "                     bbox=dict(facecolor='white', edgecolor='gray'))\n",
    "    \n",
    "    plt.xticks(np.arange(scale[1]))\n",
    "    plt.yticks(np.arange(scale[0]))\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Feature → Pixel Mapping\")\n",
    "    plt.gca().invert_yaxis()  # So row 0 is at the top\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6224cc89-a931-413b-a3f1-3bccd0d3e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_loader(loader):\n",
    "    \"\"\"\n",
    "    Combines all batches from a DataLoader into three tensors.\n",
    "    Assumes each batch is a tuple: (mlp_tensor, img_tensor, target_tensor)\n",
    "    \"\"\"\n",
    "    mlp_list, img_list, target_list = [], [], []\n",
    "    for mlp, img, target in loader:\n",
    "        mlp_list.append(mlp)\n",
    "        img_list.append(img)\n",
    "        target_list.append(target)\n",
    "    return torch.cat(mlp_list, dim=0), torch.cat(img_list, dim=0), torch.cat(target_list, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ab0b6-acee-45de-ad6d-7cb6982f79c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Functions for KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef657551-05a2-4430-8bbd-70d4bf24b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.get_default_dtype()\n",
    "min_expected = 0.\n",
    "max_expected = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "496af9fd-74ab-4fd0-9b77-944902aa3a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sorted_feature_importance(columns, importances):\n",
    "    # Move to CPU and numpy if it's a GPU tensor\n",
    "    if isinstance(importances, torch.Tensor):\n",
    "        importances = importances.detach().cpu().numpy()\n",
    "\n",
    "    # Pair columns and importances and sort by importance descending\n",
    "    sorted_pairs = sorted(zip(columns, importances), key=lambda x: x[1], reverse=True)\n",
    "    print(sorted_pairs)\n",
    "    sorted_columns, sorted_importances = zip(*sorted_pairs)\n",
    "    \n",
    "    # Create the bar plot\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.barh(sorted_columns, sorted_importances, color='royalblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('KAN Feature Importances')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return sorted_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2bcf343-299e-4421-a4b0-221973e3cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_ACC(y_true, y_pred, train_loss, val_loss, plot=False):\n",
    "    clipped = torch.clamp(torch.round(y_pred), min=min_expected, max=max_expected)\n",
    "    avg_rmse = torch.mean((clipped == y_true).type(dtype))\n",
    "    print(avg_rmse)\n",
    "    if plot:\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.plot(train_loss)\n",
    "        plt.plot(val_loss)\n",
    "        plt.legend(['train', 'val'])\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('step')\n",
    "        plt.yscale('log')\n",
    "    return avg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66843ae8-e405-45ef-9be9-d061c3c337cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true_tensor, y_pred_tensor, title=\"Confusion Matrix\", plot=False):\n",
    "    # Move tensors to CPU and detach if necessary\n",
    "    clipped = torch.clamp(torch.round(y_pred_tensor), min=min_expected, max=max_expected)\n",
    "\n",
    "    y_true = y_true_tensor.detach().cpu().numpy().flatten()\n",
    "    y_pred = clipped.detach().cpu().numpy().flatten()\n",
    "\n",
    "    # Round predictions if they are float (e.g., from sigmoid or regression)\n",
    "    if y_pred.dtype.kind in {'f'}:\n",
    "        y_pred = y_pred.round()\n",
    "\n",
    "    # Get sorted list of all unique labels\n",
    "    all_labels = sorted(set(y_true) | set(y_pred))\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=all_labels)\n",
    "    print(cm)\n",
    "    if plot:\n",
    "        # Plot\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',\n",
    "                    xticklabels=all_labels, yticklabels=all_labels)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3742bc8-6939-4cc9-a7db-1133fec39fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_acc_kan():\n",
    "    rounded = torch.round(torch.round(model(dataset['train_input'])[:,0]))\n",
    "    clipped = torch.clamp(rounded, min=min_expected, max=max_expected)\n",
    "    return torch.mean((clipped == dataset['train_label'][:,0]).type(dtype))\n",
    "\n",
    "def val_acc_kan():\n",
    "    rounded = torch.round(torch.round(model(dataset['val_input'])[:,0]))\n",
    "    clipped = torch.clamp(rounded, min=min_expected, max=max_expected)\n",
    "    return torch.mean((clipped == dataset['val_label'][:,0]).type(dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbc33172-7df2-48b2-bba8-ce9ad6003892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_fit(model, dataset, opt=\"LBFGS\", steps=100, log=1, lamb=0., lamb_l1=1., lamb_entropy=2., lamb_coef=0., lamb_coefdiff=0., update_grid=True, \n",
    "               grid_update_num=10, loss_fn=None, lr=1., start_grid_update_step=-1, stop_grid_update_step=50, batch=-1,\n",
    "               metrics=None, save_fig=False, in_vars=None, out_vars=None, beta=3, save_fig_freq=1, img_folder='./video', \n",
    "               singularity_avoiding=False, y_th=1000., reg_metric='edge_forward_spline_n', display_metrics=None):\n",
    "    '''\n",
    "    training\n",
    "\n",
    "    Args:\n",
    "    -----\n",
    "        dataset : dic\n",
    "            contains dataset['train_input'], dataset['train_label'], dataset['val_input'], dataset['val_label']\n",
    "        opt : str\n",
    "            \"LBFGS\" or \"Adam\"\n",
    "        steps : int\n",
    "            training steps\n",
    "        log : int\n",
    "            logging frequency\n",
    "        lamb : float\n",
    "            overall penalty strength\n",
    "        lamb_l1 : float\n",
    "            l1 penalty strength\n",
    "        lamb_entropy : float\n",
    "            entropy penalty strength\n",
    "        lamb_coef : float\n",
    "            coefficient magnitude penalty strength\n",
    "        lamb_coefdiff : float\n",
    "            difference of nearby coefficits (smoothness) penalty strength\n",
    "        update_grid : bool\n",
    "            If True, update grid regularly before stop_grid_update_step\n",
    "        grid_update_num : int\n",
    "            the number of grid updates before stop_grid_update_step\n",
    "        start_grid_update_step : int\n",
    "            no grid updates before this training step\n",
    "        stop_grid_update_step : int\n",
    "            no grid updates after this training step\n",
    "        loss_fn : function\n",
    "            loss function\n",
    "        lr : float\n",
    "            learning rate\n",
    "        batch : int\n",
    "            batch size, if -1 then full.\n",
    "        save_fig_freq : int\n",
    "            save figure every (save_fig_freq) steps\n",
    "        singularity_avoiding : bool\n",
    "            indicate whether to avoid singularity for the symbolic part\n",
    "        y_th : float\n",
    "            singularity threshold (anything above the threshold is considered singular and is softened in some ways)\n",
    "        reg_metric : str\n",
    "            regularization metric. Choose from {'edge_forward_spline_n', 'edge_forward_spline_u', 'edge_forward_sum', 'edge_backward', 'node_backward'}\n",
    "        metrics : a list of metrics (as functions)\n",
    "            the metrics to be computed in training\n",
    "        display_metrics : a list of functions\n",
    "            the metric to be displayed in tqdm progress bar\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "        results : dic\n",
    "            results['train_loss'], 1D array of training losses (RMSE)\n",
    "            results['val_loss'], 1D array of val losses (RMSE)\n",
    "            results['reg'], 1D array of regularization\n",
    "            other metrics specified in metrics\n",
    "        best_model:\n",
    "    '''\n",
    "\n",
    "    if lamb > 0. and not model.save_act:\n",
    "        print('setting lamb=0. If you want to set lamb > 0, set model.save_act=True')\n",
    "        \n",
    "    old_save_act, old_symbolic_enabled = model.disable_symbolic_in_fit(lamb)\n",
    "\n",
    "    pbar = tqdm(range(steps), desc='description', ncols=100)\n",
    "\n",
    "    if loss_fn == None:\n",
    "        loss_fn = loss_fn_eval = lambda x, y: torch.mean((x - y) ** 2)\n",
    "    else:\n",
    "        loss_fn = loss_fn_eval = loss_fn\n",
    "\n",
    "    grid_update_freq = int(stop_grid_update_step / grid_update_num)\n",
    "\n",
    "    if opt == \"Adam\":\n",
    "        optimizer = torch.optim.Adam(model.get_params(), lr=lr)\n",
    "    elif opt == \"LBFGS\":\n",
    "        optimizer = LBFGS(model.get_params(), lr=lr, history_size=10, \n",
    "                          line_search_fn=\"strong_wolfe\", \n",
    "                          tolerance_grad=1e-32,\n",
    "                          tolerance_change=1e-32,\n",
    "                          tolerance_ys=1e-32)\n",
    "\n",
    "    results = {}\n",
    "    results['train_loss'] = []\n",
    "    results['val_loss'] = []\n",
    "    results['reg'] = []\n",
    "    if metrics != None:\n",
    "        for i in range(len(metrics)):\n",
    "            results[metrics[i].__name__] = []\n",
    "\n",
    "    if batch == -1 or batch > dataset['train_input'].shape[0]:\n",
    "        batch_size = dataset['train_input'].shape[0]\n",
    "        batch_size_val = dataset['val_input'].shape[0]\n",
    "    else:\n",
    "        batch_size = batch\n",
    "        batch_size_val = batch\n",
    "\n",
    "    global train_loss, reg_\n",
    "\n",
    "    def closure():\n",
    "        global train_loss, reg_\n",
    "        optimizer.zero_grad()\n",
    "        pred = model.forward(dataset['train_input'][train_id], singularity_avoiding=singularity_avoiding, y_th=y_th)\n",
    "        train_loss = loss_fn(pred, dataset['train_label'][train_id])\n",
    "        if model.save_act:\n",
    "            if reg_metric == 'edge_backward':\n",
    "                model.attribute()\n",
    "            if reg_metric == 'node_backward':\n",
    "                model.node_attribute()\n",
    "            reg_ = model.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "        else:\n",
    "            reg_ = torch.tensor(0.)\n",
    "        objective = train_loss + lamb * reg_\n",
    "        objective.backward()\n",
    "        return objective\n",
    "\n",
    "    if save_fig:\n",
    "        if not os.path.exists(img_folder):\n",
    "            os.makedirs(img_folder)\n",
    "    \n",
    "    best_model_state = None\n",
    "    best_epoch = -1\n",
    "    best_metric = 0\n",
    "    val_metric = 0\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        if epoch == steps-1 and old_save_act:\n",
    "            model.save_act = True\n",
    "            \n",
    "        if save_fig and epoch % save_fig_freq == 0:\n",
    "            save_act = model.save_act\n",
    "            model.save_act = True\n",
    "        \n",
    "        train_id = np.random.choice(dataset['train_input'].shape[0], batch_size, replace=False)\n",
    "        val_id = np.random.choice(dataset['val_input'].shape[0], batch_size_val, replace=False)\n",
    "\n",
    "        if epoch % grid_update_freq == 0 and epoch < stop_grid_update_step and update_grid and epoch >= start_grid_update_step:\n",
    "            model.update_grid(dataset['train_input'][train_id])\n",
    "\n",
    "        if opt == \"LBFGS\":\n",
    "            optimizer.step(closure)\n",
    "\n",
    "        if opt == \"Adam\":\n",
    "            pred = model.forward(dataset['train_input'][train_id], singularity_avoiding=singularity_avoiding, y_th=y_th)\n",
    "            train_loss = loss_fn(pred, dataset['train_label'][train_id])\n",
    "            if model.save_act:\n",
    "                if reg_metric == 'edge_backward':\n",
    "                    model.attribute()\n",
    "                if reg_metric == 'node_backward':\n",
    "                    model.node_attribute()\n",
    "                reg_ = model.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            else:\n",
    "                reg_ = torch.tensor(0.)\n",
    "            loss = train_loss + lamb * reg_\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = loss_fn_eval(model.forward(dataset['val_input'][val_id]), dataset['val_label'][val_id])\n",
    "        \n",
    "        if metrics != None:\n",
    "            for i in range(len(metrics)):\n",
    "                results[metrics[i].__name__].append(metrics[i]().item())\n",
    "        \n",
    "        results['train_loss'].append(torch.sqrt(train_loss).cpu().detach().numpy())\n",
    "        results['val_loss'].append(torch.sqrt(val_loss).cpu().detach().numpy())\n",
    "        results['reg'].append(reg_.cpu().detach().numpy())\n",
    "\n",
    "        if epoch % log == 0:\n",
    "            if display_metrics == None:\n",
    "                pbar.set_description(\"| train_loss: %.2e | val_loss: %.2e | reg: %.2e | \" % (torch.sqrt(train_loss).cpu().detach().numpy(), torch.sqrt(val_loss).cpu().detach().numpy(), reg_.cpu().detach().numpy()))\n",
    "                val_metric = val_loss\n",
    "            else:\n",
    "                string = ''\n",
    "                data = ()\n",
    "                for metric in display_metrics:\n",
    "                    val_metric = results[metric][-1]\n",
    "                    string += f' {metric}: %.2e |'\n",
    "                    try:\n",
    "                        results[metric]\n",
    "                    except:\n",
    "                        raise Exception(f'{metric} not recognized')\n",
    "                    data += (results[metric][-1],)\n",
    "                pbar.set_description(string % data)\n",
    "\n",
    "        if val_metric > best_metric:\n",
    "            best_epoch = epoch\n",
    "            best_metric = val_metric\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if save_fig and epoch % save_fig_freq == 0:\n",
    "            model.plot(folder=img_folder, in_vars=in_vars, out_vars=out_vars, title=\"Step {}\".format(_), beta=beta)\n",
    "            plt.savefig(img_folder + '/' + str(_) + '.jpg', bbox_inches='tight', dpi=200)\n",
    "            plt.close()\n",
    "            model.save_act = save_act\n",
    "\n",
    "    model.log_history('fit')\n",
    "    # revert back to original state\n",
    "    model.symbolic_enabled = old_symbolic_enabled\n",
    "    print(f\"✅ Best validation Accuracy: {best_metric:.4e} at {best_epoch} epoch\")\n",
    "    return best_model_state, results, best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab5a49-d42a-4b73-8964-b0dce89f4dfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Grad CAM Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5b42ae4-45c3-4679-a157-26cb696dfd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam_side_by_side(model, model_state, num_input, img_input, x_col, coordinate,\n",
    "                          zoom=1, target_index=None, save_path=None, show=True):\n",
    "    model.load_state_dict(model_state)\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "\n",
    "    if num_input.dim() == 1:\n",
    "        num_input = num_input.unsqueeze(0)\n",
    "    if img_input.dim() == 3:\n",
    "        img_input = img_input.unsqueeze(0)\n",
    "\n",
    "    num_input = num_input.to(model.device)\n",
    "    img_input = img_input.to(model.device)\n",
    "\n",
    "    # Store activations and gradients\n",
    "    activations = {}\n",
    "    gradients = {}\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        activations[\"value\"] = output.detach()\n",
    "\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        gradients[\"value\"] = grad_output[0].detach()\n",
    "\n",
    "    conv_layer = model.cnn_branch[0]\n",
    "    h_fwd = conv_layer.register_forward_hook(forward_hook)\n",
    "    h_bwd = conv_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    output = model(num_input, img_input)\n",
    "    target = output if target_index is None else output[:, target_index]\n",
    "    target.backward()\n",
    "\n",
    "    act = activations[\"value\"].squeeze(0)\n",
    "    grad = gradients[\"value\"].squeeze(0)\n",
    "    weights = grad.mean(dim=(1, 2))\n",
    "    cam = torch.zeros_like(act[0])\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * act[i]\n",
    "\n",
    "    cam = torch.relu(cam)\n",
    "    if cam.max() > 0:\n",
    "        cam -= cam.min()\n",
    "        cam /= cam.max()\n",
    "    else:\n",
    "        cam[:] = 0.0\n",
    "\n",
    "\n",
    "    cam_resized = F.interpolate(cam.unsqueeze(0).unsqueeze(0), size=img_input.shape[-2:], mode='bilinear', align_corners=False)\n",
    "    #print(cam_resized)\n",
    "    heatmap = cam_resized.squeeze().cpu()\n",
    "\n",
    "    img_disp = img_input.squeeze().detach().cpu()\n",
    "    if img_disp.shape[0] == 1:\n",
    "        img_disp = img_disp[0]\n",
    "        cmap = 'gray'\n",
    "    else:\n",
    "        img_disp = img_disp.permute(1, 2, 0)\n",
    "        cmap = None\n",
    "\n",
    "    h, w = img_disp.shape[:2]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axs[0].imshow(img_disp, cmap=cmap)\n",
    "    axs[0].set_title(\"Original Image\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    im = axs[1].imshow(img_disp, cmap=cmap)\n",
    "    heatmap_img = axs[1].imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "    axs[1].set_title(\"Grad-CAM\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    # Add colorbar (legend)\n",
    "    cbar = fig.colorbar(heatmap_img, ax=axs[1], fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Grad-CAM Intensity\", fontsize=10)\n",
    "\n",
    "    # Overlay abbreviated features\n",
    "    for i, col in enumerate(x_col):\n",
    "        abbrev = col.split(\"-\")[0][:8]\n",
    "        if i < len(coordinate[0]):\n",
    "            r, c = coordinate[0][i], coordinate[1][i]\n",
    "            ry = r * zoom + zoom // 2\n",
    "            cx = c * zoom + zoom // 2\n",
    "            if ry < h and cx < w:\n",
    "                axs[1].text(cx, ry, abbrev,\n",
    "                            color='white', fontsize=9, ha='center', va='center',\n",
    "                            bbox=dict(facecolor='black', edgecolor='none', pad=1.0, alpha=0.4))\n",
    "                axs[0].text(cx, ry, abbrev,\n",
    "                            color='white', fontsize=9, ha='center', va='center',\n",
    "                            bbox=dict(facecolor='black', edgecolor='none', pad=1.0, alpha=0.4))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "    h_fwd.remove()\n",
    "    h_bwd.remove()\n",
    "\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91437092-eb7c-464b-bed8-d608e690dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_to_feature_relevance(heatmap, coordinate, x_col, zoom=1):\n",
    "    \"\"\"\n",
    "    Map heatmap pixel intensities to their corresponding features using coordinate and zoom.\n",
    "    Returns a dictionary of {feature_name: relevance_score}.\n",
    "    \"\"\"\n",
    "    feature_scores = {}\n",
    "\n",
    "    for i, col in enumerate(x_col):\n",
    "        if i < len(coordinate[0]):\n",
    "            r, c = coordinate[0][i], coordinate[1][i]\n",
    "            ry = r * zoom + zoom // 2\n",
    "            cx = c * zoom + zoom // 2\n",
    "            if ry < heatmap.shape[0] and cx < heatmap.shape[1]:\n",
    "                feature_scores[col] = heatmap[ry, cx].item()\n",
    "\n",
    "    return feature_scores\n",
    "\n",
    "def plot_feature_relevance_bar(feature_scores):\n",
    "    \"\"\"\n",
    "    Plots a horizontal bar chart of feature relevance from Grad-CAM heatmap.\n",
    "    \"\"\"\n",
    "    sorted_scores = sorted(feature_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    print(sorted_scores)\n",
    "    features, scores = zip(*sorted_scores)\n",
    "\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.barh(features, scores, color='royalblue')\n",
    "    plt.xlabel(\"Grad-CAM Relevance\")\n",
    "    plt.title(\"Feature Relevance for Test\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9799c37d-4ad2-4e27-939b-2feffa63c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_feature_relevance_from_val(model, model_state, val_inputs, val_imgs, coordinate, x_col, zoom=1):\n",
    "    \"\"\"\n",
    "    Computes average Grad-CAM feature relevance over all validation instances with a tqdm progress bar.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        model_state: Trained weights to be loaded\n",
    "        val_inputs: List or tensor of numerical inputs\n",
    "        val_imgs: List or tensor of image inputs\n",
    "        coordinate: IGTD-style coordinate tuple (row array, col array)\n",
    "        x_col: List of feature names (including extras)\n",
    "        zoom: Zoom level used when generating the images\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of average feature relevance\n",
    "    \"\"\"\n",
    "    accumulated_scores = {feature: [] for feature in x_col}\n",
    "\n",
    "    for num_input, img_input in tqdm(zip(val_inputs, val_imgs), total=len(val_inputs), desc=\"Computing Grad-CAM\"):\n",
    "        heatmap = grad_cam_side_by_side(\n",
    "            model=model,\n",
    "            model_state=model_state,\n",
    "            num_input=num_input,\n",
    "            img_input=img_input,\n",
    "            coordinate=coordinate,\n",
    "            x_col=x_col,\n",
    "            zoom=zoom,\n",
    "            show=False\n",
    "        )\n",
    "        scores = heatmap_to_feature_relevance(heatmap, coordinate, x_col, zoom)\n",
    "        for feature, value in scores.items():\n",
    "            accumulated_scores[feature].append(value)\n",
    "\n",
    "    # Compute average\n",
    "    avg_scores = {feature: float(np.mean(values)) if values else 0.0\n",
    "                  for feature, values in accumulated_scores.items()}\n",
    "\n",
    "    return avg_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbba9e-3360-4f05-b3c5-1b1cac214ffd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### CNN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66c22e16-f2aa-4f59-8d1d-8af0920d7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cnn_only_model(model, dataset, steps=100, lr=1.0, loss_fn=None, batch=-1, opt=\"LBFGS\"):\n",
    "    \"\"\"\n",
    "    Trains a CNN-only model using LBFGS.\n",
    "\n",
    "    Args:\n",
    "        model: CNN-only PyTorch model.\n",
    "        dataset: Dictionary with keys: 'train_img', 'train_label', 'val_img', 'val_label'.\n",
    "        steps: Number of training iterations.\n",
    "        lr: Learning rate.\n",
    "        loss_fn: Loss function. Defaults to MSE.\n",
    "\n",
    "    Returns:\n",
    "        results: Dict with lists of train/val losses.\n",
    "        best_model_state: Best weights based on val loss.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    if loss_fn is None:\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Optimizer selection\n",
    "    if opt == \"LBFGS\":\n",
    "        optimizer = LBFGS(model.parameters(), lr=lr, history_size=10, \n",
    "                          line_search_fn=\"strong_wolfe\", \n",
    "                          tolerance_grad=1e-32, \n",
    "                          tolerance_change=1e-32, \n",
    "                          tolerance_ys=1e-32)\n",
    "    elif opt == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer '{opt}'\")\n",
    "\n",
    "    n_train = dataset[\"train_img\"].shape[0]\n",
    "    n_val = dataset[\"val_img\"].shape[0]\n",
    "    batch_size = n_train if batch == -1 or batch > n_train else batch\n",
    "\n",
    "    results = {'train_loss': [], 'val_loss': []}\n",
    "    best_model_state = None\n",
    "    best_loss = float(\"inf\")\n",
    "    best_epoch = -1\n",
    "\n",
    "    pbar = tqdm(range(steps), desc=\"Training CNN Only ({opt})\", ncols=100)\n",
    "\n",
    "    for step in pbar:\n",
    "        train_idx = np.random.choice(n_train, batch_size, replace=False)\n",
    "        #train_idx = torch.randperm(n_train)[:min(32, n_train)]  # small batch\n",
    "        x_train = dataset[\"train_img\"][train_idx].to(device)\n",
    "        y_train = dataset[\"train_label\"][train_idx].to(device)\n",
    "        if opt == \"LBFGS\":\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                output = model(0, x_train)\n",
    "                loss = loss_fn(output, y_train)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "            train_loss = closure().item()\n",
    "\n",
    "        else:  # AdamW\n",
    "            optimizer.zero_grad()\n",
    "            output = model(0, x_train)\n",
    "            loss = loss_fn(output, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_output = model(0, dataset[\"val_img\"].to(device))\n",
    "            val_loss = loss_fn(val_output, dataset[\"val_label\"].to(device)).item()\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_epoch = step\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        pbar.set_description(f\"| Train: {train_loss:.4e} | Val: {val_loss:.4e} |\")\n",
    "\n",
    "    print(f\"✅ Best validation loss: {best_loss:.4e} at {best_epoch} epoch\")\n",
    "    return best_model_state, results, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f060c34-6fca-4ab9-9bc0-70e6175b2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_custom_cnn_model(cnn_blocks, dense_layers, imgs_shape, device='cuda'):\n",
    "    class CustomCNNModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CustomCNNModel, self).__init__()\n",
    "            self.device = device\n",
    "\n",
    "            cnn_layers = []\n",
    "            in_channels = imgs_shape[0]\n",
    "            out_channels = 16\n",
    "            cnn_blocks_list = [10, 7, 5]\n",
    "            size_layer_norm = cnn_blocks_list[cnn_blocks-1]\n",
    "            \n",
    "            f_layer_size = 10 - cnn_blocks*2\n",
    "            for i in range(cnn_blocks):\n",
    "                cnn_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=2))\n",
    "                \n",
    "                if i < cnn_blocks - 1:\n",
    "                    cnn_layers.append(nn.BatchNorm2d(out_channels))\n",
    "                    cnn_layers.append(nn.ReLU())\n",
    "                    cnn_layers.append(nn.MaxPool2d(2))\n",
    "                else:\n",
    "                    # Last block: LayerNorm + Sigmoid + Flatten\n",
    "                    cnn_layers.append(nn.LayerNorm([out_channels, size_layer_norm, size_layer_norm]))\n",
    "                    cnn_layers.append(nn.Sigmoid())\n",
    "                    cnn_layers.append(nn.Flatten())\n",
    "                in_channels = out_channels\n",
    "                out_channels *= 2\n",
    "\n",
    "            self.cnn_branch = nn.Sequential(*cnn_layers).to(device)\n",
    "            self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "            # Dense (FC) layers\n",
    "            fc_layers = []\n",
    "            input_dim = self.flat_size\n",
    "            for i in range(dense_layers - 1):\n",
    "                fc_layers.append(nn.Linear(int(input_dim), int(input_dim // 2)))\n",
    "                fc_layers.append(nn.ReLU())\n",
    "                input_dim = input_dim // 2\n",
    "            fc_layers.append(nn.Linear(int(input_dim), 1))\n",
    "\n",
    "            self.fc = nn.Sequential(*fc_layers).to(device)\n",
    "\n",
    "        def _get_flat_size(self, imgs_shape):\n",
    "            dummy_input = torch.zeros(1, *imgs_shape, device=self.device)\n",
    "            x = self.cnn_branch(dummy_input)\n",
    "            return x.shape[1]\n",
    "\n",
    "        def forward(self, num_input, img_input):\n",
    "            img_input = img_input.to(self.device)\n",
    "            features = self.cnn_branch(img_input)\n",
    "            output = self.fc(features)\n",
    "            return output\n",
    "\n",
    "    return CustomCNNModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fddce5-8c8a-4d14-a15b-52c971b9a1f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Write metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de86122f-a857-4140-af9f-f79668f33903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_with_header(filename, columns_opt):\n",
    "    header=['kan_neurons', 'kan_grid', 'lamb', columns_opt, 'ACC','Conf_Mtx','Best_Epoch','KAN_Relevance','CNN_Relevance','KAN M.R.F.','CNN M.R.F.']\n",
    "    \"\"\"Creates a CSV file with a given header.\"\"\"\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7f1da54-a3e2-4dd4-8333-9e3282f68b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_top_3(pairs):\n",
    "    return '\\n'.join(f\"{k}: {v:.2f}\" for k, v in pairs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac2de6d0-68fd-431f-9aba-942d3bd5d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_row_to_csv(filename, kan_neurons, kan_grid, lamb, opt_col_val, acc, cm, best_epoch, k_rel, cnn_rel, kan_mrf, cnn_mrf):\n",
    "    row = [kan_neurons, kan_grid, lamb, opt_col_val, acc, cm, best_epoch, k_rel, cnn_rel, format_top_3(kan_mrf), format_top_3(cnn_mrf)]\n",
    "    ['Configuration', opt_col_val,'ACC','Conf_Mtx','Best_Epoch','KAN_Relevance','CNN_Relevance','KAN M.R.F.','CNN M.R.F.']\n",
    "    \"\"\"Appends a single row to an existing CSV file.\"\"\"\n",
    "    if not os.path.isfile(filename):\n",
    "        raise FileNotFoundError(f\"{filename} does not exist. Please create the file first with a header.\")\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3b94d-b95b-44ad-a4dd-84781fdf0a7a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Hybrid Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e554e5a-af4d-4f84-bc39-a076c11b2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mkan_vs_cnn_relevance(feature_scores, mkan_len):\n",
    "    if isinstance(feature_scores, torch.Tensor):\n",
    "        feature_scores = feature_scores.detach().cpu().numpy()\n",
    "\n",
    "    mkan_relevance = feature_scores[:mkan_len].sum()\n",
    "    cnn_relevance = feature_scores[mkan_len:].sum()\n",
    "    m_kan_relevance_perct = float(mkan_relevance/(mkan_relevance+cnn_relevance))\n",
    "    cnn_relevance_perct = float(cnn_relevance/(mkan_relevance+cnn_relevance))\n",
    "    print(f\"M_KAN Relevance: {m_kan_relevance_perct}\")\n",
    "    print(f\"CNN Relevance: {cnn_relevance_perct}\")\n",
    "    return m_kan_relevance_perct, cnn_relevance_perct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ec3d8d9-59b8-43a4-8726-66e455fcec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mkan_vs_cnn_relevance(feature_scores, mkan_len=6, title=\"Feature Relevance Split\"):\n",
    "    \"\"\"\n",
    "    Plots a pie chart comparing the total feature relevance from m_kan output vs CNN output.\n",
    "\n",
    "    Args:\n",
    "        feature_scores (torch.Tensor or list): 1D tensor of relevance values from final_kan.\n",
    "        mkan_len (int): Number of dimensions from m_kan (default: 6).\n",
    "        title (str): Title for the pie chart.\n",
    "    \"\"\"\n",
    "    if isinstance(feature_scores, torch.Tensor):\n",
    "        feature_scores = feature_scores.detach().cpu().numpy()\n",
    "\n",
    "    mkan_relevance = feature_scores[:mkan_len].sum()\n",
    "    cnn_relevance = feature_scores[mkan_len:].sum()\n",
    "\n",
    "    sizes = [mkan_relevance, cnn_relevance]\n",
    "    labels = ['m_kan Output', 'CNN Output']\n",
    "    explode = (0.05, 0)  # Slightly explode m_kan slice for emphasis\n",
    "\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.pie(sizes, labels=labels, explode=explode, autopct='%1.1f%%',\n",
    "            shadow=True, startangle=140, colors=[\"#66c2a5\", \"#fc8d62\"])\n",
    "    plt.title(title)\n",
    "    plt.axis('equal')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a73aa349-137e-43f2-9d51-d788b1585dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_hybrid_dataloaders(model, \n",
    "                           dataset,\n",
    "                           opt=\"AdamW\", \n",
    "                           steps=100, \n",
    "                           log=1, \n",
    "                           lamb=0., \n",
    "                           lamb_l1=1., \n",
    "                           lamb_entropy=2., \n",
    "                           lamb_coef=0., \n",
    "                           lamb_coefdiff=0., \n",
    "                           update_grid=True, \n",
    "                           grid_update_num=10, \n",
    "                           loss_fn=None, \n",
    "                           lr=1., \n",
    "                           start_grid_update_step=-1, \n",
    "                           stop_grid_update_step=50, \n",
    "                           batch=-1,\n",
    "                           metrics=None, \n",
    "                           save_fig=False, \n",
    "                           in_vars=None, \n",
    "                           out_vars=None, \n",
    "                           beta=3, \n",
    "                           save_fig_freq=1, \n",
    "                           img_folder='./video', \n",
    "                           singularity_avoiding=False, \n",
    "                           y_th=1000., \n",
    "                           reg_metric='edge_forward_spline_n', \n",
    "                           display_metrics=None,\n",
    "                           sum_f_reg=True):\n",
    "    \"\"\"\n",
    "    Trains the hybrid model (with a KAN branch and a CNN branch) using a steps-based loop\n",
    "    adapted from KAN.fit(), with grid updates and regularization.\n",
    "    \n",
    "    Instead of a single dataset dict, this function accepts three DataLoaders:\n",
    "        - train_loader: provides (mlp, img, target) for training\n",
    "        - val_loader: provides (mlp, img, target) for evaluation during training\n",
    "        - test_loader: provides (mlp, img, target) for validation\n",
    "\n",
    "    Internally, the function combines each loader into a dataset dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        results: dictionary containing training loss, evaluation loss, regularization values,\n",
    "                 and any additional metrics recorded during training.\n",
    "    \"\"\"\n",
    "    #device = next(model.parameters()).device\n",
    "\n",
    "    # Warn if regularization is requested but model's internal flag isn't enabled.\n",
    "    if lamb > 0. and not getattr(model.m_kan, \"save_act\", False):\n",
    "        print(\"setting lamb=0. If you want to set lamb > 0, set model.m_kan.save_act=True\")\n",
    "    \n",
    "    # Disable symbolic processing for training if applicable (KAN internal logic)\n",
    "    if hasattr(model.m_kan, \"disable_symbolic_in_fit\"):\n",
    "        old_save_act, old_symbolic_enabled = model.m_kan.disable_symbolic_in_fit(lamb)\n",
    "        f_old_save_act, f_old_symbolic_enabled = model.final_kan.disable_symbolic_in_fit(lamb)\n",
    "    else:\n",
    "        old_save_act, old_symbolic_enabled = None, None\n",
    "\n",
    "    pbar = tqdm(range(steps), desc='Training', ncols=100)\n",
    "\n",
    "    # Default loss function (mean squared error) if not provided\n",
    "    if loss_fn is None:\n",
    "        loss_fn = lambda x, y: torch.mean((x - y) ** 2)\n",
    "\n",
    "    # Determine grid update frequency\n",
    "    grid_update_freq = int(stop_grid_update_step / grid_update_num) if grid_update_num > 0 else 1\n",
    "\n",
    "    # Determine total number of training examples\n",
    "    n_train = dataset[\"train_input\"].shape[0]\n",
    "    n_eval  = dataset[\"val_input\"].shape[0]  # using val set for evaluation during training\n",
    "    batch_size = n_train if batch == -1 or batch > n_train else batch\n",
    "\n",
    "    # Set up optimizer: choose between Adam and LBFGS (removed tolerance_ys)\n",
    "    if opt == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    elif opt == \"LBFGS\":        \n",
    "        optimizer = LBFGS(model.parameters(), lr=lr, history_size=10, \n",
    "                          line_search_fn=\"strong_wolfe\", \n",
    "                          tolerance_grad=1e-32, \n",
    "                          tolerance_change=1e-32, \n",
    "                          tolerance_ys=1e-32)\n",
    "    else:\n",
    "        raise ValueError(\"Optimizer not recognized. Use 'Adam' or 'LBFGS'.\")\n",
    "\n",
    "    # Prepare results dictionary.\n",
    "    results = {'train_loss': [], 'eval_loss': [], 'reg': []}\n",
    "    \n",
    "    if metrics is not None:\n",
    "        for metric in metrics:\n",
    "            results[metric.__name__] = []\n",
    "\n",
    "    best_model_state = None\n",
    "    best_epoch = -1\n",
    "    best_metric = 0\n",
    "    val_metric = 0\n",
    "\n",
    "    for step in pbar:\n",
    "        # Randomly sample indices for a mini-batch from the training set.\n",
    "        train_indices = np.random.choice(n_train, batch_size, replace=False)\n",
    "        # Use full evaluation set for evaluation; you can also sample if desired.\n",
    "        eval_indices = np.arange(n_eval)\n",
    "        \n",
    "        cached_loss = {}\n",
    "        # Closure for LBFGS\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            mlp_batch = dataset[\"train_input\"][train_indices]\n",
    "            img_batch = dataset[\"train_img\"][train_indices]\n",
    "            target_batch = dataset[\"train_label\"][train_indices]\n",
    "            outputs = model(mlp_batch, img_batch)\n",
    "            train_loss = loss_fn(outputs, target_batch)\n",
    "            # Compute regularization term if enabled.\n",
    "            if hasattr(model.m_kan, \"save_act\") and model.m_kan.save_act:\n",
    "                if reg_metric == 'edge_backward':\n",
    "                    model.m_kan.attribute()\n",
    "                    model.final_kan.attribute()\n",
    "                if reg_metric == 'node_backward':\n",
    "                    model.m_kan.node_attribute()\n",
    "                    model.final_kan.node_attribute()\n",
    "                reg_val_inner = model.m_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "                if sum_f_reg:\n",
    "                    reg_val_inner += model.final_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            else:\n",
    "                reg_val_inner = torch.tensor(0., device=device)\n",
    "            loss_val_inner = train_loss + lamb * reg_val_inner\n",
    "            loss_val_inner.backward()\n",
    "            cached_loss['loss'] = loss_val_inner.detach()\n",
    "            cached_loss['reg'] = reg_val_inner.detach()\n",
    "            return loss_val_inner\n",
    "\n",
    "        # Perform grid update if applicable.\n",
    "        if (step % grid_update_freq == 0 and step < stop_grid_update_step \n",
    "            and update_grid and step >= start_grid_update_step):\n",
    "            \n",
    "            mlp_batch = dataset['train_input'][train_indices]\n",
    "            cnn_batch = dataset['train_img'][train_indices]\n",
    "            \n",
    "            model.m_kan.update_grid(mlp_batch)\n",
    "            #cnn_output = model.cnn_branch(cnn_batch)  # Process image input\n",
    "            concatenated = model.get_concat_output(mlp_batch, cnn_batch)\n",
    "\n",
    "            model.final_kan.update_grid(concatenated)\n",
    "\n",
    "        # Perform an optimizer step.\n",
    "        if opt == \"LBFGS\":\n",
    "            optimizer.step(closure)\n",
    "            loss_val = cached_loss['loss']\n",
    "            reg_val = cached_loss['reg']\n",
    "        else:  # AdamW branch\n",
    "            optimizer.zero_grad()\n",
    "            mlp_batch = dataset[\"train_input\"][train_indices]\n",
    "            img_batch = dataset[\"train_img\"][train_indices]\n",
    "            target_batch = dataset[\"train_label\"][train_indices]\n",
    "            outputs = model(mlp_batch, img_batch)\n",
    "            train_loss = loss_fn(outputs, target_batch)\n",
    "            if hasattr(model.m_kan, \"save_act\") and model.m_kan.save_act:\n",
    "                if reg_metric == 'edge_backward':\n",
    "                    model.m_kan.attribute()\n",
    "                    model.final_kan.attribute()\n",
    "                if reg_metric == 'node_backward':\n",
    "                    model.m_kan.node_attribute()\n",
    "                    model.final_kan.node_attribute()\n",
    "                reg_val = model.m_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "                if sum_f_reg:\n",
    "                    reg_val = reg_val + model.final_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            else:\n",
    "                reg_val = torch.tensor(0., device=device)\n",
    "            loss_val = train_loss + lamb * reg_val\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mlp_eval = dataset[\"val_input\"][eval_indices]\n",
    "            img_eval = dataset[\"val_img\"][eval_indices]\n",
    "            target_eval = dataset[\"val_label\"][eval_indices]\n",
    "            eval_loss = loss_fn(model(mlp_eval, img_eval), target_eval)\n",
    "\n",
    "        # Record results (using square-root of loss similar to KAN.fit)\n",
    "        results['eval_loss'].append(torch.sqrt(eval_loss.detach()).item())\n",
    "        results['reg'].append(reg_val.detach().item())\n",
    "\n",
    "        if metrics is not None:\n",
    "            for metric in metrics:\n",
    "                # Here, we assume each metric returns a tensor.\n",
    "                results[metric.__name__].append(metric().item())\n",
    "\n",
    "\n",
    "        # Update progress bar.\n",
    "        if display_metrics is None:\n",
    "            pbar.set_description(\"| train_loss: %.2e | eval_loss: %.2e | reg: %.2e |\" %\n",
    "                                 (torch.sqrt(loss_val.detach()).item(),\n",
    "                                  torch.sqrt(eval_loss.detach()).item(),\n",
    "                                  reg_val.detach().item()))\n",
    "        else:\n",
    "            string = ''\n",
    "            data = ()\n",
    "            for metric in display_metrics:\n",
    "                val_metric = results[metric][-1]\n",
    "                string += f' {metric}: %.2e |'\n",
    "                try:\n",
    "                    results[metric]\n",
    "                except:\n",
    "                    raise Exception(f'{metric} not recognized')\n",
    "                data += (results[metric][-1],)\n",
    "            pbar.set_description(string % data)\n",
    "\n",
    "        if val_metric > best_metric:\n",
    "            best_epoch = step\n",
    "            best_metric = val_metric\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Optionally save a figure snapshot.\n",
    "        if save_fig and step % save_fig_freq == 0:\n",
    "            save_act_backup = getattr(model.m_kan, \"save_act\", False)\n",
    "            model.m_kan.save_act = True\n",
    "            model.plot(folder=img_folder, in_vars=in_vars, out_vars=out_vars, title=f\"Step {step}\", beta=beta)\n",
    "            plt.savefig(os.path.join(img_folder, f\"{step}.jpg\"), bbox_inches='tight', dpi=200)\n",
    "            plt.close()\n",
    "            model.m_kan.save_act = save_act_backup\n",
    "\n",
    "    # Restore original settings if applicable.\n",
    "    if old_symbolic_enabled is not None:\n",
    "        model.m_kan.symbolic_enabled = old_symbolic_enabled\n",
    "    if hasattr(model.m_kan, \"log_history\"):\n",
    "        model.m_kan.log_history('fit')\n",
    "    print(f\"✅ Best validation Accuracy: {best_metric:.4e} at {best_epoch} epoch\")\n",
    "    return best_model_state, results, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b2674e0-7282-46a1-aa61-f72459feb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def try_create_model(model_class, attributes, imgs_shape, kan_neurons, kan_grid, cnn_bottleneck_dim, alpha, hidden_dim, embed_dim, num_heads):\n",
    "    try:\n",
    "        model = model_class(attributes, imgs_shape, kan_neurons, kan_grid,\n",
    "                            cnn_bottleneck_dim=cnn_bottleneck_dim, alpha=alpha, hidden_dim=hidden_dim, embed_dim=embed_dim, num_heads=num_heads)\n",
    "        # Test the model with a sample input\n",
    "        num_input = torch.randn(4, attributes)\n",
    "        img_input = torch.randn(4, *imgs_shape)\n",
    "        output = model(num_input, img_input)\n",
    "        \n",
    "        print(f\"Successfully created and tested {model_class.__name__}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating or testing {model_class.__name__}:\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efc69566-0316-47ec-8c23-bc6d5cf685e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_branch_relevance(model, best_model_state):\n",
    "    avg_scores = compute_avg_feature_relevance_from_val(\n",
    "        model=model,\n",
    "        model_state=best_model_state,\n",
    "        val_inputs=dataset[\"test_input\"],\n",
    "        val_imgs=dataset[\"test_img\"],\n",
    "        coordinate=completed_coordinate,\n",
    "        x_col=completed_x_col,\n",
    "        zoom=2\n",
    "    )\n",
    "    return plot_feature_relevance_bar(avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb693e0d-e74d-4b67-b284-1b9d71e05295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_plot_relevance(model_class, kan_neurons, kan_grid, lamb, steps, cnn_bottleneck_dim=-1, alpha=-1, hidden_dim=-1, embed_dim=-1, num_heads=-1, n_kan_len=None, filename=None, opt_col_val=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model = try_create_model(model_class, attributes, imgs_shape, kan_neurons=kan_neurons, kan_grid=kan_grid,\n",
    "                             cnn_bottleneck_dim=cnn_bottleneck_dim, alpha=alpha, hidden_dim=hidden_dim, embed_dim=embed_dim, num_heads=num_heads)\n",
    "    def train_acc_hybrid():\n",
    "        rounded = torch.round(torch.round(model(dataset['train_input'], dataset['train_img'])[:,0]))\n",
    "        clipped = torch.clamp(rounded, min=min_expected, max=max_expected)\n",
    "        return torch.mean((clipped == dataset['train_label'][:,0]).type(dtype))\n",
    "    \n",
    "    def val_acc_hybrid():\n",
    "        rounded = torch.round(torch.round(model(dataset['val_input'], dataset['val_img'])[:,0]))\n",
    "        clipped = torch.clamp(rounded, min=min_expected, max=max_expected)\n",
    "        return torch.mean((clipped == dataset['val_label'][:,0]).type(dtype))\n",
    "\n",
    "    model_state, results, best_epoch = fit_hybrid_dataloaders(\n",
    "        model, dataset, opt=\"LBFGS\", steps=steps, lamb=lamb,\n",
    "        metrics=(train_acc_hybrid, val_acc_hybrid), display_metrics=['train_acc_hybrid', 'val_acc_hybrid'])\n",
    "\n",
    "    model.load_state_dict(model_state)\n",
    "    acc = plot_training_ACC(dataset['test_label'][:,0], model(dataset['test_input'], dataset['test_img'])[:,0],\n",
    "                            results['train_acc_hybrid'], results['val_acc_hybrid'])\n",
    "\n",
    "    cm = plot_confusion_matrix(dataset['test_label'][:,0], model(dataset['test_input'], dataset['test_img'])[:,0], title=\"Confusion Matrix\")\n",
    "\n",
    "    if not n_kan_len:\n",
    "        n_kan_len = kan_neurons\n",
    "    k_rel, cnn_rel = print_mkan_vs_cnn_relevance(model.final_kan.feature_score, mkan_len=n_kan_len)\n",
    "    #plot_mkan_vs_cnn_relevance(model.final_kan.feature_score, mkan_len=kan_neurons)\n",
    "    kan_mrf = \"\"#plot_sorted_feature_importance(x_col, model.m_kan.feature_score)\n",
    "    cnn_mrf = \"\"#cnn_branch_relevance(model, model_state)\n",
    "\n",
    "    append_row_to_csv(filename, kan_neurons, kan_grid, lamb, opt_col_val, acc.item(), cm, best_epoch, k_rel, cnn_rel, kan_mrf, cnn_mrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8c13c-88aa-4cbd-9819-bf1d37c8027a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Models Class Hybrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2c4625e-ed3d-43de-85d0-126f94916b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4_1(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, kan_neurons, kan_grid, cnn_bottleneck_dim=-1, alpha=-1, hidden_dim=-1, embed_dim=-1, num_heads=-1, device=device):\n",
    "        super(Model4_1, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], 16, kernel_size=3, padding=2),     # out: 16 x 9 x 9\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                            # out: 16 x 4 x 4\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=2),\n",
    "            nn.LayerNorm([32, 13, 13]),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "        # Dummy pass to get flattened size\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        # Bottleneck layer\n",
    "        self.cnn_bottleneck = nn.Linear(self.flat_size, cnn_bottleneck_dim).to(device)\n",
    "\n",
    "        # KAN branch\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, kan_neurons],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Final KAN layer\n",
    "        self.final_kan = KAN(\n",
    "            width=[cnn_bottleneck_dim + kan_neurons, 1],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        dummy_input = torch.zeros(1, *imgs_shape, device=self.device)\n",
    "        x = self.cnn_branch(dummy_input)\n",
    "        return x.shape[1]\n",
    "\n",
    "    def get_concat_output(self, mlp_input, cnn_input):\n",
    "        kan_input = mlp_input.to(self.device)\n",
    "        cnn_input = cnn_input.to(self.device)\n",
    "\n",
    "        conv_out = self.cnn_branch(cnn_input)\n",
    "        cnn_output = self.cnn_bottleneck(conv_out)\n",
    "\n",
    "        kan_output = self.m_kan(kan_input)\n",
    "\n",
    "        return torch.cat((kan_output, cnn_output), dim=1)\n",
    "\n",
    "    def forward(self, mlp_input, cnn_input):\n",
    "        concat_output = self.get_concat_output(mlp_input, cnn_input)\n",
    "        return self.final_kan(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0116754e-6c80-4271-99db-c698d41381fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4_2(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, kan_neurons, kan_grid, cnn_bottleneck_dim=-1, alpha=-1, hidden_dim=-1, embed_dim=-1, num_heads=-1, device=device):\n",
    "        super(Model4_2, self).__init__()\n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], 16, kernel_size=3, padding=2),     # out: 16 x 9 x 9\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                            # out: 16 x 4 x 4\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=2),\n",
    "            nn.LayerNorm([32, 13, 13]),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "        # Final KAN layers\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, kan_neurons],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Calculate the size of the flattened output\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        # Final MLP layers\n",
    "        self.final_kan = KAN(\n",
    "            width=[self.flat_size + kan_neurons, 1],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        # Forward pass with dummy input to calculate flat size\n",
    "        dummy_input = torch.zeros(4, *imgs_shape, device=device)\n",
    "        x = self.cnn_branch(dummy_input)\n",
    "        return x.size(1)\n",
    "\n",
    "    def get_concat_output(self, mlp_input, cnn_input):\n",
    "        # Ensure inputs are moved to the correct device\n",
    "        kan_input = mlp_input.to(self.device)\n",
    "        cnn_input = cnn_input.to(self.device)\n",
    "        \n",
    "        cnn_output = self.cnn_branch(cnn_input)  # Process image input\n",
    "        cnn_output = cnn_output * self.alpha\n",
    "        kan_output = self.m_kan(kan_input)  # Process numerical input\n",
    "        \n",
    "        return torch.cat((kan_output, cnn_output), dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self, mlp_input, cnn_input):\n",
    "        concat_output = self.get_concat_output(mlp_input, cnn_input)\n",
    "        return self.final_kan(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9dcea60-6f23-4bd6-bfab-ad94c179576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4_3(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, kan_neurons, kan_grid, cnn_bottleneck_dim=-1, alpha=-1, hidden_dim=-1, embed_dim=-1, num_heads=-1, device=device):\n",
    "        super(Model4_3, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], 16, kernel_size=3, padding=2),     # out: 16 x 9 x 9\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                            # out: 16 x 4 x 4\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=2),\n",
    "            nn.LayerNorm([32, 13, 13]),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        # KAN branch\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, kan_neurons],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Gating MLP: inputs are concatenated CNN + KAN representations\n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(self.flat_size + kan_neurons, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # Output ∈ [0,1]\n",
    "        ).to(device)\n",
    "\n",
    "        # Final regressor (KAN layer)\n",
    "        self.final_kan = KAN(\n",
    "            width=[kan_neurons + self.flat_size, 1],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        dummy_input = torch.zeros(4, *imgs_shape, device=self.device)\n",
    "        x = self.cnn_branch(dummy_input)\n",
    "        return x.size(1)\n",
    "\n",
    "    def get_concat_output(self, mlp_input, cnn_input):\n",
    "        mlp_input = mlp_input.to(self.device)\n",
    "        cnn_input = cnn_input.to(self.device)\n",
    "\n",
    "        kan_out = self.m_kan(mlp_input)                  # shape: (B, kan_neurons)\n",
    "        cnn_out = self.cnn_branch(cnn_input)             # shape: (B, cnn_flat)\n",
    "\n",
    "        concat = torch.cat((kan_out, cnn_out), dim=1)    # For gating\n",
    "        alpha = self.gate_net(concat)                    # shape: (B, 1)\n",
    "\n",
    "        gated_kan = (1 - alpha) * kan_out                # shape: (B, kan_neurons)\n",
    "        gated_cnn = alpha * cnn_out                      # shape: (B, cnn_flat)\n",
    "\n",
    "        return torch.cat((gated_kan, gated_cnn), dim=1)  # shape: (B, total)\n",
    "\n",
    "    def forward(self, mlp_input, cnn_input):\n",
    "        fused = self.get_concat_output(mlp_input, cnn_input)\n",
    "        return self.final_kan(fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1f9b233-275c-4e83-8bed-7c12e53e3b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4_4(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, kan_neurons, kan_grid, cnn_bottleneck_dim=-1, alpha=-1, hidden_dim=-1, embed_dim=-1, num_heads=-1, device=device):\n",
    "        super(Model4_4, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], 16, kernel_size=3, padding=2),     # out: 16 x 9 x 9\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                            # out: 16 x 4 x 4\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=2),\n",
    "            nn.LayerNorm([32, 13, 13]),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        # KAN Branch\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, kan_neurons],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.query_proj = nn.Linear(kan_neurons, embed_dim).to(device)\n",
    "        self.key_proj = nn.Linear(self.flat_size, embed_dim).to(device)\n",
    "        self.value_proj = nn.Linear(self.flat_size, embed_dim).to(device)\n",
    "\n",
    "        # Attention module\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True).to(device)\n",
    "\n",
    "        # Final regression layer (KAN again)\n",
    "        self.final_kan = KAN(\n",
    "            width=[embed_dim, 1], grid=kan_grid, k=3, seed=SEED, device=device\n",
    "        )\n",
    "    \n",
    "    def get_concat_output(self, mlp_input, cnn_input):\n",
    "        # Get KAN and CNN outputs\n",
    "        kan_out = self.m_kan(mlp_input.to(self.device))  # [B, D_kan]\n",
    "        cnn_out = self.cnn_branch(cnn_input.to(self.device))  # [B, D_cnn]\n",
    "\n",
    "        # Project into Q, K, V space\n",
    "        Q = self.query_proj(kan_out).unsqueeze(1)  # [B, 1, E]\n",
    "        K = self.key_proj(cnn_out).unsqueeze(1)    # [B, 1, E]\n",
    "        V = self.value_proj(cnn_out).unsqueeze(1)  # [B, 1, E]\n",
    "        # Cross-attention: KAN attends to CNN\n",
    "        attn_out, _ = self.attn(Q, K, V)  # [B, 1, E]\n",
    "        attn_out = attn_out.squeeze(1)   # [B, E]\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "\n",
    "\n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        dummy_input = torch.zeros(1, *imgs_shape, device=self.device)\n",
    "        return self.cnn_branch(dummy_input).shape[1]\n",
    "\n",
    "    def forward(self, mlp_input, cnn_input):\n",
    "        attn_out = self.get_concat_output(mlp_input, cnn_input)\n",
    "\n",
    "        return self.final_kan(attn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb7b42-d9ea-4e45-9b6b-da78e1bb43d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Dataset and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aee6586c-7ff1-40f9-9768-c6adccae02bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_and_clean('N_train.npy', 'y_train.npy',x_col, target_col)\n",
    "X_test, y_test   = load_and_clean('N_test.npy',  'y_test.npy', x_col, target_col)\n",
    "X_val, y_val     = load_and_clean('N_val.npy',   'y_val.npy', x_col, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92613d4f-1cd2-49f8-82a9-620fda68b565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the dataframe\n",
    "num_columns = X_train.shape[1]\n",
    "\n",
    "# Calculate number of columns - 1\n",
    "columns_minus_one = num_columns - 1\n",
    "\n",
    "# Calculate the square root for image size\n",
    "image_size = math.ceil(math.sqrt(num_columns))\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a28853f7-dcee-47de-91eb-8f35177b8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'wall-robot-navigation'\n",
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "pixel=20\n",
    "image_model = TINTO(problem=problem_type, blur=False, pixels=pixel, random_seed=SEED)\n",
    "name = f\"TINTO\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a01e98f2-1bea-4364-b0bc-3d0412ebba5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images are already generated\n",
      "The images are already generated\n",
      "The images are already generated\n",
      "Images shape:  (3, 8, 8)\n",
      "Attributes:  10\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape = load_and_preprocess_data(\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val,\n",
    "    image_model=image_model,\n",
    "    problem_type=problem_type,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b762542d-adc5-448e-8309-3368ef5f245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_feature_coordinates_from_model(model, with_names=False):\n",
    "#     \"\"\"\n",
    "#     Extracts the (row, col) positions of features from a fitted REFINED model.\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     model : REFINED object\n",
    "#         The fitted REFINED image_model.\n",
    "#     with_names : bool, optional\n",
    "#         If True, include feature names in the output.\n",
    "#     Returns\n",
    "#     -------\n",
    "#     feature_to_position : dict\n",
    "#         If with_names=False:\n",
    "#             {feature_idx: (row, col)}\n",
    "#         If with_names=True:\n",
    "#             {feature_idx: {'name': str, 'position': (row, col)}}\n",
    "#     \"\"\"\n",
    "#     if not hasattr(model, 'map_in_int_MDS') or not hasattr(model, 'gene_names_MDS'):\n",
    "#         raise RuntimeError(\"The REFINED algorithm has not been fitted yet. Please call `fit()` first.\")\n",
    "#     feature_to_position = {}\n",
    "#     grid = model.map_in_int_MDS\n",
    "#     for row in range(grid.shape[0]):\n",
    "#         for col in range(grid.shape[1]):\n",
    "#             feat_idx = grid[row, col]\n",
    "#             if feat_idx != -1:\n",
    "#                 if with_names:\n",
    "#                     feature_to_position[int(feat_idx)] = {\n",
    "#                         \"name\": model.gene_names_MDS[int(feat_idx)],\n",
    "#                         \"position\": (row, col)\n",
    "#                     }\n",
    "#                 else:\n",
    "#                     feature_to_position[int(feat_idx)] = (row, col)\n",
    "#     return feature_to_position\n",
    "\n",
    "# coords = get_feature_coordinates_from_model(image_model, with_names=True)\n",
    "# for idx, info in coords.items():\n",
    "#     print(f\"Feature {idx} ({info['name']}) → Position {info['position']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5818e069-06ec-4869-b410-e37d75bab205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43a1f4c-b496-4232-9d5f-fddb77c9f1de",
   "metadata": {},
   "source": [
    "Feature 0 (0) → Position (1, 2)\n",
    "Feature 1 (1) → Position (2, 3)\n",
    "Feature 2 (2) → Position (3, 2)\n",
    "Feature 3 (3) → Position (2, 2)\n",
    "Feature 4 (4) → Position (1, 0)\n",
    "Feature 5 (5) → Position (0, 3)\n",
    "Feature 6 (6) → Position (1, 3)\n",
    "Feature 7 (7) → Position (3, 3)\n",
    "Feature 8 (8) → Position (3, 1)\n",
    "Feature 9 (9) → Position (0, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c84fd70-94ac-4546-b907-00fe548676ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine dataloaders into tensors.\n",
    "train_mlp, train_img, train_target = combine_loader(train_loader)\n",
    "val_mlp, val_img, val_target = combine_loader(val_loader)\n",
    "test_mlp, test_img, test_target = combine_loader(test_loader)\n",
    "\n",
    "dataset = {\n",
    "    \"train_input\": train_mlp.to(device),\n",
    "    \"train_img\": train_img.to(device),\n",
    "    \"train_label\": train_target.to(device),\n",
    "    \"val_input\": val_mlp.to(device),\n",
    "    \"val_img\": val_img.to(device),\n",
    "    \"val_label\": val_target.to(device),\n",
    "    \"test_input\": test_mlp.to(device),\n",
    "    \"test_img\": test_img.to(device),\n",
    "    \"test_label\": test_target.to(device),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8382d81-df3d-4828-90a0-a472f0fb312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([1173, 10])\n",
      "Train target shape: torch.Size([1173, 1])\n",
      "Test data shape: torch.Size([367, 10])\n",
      "Test target shape: torch.Size([367, 1])\n",
      "Validation data shape: torch.Size([294, 10])\n",
      "Validation target shape: torch.Size([294, 1])\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the tensors\n",
    "print(\"Train data shape:\", dataset['train_input'].shape)\n",
    "print(\"Train target shape:\", dataset['train_label'].shape)\n",
    "print(\"Test data shape:\", dataset['test_input'].shape)\n",
    "print(\"Test target shape:\", dataset['test_label'].shape)\n",
    "print(\"Validation data shape:\", dataset['val_input'].shape)\n",
    "print(\"Validation target shape:\", dataset['val_label'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126399a6-fb71-40f7-9d1c-6014109774fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set Files Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "60d3d7f4-2829-4888-aafe-31f3dce2d905",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_1=f'{dataset_name}_tinto_Concat_Op1.csv'\n",
    "filename_2=f'{dataset_name}_tinto_Concat_Op2.csv'\n",
    "filename_3=f'{dataset_name}_tinto_Concat_Op3.csv'\n",
    "filename_4=f'{dataset_name}_tinto_Concat_Op4.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d50d1c4-7175-4851-b257-81ca5be71e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_opt1 = 'cnn_bottleneck_dim'\n",
    "columns_opt2 = 'alpha'\n",
    "columns_opt3 = 'hidden_dim'\n",
    "columns_opt4 = 'embed_dim, num_heads'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70ecc0-18fe-4e79-9caa-b5d7236f22eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce5d53e6-cabe-4cfa-9679-4f67f9113dda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Option 1: Concat KAN with (CNN with dense layer to reduce output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827fdb0e-74b3-4944-8f16-a0979b5c9315",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_with_header(filename_1, columns_opt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e3d8b32-5f82-4965-b00f-47432526fc5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ kan_neurons=3, kan_grid=7, lamb=1e-06 ------------------------------\n",
      "cnn_bottleneck_dim: 1\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.89e-01 | val_acc_hybrid: 6.84e-01 |: 100%|█████| 50/50 [00:16<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3129e-01 at 2 epoch\n",
      "tensor(0.7139, device='cuda:0')\n",
      "[[126  57]\n",
      " [ 48 136]]\n",
      "M_KAN Relevance: 0.2593177855014801\n",
      "CNN Relevance: 0.7406821846961975\n",
      "cnn_bottleneck_dim: 2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.90e-01 | val_acc_hybrid: 6.77e-01 |: 100%|█████| 50/50 [00:15<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2449e-01 at 8 epoch\n",
      "tensor(0.7248, device='cuda:0')\n",
      "[[132  51]\n",
      " [ 50 134]]\n",
      "M_KAN Relevance: 0.29896867275238037\n",
      "CNN Relevance: 0.7010313272476196\n",
      "cnn_bottleneck_dim: 3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.86e-01 | val_acc_hybrid: 6.84e-01 |: 100%|█████| 50/50 [00:15<00:00,  3.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2449e-01 at 1 epoch\n",
      "tensor(0.7084, device='cuda:0')\n",
      "[[125  58]\n",
      " [ 49 135]]\n",
      "M_KAN Relevance: 0.2637682557106018\n",
      "CNN Relevance: 0.7362317442893982\n",
      "cnn_bottleneck_dim: 6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.41e-01 | val_acc_hybrid: 6.70e-01 |: 100%|█████| 50/50 [00:15<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 6.9048e-01 at 40 epoch\n",
      "tensor(0.6894, device='cuda:0')\n",
      "[[120  63]\n",
      " [ 51 133]]\n",
      "M_KAN Relevance: 0.30125123262405396\n",
      "CNN Relevance: 0.698748767375946\n",
      "cnn_bottleneck_dim: 7\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.94e-01 | val_acc_hybrid: 6.84e-01 |: 100%|█████| 50/50 [00:15<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2449e-01 at 2 epoch\n",
      "tensor(0.7302, device='cuda:0')\n",
      "[[131  52]\n",
      " [ 47 137]]\n",
      "M_KAN Relevance: 0.24447648227214813\n",
      "CNN Relevance: 0.7555235028266907\n",
      "cnn_bottleneck_dim: 9\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.95e-01 | val_acc_hybrid: 7.24e-01 |: 100%|█████| 50/50 [00:15<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.4830e-01 at 36 epoch\n",
      "tensor(0.6839, device='cuda:0')\n",
      "[[131  52]\n",
      " [ 64 120]]\n",
      "M_KAN Relevance: 0.2285248041152954\n",
      "CNN Relevance: 0.7714751958847046\n",
      "cnn_bottleneck_dim: 10\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.82e-01 | val_acc_hybrid: 6.87e-01 |: 100%|█████| 50/50 [00:15<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2789e-01 at 5 epoch\n",
      "tensor(0.7057, device='cuda:0')\n",
      "[[123  60]\n",
      " [ 48 136]]\n",
      "M_KAN Relevance: 0.15092574059963226\n",
      "CNN Relevance: 0.8490742444992065\n",
      "cnn_bottleneck_dim: 12\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.76e-01 | val_acc_hybrid: 6.87e-01 |: 100%|█████| 50/50 [00:15<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3810e-01 at 7 epoch\n",
      "tensor(0.6975, device='cuda:0')\n",
      "[[127  56]\n",
      " [ 55 129]]\n",
      "M_KAN Relevance: 0.229435995221138\n",
      "CNN Relevance: 0.7705640196800232\n",
      "------------------------------ kan_neurons=5, kan_grid=7, lamb=0.0001 ------------------------------\n",
      "cnn_bottleneck_dim: 1\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.06e-01 | val_acc_hybrid: 6.53e-01 |: 100%|█████| 50/50 [00:15<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 6.8367e-01 at 5 epoch\n",
      "tensor(0.7112, device='cuda:0')\n",
      "[[136  47]\n",
      " [ 59 125]]\n",
      "M_KAN Relevance: 0.9483941793441772\n",
      "CNN Relevance: 0.05160585790872574\n",
      "cnn_bottleneck_dim: 2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.00e-01 | val_acc_hybrid: 7.01e-01 |: 100%|█████| 50/50 [00:15<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.4490e-01 at 12 epoch\n",
      "tensor(0.7057, device='cuda:0')\n",
      "[[133  50]\n",
      " [ 58 126]]\n",
      "M_KAN Relevance: 0.31496790051460266\n",
      "CNN Relevance: 0.6850321292877197\n",
      "cnn_bottleneck_dim: 4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.78e-01 | val_acc_hybrid: 6.84e-01 |: 100%|█████| 50/50 [00:15<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3129e-01 at 6 epoch\n",
      "tensor(0.7248, device='cuda:0')\n",
      "[[132  51]\n",
      " [ 50 134]]\n",
      "M_KAN Relevance: 0.20957835018634796\n",
      "CNN Relevance: 0.7904216647148132\n",
      "cnn_bottleneck_dim: 6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.84e-01 | val_acc_hybrid: 6.94e-01 |: 100%|█████| 50/50 [00:15<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3129e-01 at 2 epoch\n",
      "tensor(0.7302, device='cuda:0')\n",
      "[[130  53]\n",
      " [ 46 138]]\n",
      "M_KAN Relevance: 0.31841519474983215\n",
      "CNN Relevance: 0.6815847754478455\n",
      "cnn_bottleneck_dim: 8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.19e-01 | val_acc_hybrid: 6.60e-01 |: 100%|█████| 50/50 [00:15<00:00,  3.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1429e-01 at 13 epoch\n",
      "tensor(0.7084, device='cuda:0')\n",
      "[[129  54]\n",
      " [ 53 131]]\n",
      "M_KAN Relevance: 0.24121418595314026\n",
      "CNN Relevance: 0.7587857842445374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------ kan_neurons=12, kan_grid=8, lamb=0.001 ------------------------------\")\n",
    "for cnn_bottleneck_dim in [1, 3, 6, 9, 12, 15, 18, 21, 24, 27]:\n",
    "    print(f\"cnn_bottleneck_dim: {cnn_bottleneck_dim}\")\n",
    "    train_and_plot_relevance(Model4_1, kan_neurons=12, kan_grid=8, lamb=0.001, steps=100, \n",
    "                             cnn_bottleneck_dim=cnn_bottleneck_dim, filename=filename_1, opt_col_val=cnn_bottleneck_dim)\n",
    "\n",
    "print(\"------------------------------ kan_neurons=6, kan_grid=7, lamb=1e-05 ------------------------------\")\n",
    "for cnn_bottleneck_dim in [1, 2, 3, 6, 9, 12, 15]:\n",
    "    print(f\"cnn_bottleneck_dim: {cnn_bottleneck_dim}\")\n",
    "    train_and_plot_relevance(Model4_1, kan_neurons=6, kan_grid=7, lamb=1e-05, steps=100, \n",
    "                             cnn_bottleneck_dim=cnn_bottleneck_dim, filename=filename_1, opt_col_val=cnn_bottleneck_dim)\n",
    "\n",
    "\n",
    "print(\"------------------------------ kan_neurons=8, kan_grid=8, lamb=0.001 ------------------------------\")\n",
    "for cnn_bottleneck_dim in [1, 2, 3, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24]:\n",
    "    print(f\"cnn_bottleneck_dim: {cnn_bottleneck_dim}\")\n",
    "    train_and_plot_relevance(Model4_1, kan_neurons=8, kan_grid=8, lamb=0.001, steps=100, \n",
    "                             cnn_bottleneck_dim=cnn_bottleneck_dim, filename=filename_1, opt_col_val=cnn_bottleneck_dim)\n",
    "\n",
    "\n",
    "print(\"------------------------------ kan_neurons=3, kan_grid=7, lamb=0.001 ------------------------------\")\n",
    "for cnn_bottleneck_dim in [1, 2, 3, 4, 6, 8, 9]:\n",
    "    print(f\"cnn_bottleneck_dim: {cnn_bottleneck_dim}\")\n",
    "    train_and_plot_relevance(Model4_1, kan_neurons=3, kan_grid=7, lamb=0.001, steps=100, \n",
    "                             cnn_bottleneck_dim=cnn_bottleneck_dim, filename=filename_1, opt_col_val=cnn_bottleneck_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d2af7-772b-4317-a7da-4446cbaf2990",
   "metadata": {},
   "source": [
    "# Option 2: Multiply CNN output by factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed339113-c866-41b2-9e21-47cc7a4f45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_with_header(filename_2, columns_opt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fed66cd4-7049-49d9-aee1-38f9ed14201d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ kan_neurons=3, kan_grid=7, lamb=1e-06 ------------------------------\n",
      "alpha: 0.9\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.70e-01 | val_acc_hybrid: 6.22e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3469e-01 at 1 epoch\n",
      "tensor(0.7166, device='cuda:0')\n",
      "[[126  57]\n",
      " [ 47 137]]\n",
      "M_KAN Relevance: 0.0019358622375875711\n",
      "CNN Relevance: 0.9980641603469849\n",
      "alpha: 0.8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.60e-01 | val_acc_hybrid: 6.36e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3129e-01 at 1 epoch\n",
      "tensor(0.7248, device='cuda:0')\n",
      "[[124  59]\n",
      " [ 42 142]]\n",
      "M_KAN Relevance: 0.0020198177080601454\n",
      "CNN Relevance: 0.9979802370071411\n",
      "alpha: 0.75\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.56e-01 | val_acc_hybrid: 6.19e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2789e-01 at 1 epoch\n",
      "tensor(0.7166, device='cuda:0')\n",
      "[[123  60]\n",
      " [ 44 140]]\n",
      "M_KAN Relevance: 0.002253951271995902\n",
      "CNN Relevance: 0.9977460503578186\n",
      "alpha: 0.7\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.66e-01 | val_acc_hybrid: 6.43e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2789e-01 at 2 epoch\n",
      "tensor(0.7384, device='cuda:0')\n",
      "[[137  46]\n",
      " [ 50 134]]\n",
      "M_KAN Relevance: 0.0033220998011529446\n",
      "CNN Relevance: 0.9966778755187988\n",
      "alpha: 0.6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.76e-01 | val_acc_hybrid: 5.65e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2789e-01 at 4 epoch\n",
      "tensor(0.7330, device='cuda:0')\n",
      "[[134  49]\n",
      " [ 49 135]]\n",
      "M_KAN Relevance: 0.0034475724678486586\n",
      "CNN Relevance: 0.9965524673461914\n",
      "alpha: 0.5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.58e-01 | val_acc_hybrid: 6.29e-01 |: 100%|█████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2449e-01 at 27 epoch\n",
      "tensor(0.6894, device='cuda:0')\n",
      "[[126  57]\n",
      " [ 57 127]]\n",
      "M_KAN Relevance: 0.0015984050696715713\n",
      "CNN Relevance: 0.9984015822410583\n",
      "alpha: 0.4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 9.20e-01 | val_acc_hybrid: 6.02e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1769e-01 at 7 epoch\n",
      "tensor(0.7248, device='cuda:0')\n",
      "[[140  43]\n",
      " [ 58 126]]\n",
      "M_KAN Relevance: 0.0027699486818164587\n",
      "CNN Relevance: 0.997230052947998\n",
      "alpha: 0.3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.66e-01 | val_acc_hybrid: 5.78e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1429e-01 at 5 epoch\n",
      "tensor(0.7084, device='cuda:0')\n",
      "[[133  50]\n",
      " [ 57 127]]\n",
      "M_KAN Relevance: 0.0034978354815393686\n",
      "CNN Relevance: 0.996502161026001\n",
      "alpha: 0.25\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.82e-01 | val_acc_hybrid: 5.75e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1429e-01 at 3 epoch\n",
      "tensor(0.7166, device='cuda:0')\n",
      "[[132  51]\n",
      " [ 53 131]]\n",
      "M_KAN Relevance: 0.0038090243469923735\n",
      "CNN Relevance: 0.9961909651756287\n",
      "alpha: 0.2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.96e-01 | val_acc_hybrid: 5.99e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2449e-01 at 7 epoch\n",
      "tensor(0.7139, device='cuda:0')\n",
      "[[136  47]\n",
      " [ 58 126]]\n",
      "M_KAN Relevance: 0.004009689204394817\n",
      "CNN Relevance: 0.9959903359413147\n",
      "alpha: 0.1\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 9.15e-01 | val_acc_hybrid: 5.61e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.0748e-01 at 7 epoch\n",
      "tensor(0.7193, device='cuda:0')\n",
      "[[133  50]\n",
      " [ 53 131]]\n",
      "M_KAN Relevance: 0.004993771202862263\n",
      "CNN Relevance: 0.9950062036514282\n",
      "alpha: 0.05\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 9.01e-01 | val_acc_hybrid: 5.78e-01 |: 100%|█████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.0068e-01 at 7 epoch\n",
      "tensor(0.7248, device='cuda:0')\n",
      "[[136  47]\n",
      " [ 54 130]]\n",
      "M_KAN Relevance: 0.0055891661904752254\n",
      "CNN Relevance: 0.9944108128547668\n",
      "alpha: 0.01\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.73e-01 | val_acc_hybrid: 6.22e-01 |: 100%|█████| 60/60 [00:33<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2109e-01 at 5 epoch\n",
      "tensor(0.7057, device='cuda:0')\n",
      "[[130  53]\n",
      " [ 55 129]]\n",
      "M_KAN Relevance: 0.016921790316700935\n",
      "CNN Relevance: 0.9830781817436218\n",
      "------------------------------ kan_neurons=5, kan_grid=7, lamb=0.0001 ------------------------------\n",
      "alpha: 0.9\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.73e-01 | val_acc_hybrid: 5.58e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3129e-01 at 1 epoch\n",
      "tensor(0.7330, device='cuda:0')\n",
      "[[130  53]\n",
      " [ 45 139]]\n",
      "M_KAN Relevance: 0.013979962095618248\n",
      "CNN Relevance: 0.9860200881958008\n",
      "alpha: 0.8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.74e-01 | val_acc_hybrid: 6.26e-01 |: 100%|█████| 60/60 [00:33<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2109e-01 at 3 epoch\n",
      "tensor(0.7221, device='cuda:0')\n",
      "[[131  52]\n",
      " [ 50 134]]\n",
      "M_KAN Relevance: 0.02135593444108963\n",
      "CNN Relevance: 0.978644073009491\n",
      "alpha: 0.75\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.58e-01 | val_acc_hybrid: 6.60e-01 |: 100%|█████| 60/60 [00:33<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1769e-01 at 1 epoch\n",
      "tensor(0.7030, device='cuda:0')\n",
      "[[125  58]\n",
      " [ 51 133]]\n",
      "M_KAN Relevance: 0.013213453814387321\n",
      "CNN Relevance: 0.9867866039276123\n",
      "alpha: 0.7\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 9.00e-01 | val_acc_hybrid: 6.46e-01 |: 100%|█████| 60/60 [00:31<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1088e-01 at 1 epoch\n",
      "tensor(0.7139, device='cuda:0')\n",
      "[[126  57]\n",
      " [ 48 136]]\n",
      "M_KAN Relevance: 0.01624481752514839\n",
      "CNN Relevance: 0.9837552309036255\n",
      "alpha: 0.6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.25e-01 | val_acc_hybrid: 6.63e-01 |: 100%|█████| 60/60 [00:33<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2109e-01 at 11 epoch\n",
      "tensor(0.6948, device='cuda:0')\n",
      "[[116  67]\n",
      " [ 45 139]]\n",
      "M_KAN Relevance: 0.014568538405001163\n",
      "CNN Relevance: 0.9854314923286438\n",
      "alpha: 0.5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.93e-01 | val_acc_hybrid: 6.53e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1769e-01 at 8 epoch\n",
      "tensor(0.7084, device='cuda:0')\n",
      "[[130  53]\n",
      " [ 54 130]]\n",
      "M_KAN Relevance: 0.01920514740049839\n",
      "CNN Relevance: 0.9807949066162109\n",
      "alpha: 0.4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.63e-01 | val_acc_hybrid: 6.22e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1429e-01 at 6 epoch\n",
      "tensor(0.7112, device='cuda:0')\n",
      "[[133  50]\n",
      " [ 56 128]]\n",
      "M_KAN Relevance: 0.021625559777021408\n",
      "CNN Relevance: 0.9783744215965271\n",
      "alpha: 0.3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 9.03e-01 | val_acc_hybrid: 5.95e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.0408e-01 at 4 epoch\n",
      "tensor(0.7221, device='cuda:0')\n",
      "[[137  46]\n",
      " [ 56 128]]\n",
      "M_KAN Relevance: 0.03119996003806591\n",
      "CNN Relevance: 0.9688000679016113\n",
      "alpha: 0.25\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.58e-01 | val_acc_hybrid: 6.63e-01 |: 100%|█████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1429e-01 at 13 epoch\n",
      "tensor(0.7275, device='cuda:0')\n",
      "[[131  52]\n",
      " [ 48 136]]\n",
      "M_KAN Relevance: 0.012129932641983032\n",
      "CNN Relevance: 0.9878700375556946\n",
      "alpha: 0.2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.36e-01 | val_acc_hybrid: 6.84e-01 |: 100%|█████| 60/60 [00:33<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2449e-01 at 23 epoch\n",
      "tensor(0.7003, device='cuda:0')\n",
      "[[128  55]\n",
      " [ 55 129]]\n",
      "M_KAN Relevance: 0.01939028687775135\n",
      "CNN Relevance: 0.9806097149848938\n",
      "alpha: 0.1\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.68e-01 | val_acc_hybrid: 6.53e-01 |: 100%|█████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1429e-01 at 14 epoch\n",
      "tensor(0.6975, device='cuda:0')\n",
      "[[136  47]\n",
      " [ 64 120]]\n",
      "M_KAN Relevance: 0.03770007938146591\n",
      "CNN Relevance: 0.9622999429702759\n",
      "alpha: 0.05\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.56e-01 | val_acc_hybrid: 6.02e-01 |: 100%|█████| 60/60 [00:33<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2449e-01 at 2 epoch\n",
      "tensor(0.7057, device='cuda:0')\n",
      "[[130  53]\n",
      " [ 55 129]]\n",
      "M_KAN Relevance: 0.10377513617277145\n",
      "CNN Relevance: 0.8962247967720032\n",
      "alpha: 0.01\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.97e-01 | val_acc_hybrid: 6.56e-01 |: 100%|█████| 60/60 [00:32<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2449e-01 at 1 epoch\n",
      "tensor(0.6948, device='cuda:0')\n",
      "[[106  77]\n",
      " [ 35 149]]\n",
      "M_KAN Relevance: 0.015258121304214\n",
      "CNN Relevance: 0.9847419261932373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------ kan_neurons=12, kan_grid=8, lamb=0.001 ------------------------------\")\n",
    "for alpha in [.9,.8,.75,.7,.6,.5,.4,.3,.25,.2,.1,.05,.01]:\n",
    "    print(f\"alpha: {alpha}\")\n",
    "    train_and_plot_relevance(Model4_2, kan_neurons=12, kan_grid=8, lamb=0.001, steps=120, \n",
    "                             alpha=alpha, filename=filename_2, opt_col_val=alpha)\n",
    "\n",
    "print(\"------------------------------ kan_neurons=6, kan_grid=7, lamb=1e-05 ------------------------------\")\n",
    "for alpha in [.9,.8,.75,.7,.6,.5,.4,.3,.25,.2,.1,.05,.01]:\n",
    "    print(f\"alpha: {alpha}\")\n",
    "    train_and_plot_relevance(Model4_2, kan_neurons=6, kan_grid=7, lamb=1e-05, steps=120, \n",
    "                             alpha=alpha, filename=filename_2, opt_col_val=alpha)\n",
    "\n",
    "print(\"------------------------------ kan_neurons=8, kan_grid=8, lamb=0.001 ------------------------------\")\n",
    "for alpha in [.9,.8,.75,.7,.6,.5,.4,.3,.25,.2,.1,.05,.01]:\n",
    "    print(f\"alpha: {alpha}\")\n",
    "    train_and_plot_relevance(Model4_2, kan_neurons=8, kan_grid=8, lamb=0.001, steps=120, \n",
    "                             alpha=alpha, filename=filename_2, opt_col_val=alpha)\n",
    "\n",
    "print(\"------------------------------ kan_neurons=3, kan_grid=7, lamb=0.001 ------------------------------\")\n",
    "for alpha in [.9,.8,.75,.7,.6,.5,.4,.3,.25,.2,.1,.05,.01]:\n",
    "    print(f\"alpha: {alpha}\")\n",
    "    train_and_plot_relevance(Model4_2, kan_neurons=3, kan_grid=7, lamb=0.001, steps=120, \n",
    "                             alpha=alpha, filename=filename_2, opt_col_val=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065d450-ce5f-48e1-866f-2c20dbf78c5e",
   "metadata": {},
   "source": [
    "# Option 3: Dynamic factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46050930-e94c-431b-b7f7-abcb22f2c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_with_header(filename_3, columns_opt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5f2dc1a-dede-4822-9191-08a94313d345",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ kan_neurons=3, kan_grid=7, lamb=1e-06 ------------------------------\n",
      "hidden_dim: 128\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.95e-01 | val_acc_hybrid: 6.60e-01 |: 100%|█████| 70/70 [00:41<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1769e-01 at 4 epoch\n",
      "tensor(0.7139, device='cuda:0')\n",
      "[[127  56]\n",
      " [ 49 135]]\n",
      "M_KAN Relevance: 2.2801490558777004e-05\n",
      "CNN Relevance: 0.999977171421051\n",
      "hidden_dim: 64\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.08e-01 | val_acc_hybrid: 6.77e-01 |: 100%|█████| 70/70 [00:42<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2109e-01 at 4 epoch\n",
      "tensor(0.7193, device='cuda:0')\n",
      "[[133  50]\n",
      " [ 53 131]]\n",
      "M_KAN Relevance: 0.0012987710069864988\n",
      "CNN Relevance: 0.9987012147903442\n",
      "hidden_dim: 32\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.34e-01 | val_acc_hybrid: 6.22e-01 |: 100%|█████| 70/70 [00:41<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2109e-01 at 1 epoch\n",
      "tensor(0.7057, device='cuda:0')\n",
      "[[126  57]\n",
      " [ 51 133]]\n",
      "M_KAN Relevance: 0.0003959068562835455\n",
      "CNN Relevance: 0.9996040463447571\n",
      "hidden_dim: 16\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.73e-01 | val_acc_hybrid: 6.70e-01 |: 100%|█████| 70/70 [00:40<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3810e-01 at 10 epoch\n",
      "tensor(0.7193, device='cuda:0')\n",
      "[[134  49]\n",
      " [ 54 130]]\n",
      "M_KAN Relevance: 0.001716263359412551\n",
      "CNN Relevance: 0.9982837438583374\n",
      "hidden_dim: 8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 0.00e+00 | val_acc_hybrid: 0.00e+00 |: 100%|█████| 70/70 [00:42<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2449e-01 at 4 epoch\n",
      "tensor(0.7357, device='cuda:0')\n",
      "[[132  51]\n",
      " [ 46 138]]\n",
      "M_KAN Relevance: 0.0\n",
      "CNN Relevance: 1.0\n",
      "------------------------------ kan_neurons=5, kan_grid=7, lamb=0.0001 ------------------------------\n",
      "hidden_dim: 128\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.06e-01 | val_acc_hybrid: 6.90e-01 |: 100%|█████| 70/70 [00:41<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3129e-01 at 23 epoch\n",
      "tensor(0.7166, device='cuda:0')\n",
      "[[133  50]\n",
      " [ 54 130]]\n",
      "M_KAN Relevance: 0.00813452061265707\n",
      "CNN Relevance: 0.9918654561042786\n",
      "hidden_dim: 64\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.30e-01 | val_acc_hybrid: 6.63e-01 |: 100%|█████| 70/70 [00:40<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1769e-01 at 11 epoch\n",
      "tensor(0.7221, device='cuda:0')\n",
      "[[133  50]\n",
      " [ 52 132]]\n",
      "M_KAN Relevance: 0.005971659906208515\n",
      "CNN Relevance: 0.9940283298492432\n",
      "hidden_dim: 32\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.95e-01 | val_acc_hybrid: 6.97e-01 |: 100%|█████| 70/70 [00:42<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3469e-01 at 33 epoch\n",
      "tensor(0.7193, device='cuda:0')\n",
      "[[141  42]\n",
      " [ 61 123]]\n",
      "M_KAN Relevance: 0.006340572610497475\n",
      "CNN Relevance: 0.9936594367027283\n",
      "hidden_dim: 16\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.36e-01 | val_acc_hybrid: 6.53e-01 |: 100%|█████| 70/70 [00:40<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1769e-01 at 4 epoch\n",
      "tensor(0.7357, device='cuda:0')\n",
      "[[133  50]\n",
      " [ 47 137]]\n",
      "M_KAN Relevance: 0.006724129896610975\n",
      "CNN Relevance: 0.9932758808135986\n",
      "hidden_dim: 8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.76e-01 | val_acc_hybrid: 6.50e-01 |: 100%|█████| 70/70 [00:40<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2109e-01 at 10 epoch\n",
      "tensor(0.7302, device='cuda:0')\n",
      "[[135  48]\n",
      " [ 51 133]]\n",
      "M_KAN Relevance: 0.008050317876040936\n",
      "CNN Relevance: 0.9919496774673462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------ kan_neurons=12, kan_grid=8, lamb=0.001 ------------------------------\")\n",
    "for hidden_dim in [128, 64, 32, 16, 8]:\n",
    "    print(f\"hidden_dim: {hidden_dim}\")\n",
    "    train_and_plot_relevance(Model4_3, kan_neurons=12, kan_grid=8, lamb=0.001, steps=150, \n",
    "                             hidden_dim=hidden_dim, filename=filename_3, opt_col_val=hidden_dim)\n",
    "\n",
    "print(\"------------------------------ kan_neurons=6, kan_grid=7, lamb=1e-05 ------------------------------\")\n",
    "for hidden_dim in [128, 64, 32, 16, 8]:\n",
    "    print(f\"hidden_dim: {hidden_dim}\")\n",
    "    train_and_plot_relevance(Model4_3, kan_neurons=6, kan_grid=7, lamb=1e-05, steps=150, \n",
    "                             hidden_dim=hidden_dim, filename=filename_3, opt_col_val=hidden_dim)\n",
    "\"Hybrid3\n",
    "cnn_blocks=2\"\n",
    "0.96153\n",
    "60\n",
    "\"width=[24, 11], \n",
    "grid=3, \n",
    "lamb=0.001\"\n",
    "\n",
    "print(\"------------------------------ kan_neurons=8, kan_grid=8, lamb=0.001 ------------------------------\")\n",
    "for hidden_dim in [128, 64, 32, 16, 8]:\n",
    "    print(f\"hidden_dim: {hidden_dim}\")\n",
    "    train_and_plot_relevance(Model4_3, kan_neurons=8, kan_grid=8, lamb=0.001, steps=150, \n",
    "                             hidden_dim=hidden_dim, filename=filename_3, opt_col_val=hidden_dim)\n",
    "\n",
    "print(\"------------------------------ kan_neurons=3, kan_grid=7, lamb=0.001 ------------------------------\")\n",
    "for hidden_dim in [128, 64, 32, 16, 8]:\n",
    "    print(f\"hidden_dim: {hidden_dim}\")\n",
    "    train_and_plot_relevance(Model4_3, kan_neurons=3, kan_grid=7, lamb=0.001, steps=150, \n",
    "                             hidden_dim=hidden_dim, filename=filename_3, opt_col_val=hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3977ed-3ba9-42b4-aedc-258f899e85a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Opt4: MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9631924d-a0a7-4742-b4ef-3642cf6f56d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_csv_with_header(filename_4, columns_opt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51ab61bc-b2d3-434e-a0a2-87f9e0756683",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ kan_neurons=3, kan_grid=7, lamb=1e-06 ------------------------------\n",
      "embed_dim: 64, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.44e-01 | val_acc_hybrid: 7.07e-01 |: 100%|█████| 90/90 [00:29<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.4150e-01 at 8 epoch\n",
      "tensor(0.7084, device='cuda:0')\n",
      "[[121  62]\n",
      " [ 45 139]]\n",
      "M_KAN Relevance: 0.08535192161798477\n",
      "CNN Relevance: 0.9146481156349182\n",
      "embed_dim: 64, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.44e-01 | val_acc_hybrid: 7.07e-01 |: 100%|█████| 90/90 [00:29<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.4150e-01 at 8 epoch\n",
      "tensor(0.7084, device='cuda:0')\n",
      "[[121  62]\n",
      " [ 45 139]]\n",
      "M_KAN Relevance: 0.08535192161798477\n",
      "CNN Relevance: 0.9146481156349182\n",
      "embed_dim: 64, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.44e-01 | val_acc_hybrid: 7.07e-01 |: 100%|█████| 90/90 [00:29<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.4150e-01 at 8 epoch\n",
      "tensor(0.7084, device='cuda:0')\n",
      "[[121  62]\n",
      " [ 45 139]]\n",
      "M_KAN Relevance: 0.08535192161798477\n",
      "CNN Relevance: 0.9146481156349182\n",
      "embed_dim: 32, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.42e-01 | val_acc_hybrid: 6.77e-01 |: 100%|█████| 90/90 [00:29<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.0748e-01 at 3 epoch\n",
      "tensor(0.6785, device='cuda:0')\n",
      "[[141  42]\n",
      " [ 76 108]]\n",
      "M_KAN Relevance: 0.09611522406339645\n",
      "CNN Relevance: 0.903884768486023\n",
      "embed_dim: 32, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.56e-01 | val_acc_hybrid: 6.97e-01 |: 100%|█████| 90/90 [00:30<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2109e-01 at 44 epoch\n",
      "tensor(0.7139, device='cuda:0')\n",
      "[[131  52]\n",
      " [ 53 131]]\n",
      "M_KAN Relevance: 0.15937860310077667\n",
      "CNN Relevance: 0.8406214118003845\n",
      "embed_dim: 32, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.80e-01 | val_acc_hybrid: 6.84e-01 |: 100%|█████| 90/90 [00:28<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3129e-01 at 4 epoch\n",
      "tensor(0.7166, device='cuda:0')\n",
      "[[121  62]\n",
      " [ 42 142]]\n",
      "M_KAN Relevance: 0.04014917090535164\n",
      "CNN Relevance: 0.9598508477210999\n",
      "embed_dim: 16, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.04e-01 | val_acc_hybrid: 6.77e-01 |: 100%|█████| 90/90 [00:28<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2789e-01 at 3 epoch\n",
      "tensor(0.6975, device='cuda:0')\n",
      "[[124  59]\n",
      " [ 52 132]]\n",
      "M_KAN Relevance: 0.27207615971565247\n",
      "CNN Relevance: 0.7279238104820251\n",
      "embed_dim: 16, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.61e-01 | val_acc_hybrid: 6.90e-01 |: 100%|█████| 90/90 [00:28<00:00,  3.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.4490e-01 at 4 epoch\n",
      "tensor(0.7166, device='cuda:0')\n",
      "[[129  54]\n",
      " [ 50 134]]\n",
      "M_KAN Relevance: 0.2094883769750595\n",
      "CNN Relevance: 0.7905116081237793\n",
      "embed_dim: 16, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.04e-01 | val_acc_hybrid: 6.77e-01 |: 100%|█████| 90/90 [00:28<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2789e-01 at 3 epoch\n",
      "tensor(0.6975, device='cuda:0')\n",
      "[[124  59]\n",
      " [ 52 132]]\n",
      "M_KAN Relevance: 0.27207615971565247\n",
      "CNN Relevance: 0.7279238104820251\n",
      "embed_dim: 48, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.65e-01 | val_acc_hybrid: 7.11e-01 |: 100%|█████| 90/90 [00:29<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2449e-01 at 28 epoch\n",
      "tensor(0.7221, device='cuda:0')\n",
      "[[133  50]\n",
      " [ 52 132]]\n",
      "M_KAN Relevance: 0.02622860297560692\n",
      "CNN Relevance: 0.9737713932991028\n",
      "embed_dim: 24, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.74e-01 | val_acc_hybrid: 6.97e-01 |: 100%|█████| 90/90 [00:29<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3469e-01 at 3 epoch\n",
      "tensor(0.7030, device='cuda:0')\n",
      "[[120  63]\n",
      " [ 46 138]]\n",
      "M_KAN Relevance: 0.08549175411462784\n",
      "CNN Relevance: 0.9145082831382751\n",
      "embed_dim: 12, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.70e-01 | val_acc_hybrid: 6.97e-01 |: 100%|█████| 90/90 [00:28<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3469e-01 at 2 epoch\n",
      "tensor(0.7112, device='cuda:0')\n",
      "[[128  55]\n",
      " [ 51 133]]\n",
      "M_KAN Relevance: 0.02661152556538582\n",
      "CNN Relevance: 0.9733884930610657\n",
      "------------------------------ kan_neurons=5, kan_grid=7, lamb=0.0001 ------------------------------\n",
      "embed_dim: 64, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.67e-01 | val_acc_hybrid: 7.14e-01 |: 100%|█████| 90/90 [00:28<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3810e-01 at 5 epoch\n",
      "tensor(0.7139, device='cuda:0')\n",
      "[[121  62]\n",
      " [ 43 141]]\n",
      "M_KAN Relevance: 0.07354620099067688\n",
      "CNN Relevance: 0.9264537692070007\n",
      "embed_dim: 64, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.65e-01 | val_acc_hybrid: 7.18e-01 |: 100%|█████| 90/90 [00:29<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3129e-01 at 8 epoch\n",
      "tensor(0.7112, device='cuda:0')\n",
      "[[120  63]\n",
      " [ 43 141]]\n",
      "M_KAN Relevance: 0.02184392884373665\n",
      "CNN Relevance: 0.9781560897827148\n",
      "embed_dim: 64, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.67e-01 | val_acc_hybrid: 7.14e-01 |: 100%|█████| 90/90 [00:28<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3810e-01 at 5 epoch\n",
      "tensor(0.7139, device='cuda:0')\n",
      "[[121  62]\n",
      " [ 43 141]]\n",
      "M_KAN Relevance: 0.07354620099067688\n",
      "CNN Relevance: 0.9264537692070007\n",
      "embed_dim: 32, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.67e-01 | val_acc_hybrid: 7.07e-01 |: 100%|█████| 90/90 [00:28<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.4490e-01 at 8 epoch\n",
      "tensor(0.7193, device='cuda:0')\n",
      "[[130  53]\n",
      " [ 50 134]]\n",
      "M_KAN Relevance: 0.1849890798330307\n",
      "CNN Relevance: 0.8150109052658081\n",
      "embed_dim: 32, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.82e-01 | val_acc_hybrid: 7.01e-01 |: 100%|█████| 90/90 [00:28<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3469e-01 at 2 epoch\n",
      "tensor(0.7193, device='cuda:0')\n",
      "[[126  57]\n",
      " [ 46 138]]\n",
      "M_KAN Relevance: 0.2563580870628357\n",
      "CNN Relevance: 0.7436418533325195\n",
      "embed_dim: 32, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.82e-01 | val_acc_hybrid: 7.01e-01 |: 100%|█████| 90/90 [00:28<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3469e-01 at 2 epoch\n",
      "tensor(0.7193, device='cuda:0')\n",
      "[[126  57]\n",
      " [ 46 138]]\n",
      "M_KAN Relevance: 0.2563580870628357\n",
      "CNN Relevance: 0.7436418533325195\n",
      "embed_dim: 16, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.92e-01 | val_acc_hybrid: 6.84e-01 |: 100%|█████| 90/90 [00:29<00:00,  3.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2789e-01 at 7 epoch\n",
      "tensor(0.7330, device='cuda:0')\n",
      "[[134  49]\n",
      " [ 49 135]]\n",
      "M_KAN Relevance: 0.23544730246067047\n",
      "CNN Relevance: 0.7645527124404907\n",
      "embed_dim: 16, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.60e-01 | val_acc_hybrid: 6.50e-01 |: 100%|█████| 90/90 [00:30<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.0748e-01 at 2 epoch\n",
      "tensor(0.6703, device='cuda:0')\n",
      "[[105  78]\n",
      " [ 43 141]]\n",
      "M_KAN Relevance: 0.3717891275882721\n",
      "CNN Relevance: 0.6282108426094055\n",
      "embed_dim: 16, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.73e-01 | val_acc_hybrid: 6.87e-01 |: 100%|█████| 90/90 [00:29<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.1769e-01 at 5 epoch\n",
      "tensor(0.6812, device='cuda:0')\n",
      "[[114  69]\n",
      " [ 48 136]]\n",
      "M_KAN Relevance: 0.47175654768943787\n",
      "CNN Relevance: 0.5282434225082397\n",
      "embed_dim: 48, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 8.06e-01 | val_acc_hybrid: 7.04e-01 |: 100%|█████| 90/90 [00:28<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.3810e-01 at 3 epoch\n",
      "tensor(0.7221, device='cuda:0')\n",
      "[[127  56]\n",
      " [ 46 138]]\n",
      "M_KAN Relevance: 0.07518815994262695\n",
      "CNN Relevance: 0.924811840057373\n",
      "embed_dim: 24, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.80e-01 | val_acc_hybrid: 6.80e-01 |: 100%|█████| 90/90 [00:28<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.2449e-01 at 3 epoch\n",
      "tensor(0.7248, device='cuda:0')\n",
      "[[126  57]\n",
      " [ 44 140]]\n",
      "M_KAN Relevance: 0.3621828854084015\n",
      "CNN Relevance: 0.6378171443939209\n",
      "embed_dim: 12, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " train_acc_hybrid: 7.71e-01 | val_acc_hybrid: 6.97e-01 |: 100%|█████| 90/90 [00:28<00:00,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation Accuracy: 7.4490e-01 at 5 epoch\n",
      "tensor(0.7275, device='cuda:0')\n",
      "[[131  52]\n",
      " [ 48 136]]\n",
      "M_KAN Relevance: 0.42902079224586487\n",
      "CNN Relevance: 0.5709791779518127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------ kan_neurons=12, kan_grid=8, lamb=0.001 ------------------------------\")\n",
    "for embed_dim in [64, 32, 16]:\n",
    "    for num_head in [2, 4 , 8]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model4_4, kan_neurons=12, kan_grid=8, lamb=0.001, steps=180, \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "\n",
    "for embed_dim in [48, 24, 12]:\n",
    "    for num_head in [6]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model4_4, kan_neurons=12, kan_grid=8, lamb=0.001, steps=180,  \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "\n",
    "print(\"------------------------------ kan_neurons=6, kan_grid=7, lamb=1e-05 ------------------------------\")\n",
    "for embed_dim in [64, 32, 16]:\n",
    "    for num_head in [2, 4 , 8]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model4_4, kan_neurons=6, kan_grid=7, lamb=1e-05, steps=180, \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "\n",
    "for embed_dim in [48, 24, 12]:\n",
    "    for num_head in [6]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model4_4, kan_neurons=6, kan_grid=7, lamb=1e-05, steps=180,  \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "\n",
    "\n",
    "print(\"------------------------------ kan_neurons=8, kan_grid=8, lamb=0.001 ------------------------------\")\n",
    "for embed_dim in [64, 32, 16]:\n",
    "    for num_head in [2, 4 , 8]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model4_4, kan_neurons=8, kan_grid=8, lamb=0.001, steps=180, \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "\n",
    "for embed_dim in [48, 24, 12]:\n",
    "    for num_head in [6]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model4_4, kan_neurons=8, kan_grid=8, lamb=0.001, steps=180,  \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "\n",
    "\n",
    "print(\"------------------------------ kan_neurons=3, kan_grid=7, lamb=0.001 ------------------------------\")\n",
    "for embed_dim in [64, 32, 16]:\n",
    "    for num_head in [2, 4 , 8]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model4_4, kan_neurons=3, kan_grid=7, lamb=0.001, steps=180, \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "\n",
    "for embed_dim in [48, 24, 12]:\n",
    "    for num_head in [6]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model4_4, kan_neurons=3, kan_grid=7, lamb=0.001, steps=180,  \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e90a0b-f426-4a87-9bdb-3ecaffa74351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
