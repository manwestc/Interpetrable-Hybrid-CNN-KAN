{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d5668d4-717f-4e0a-a900-f1b70dc8ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the root directory\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Add the root directory to the Python path\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4dace8b6-da36-4ed7-94f9-e7bfde57e588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from TINTOlib.tinto import TINTO\n",
    "from kan import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import traceback\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "import traceback\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "#from torch.optim import LBFGS\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787a631d-2f1e-4b05-845e-bb878447f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 381\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b71d2a5-d2ad-4b72-9869-8fdae3a98141",
   "metadata": {},
   "source": [
    "# BEST 3.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1580c162-4015-4cda-8492-b51350055b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=\"data/treasury\"\n",
    "x_col=[\"1Y-CMaturityRate\", \"30Y-CMortgageRate\", \"3M-Rate-AuctionAverage\", \"3M-Rate-SecondaryMarket\", \"3Y-CMaturityRate\", \n",
    "       \"5Y-CMaturityRate\", \"bankCredit\", \"currency\", \"demandDeposits\", \"federalFunds\", \"moneyStock\", \"checkableDeposits\", \n",
    "       \"loansLeases\", \"savingsDeposits\", \"tradeCurrencies\"]\n",
    "\n",
    "target_col=[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b4aeb",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e61699b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load Dataset and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb5e9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean(npy_filename, y_filename, x_col, target_col):\n",
    "    \"\"\"\n",
    "    Load the feature array (npy_filename) and label array (y_filename),\n",
    "    drop rows in the feature array that contain any NaNs, and apply\n",
    "    the same mask to the label array.\n",
    "    \"\"\"\n",
    "    # Load numpy arrays\n",
    "    X = np.load(os.path.join(folder, npy_filename))\n",
    "    y = np.load(os.path.join(folder, y_filename))\n",
    "    \n",
    "    # Ensure the number of rows matches between X and y\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"The number of rows in {} and {} do not match.\".format(npy_filename, y_filename))\n",
    "    \n",
    "    # Create a boolean mask for rows that do NOT have any NaN values in X\n",
    "    valid_rows = ~np.isnan(X).any(axis=1)\n",
    "    #print(valid_rows)\n",
    "    # Filter both arrays using the valid_rows mask\n",
    "    X_clean = X[valid_rows]\n",
    "    y_clean = y[valid_rows]\n",
    "    \n",
    "    # Convert arrays to DataFrames\n",
    "    df_X = pd.DataFrame(X_clean)\n",
    "    df_y = pd.DataFrame(y_clean)\n",
    "    df_X.columns = x_col\n",
    "    df_y.columns = target_col\n",
    "\n",
    "    return df_X, df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6027ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(X_train, y_train, X_test, y_test, X_val, y_val, image_model, problem_type, batch_size=32):\n",
    "    # Add target column to input for IGTD\n",
    "    X_train_full = X_train.copy()\n",
    "    X_train_full[\"target\"] = y_train.values\n",
    "\n",
    "    X_val_full = X_val.copy()\n",
    "    X_val_full[\"target\"] = y_val.values\n",
    "\n",
    "    X_test_full = X_test.copy()\n",
    "    X_test_full[\"target\"] = y_test.values\n",
    "\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(f'{images_folder}/train'):\n",
    "        image_model.fit_transform(X_train_full, f'{images_folder}/train')\n",
    "        image_model.saveHyperparameters(f'{images_folder}/model.pkl')\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    # Load image paths\n",
    "    imgs_train = pd.read_csv(os.path.join(f'{images_folder}/train', f'{problem_type}.csv'))\n",
    "    imgs_train[\"images\"] = images_folder + \"/train/\" + imgs_train[\"images\"]\n",
    "\n",
    "    if not os.path.exists(f'{images_folder}/val'):\n",
    "        image_model.transform(X_val_full, f'{images_folder}/val')\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    imgs_val = pd.read_csv(os.path.join(f'{images_folder}/val', f'{problem_type}.csv'))\n",
    "    imgs_val[\"images\"] = images_folder + \"/val/\" + imgs_val[\"images\"]\n",
    "\n",
    "    if not os.path.exists(f'{images_folder}/test'):\n",
    "        image_model.transform(X_test_full, f'{images_folder}/test')\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    imgs_test = pd.read_csv(os.path.join(f'{images_folder}/test', f'{problem_type}.csv'))\n",
    "    imgs_test[\"images\"] = images_folder + \"/test/\" + imgs_test[\"images\"]\n",
    "\n",
    "    # Image data\n",
    "    X_train_img = np.array([cv2.imread(img) for img in imgs_train[\"images\"]])\n",
    "    X_val_img = np.array([cv2.imread(img) for img in imgs_val[\"images\"]])\n",
    "    X_test_img = np.array([cv2.imread(img) for img in imgs_test[\"images\"]])\n",
    "\n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale numerical data\n",
    "    X_train_num = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_val_num = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n",
    "    X_test_num = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "    attributes = len(X_train_num.columns)\n",
    "    height, width, channels = X_train_img[0].shape\n",
    "    imgs_shape = (channels, height, width)\n",
    "\n",
    "    print(\"Images shape: \", imgs_shape)\n",
    "    print(\"Attributes: \", attributes)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_num_tensor = torch.as_tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor = torch.as_tensor(X_val_num.values, dtype=torch.float32)\n",
    "    X_test_num_tensor = torch.as_tensor(X_test_num.values, dtype=torch.float32)\n",
    "    X_train_img_tensor = torch.as_tensor(X_train_img, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n",
    "    X_val_img_tensor = torch.as_tensor(X_val_img, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n",
    "    X_test_img_tensor = torch.as_tensor(X_test_img, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n",
    "    y_train_tensor = torch.as_tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_val_tensor = torch.as_tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_test_tensor = torch.as_tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_num_tensor, X_train_img_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_num_tensor, X_val_img_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_num_tensor, X_test_img_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes, imgs_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2285bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_loader(loader):\n",
    "    \"\"\"\n",
    "    Combines all batches from a DataLoader into three tensors.\n",
    "    Assumes each batch is a tuple: (mlp_tensor, img_tensor, target_tensor)\n",
    "    \"\"\"\n",
    "    mlp_list, img_list, target_list = [], [], []\n",
    "    for mlp, img, target in loader:\n",
    "        mlp_list.append(mlp)\n",
    "        img_list.append(img)\n",
    "        target_list.append(target)\n",
    "    return torch.cat(mlp_list, dim=0), torch.cat(img_list, dim=0), torch.cat(target_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e1e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_rmse(output, val_target):\n",
    "    \"\"\"\n",
    "    Computes the root mean squared error (RMSE) between output and val_target.\n",
    "\n",
    "    Args:\n",
    "        output (torch.Tensor): The predicted output tensor.\n",
    "        val_target (torch.Tensor): The ground truth tensor.\n",
    "    \n",
    "    Returns:\n",
    "        float: The RMSE value.\n",
    "    \"\"\"\n",
    "    mse = torch.mean((output - val_target) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    return rmse.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d1d3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Functions for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46156820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cnn_only_model(model, dataset, steps=100, lr=1.0, loss_fn=None, batch=-1, opt=\"LBFGS\"):\n",
    "    \"\"\"\n",
    "    Trains a CNN-only model using LBFGS.\n",
    "\n",
    "    Args:\n",
    "        model: CNN-only PyTorch model.\n",
    "        dataset: Dictionary with keys: 'train_img', 'train_label', 'val_img', 'val_label'.\n",
    "        steps: Number of training iterations.\n",
    "        lr: Learning rate.\n",
    "        loss_fn: Loss function. Defaults to MSE.\n",
    "\n",
    "    Returns:\n",
    "        results: Dict with lists of train/val losses.\n",
    "        best_model_state: Best weights based on val loss.\n",
    "    \"\"\"\n",
    "    device = model.device\n",
    "    if loss_fn is None:\n",
    "        loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Optimizer selection\n",
    "    if opt == \"LBFGS\":\n",
    "        optimizer = LBFGS(model.parameters(), lr=lr, history_size=10, \n",
    "                          line_search_fn=\"strong_wolfe\", \n",
    "                          tolerance_grad=1e-32, \n",
    "                          tolerance_change=1e-32, \n",
    "                          tolerance_ys=1e-32)\n",
    "    elif opt == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer '{opt}'\")\n",
    "\n",
    "    n_train = dataset[\"train_img\"].shape[0]\n",
    "    n_val = dataset[\"val_img\"].shape[0]\n",
    "    batch_size = n_train if batch == -1 or batch > n_train else batch\n",
    "\n",
    "    results = {'train_loss': [], 'val_loss': []}\n",
    "    best_model_state = None\n",
    "    best_loss = float(\"inf\")\n",
    "    best_epoch = -1\n",
    "\n",
    "    pbar = tqdm(range(steps), desc=\"Training CNN Only ({opt})\", ncols=100)\n",
    "\n",
    "    for step in pbar:\n",
    "        train_idx = np.random.choice(n_train, batch_size, replace=False)\n",
    "        #train_idx = torch.randperm(n_train)[:min(32, n_train)]  # small batch\n",
    "        x_train = dataset[\"train_img\"][train_idx].to(device)\n",
    "        y_train = dataset[\"train_label\"][train_idx].to(device)\n",
    "        if opt == \"LBFGS\":\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                output = model(0, x_train)\n",
    "                loss = loss_fn(output, y_train)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "            train_loss = closure().item()\n",
    "\n",
    "        else:  # AdamW\n",
    "            optimizer.zero_grad()\n",
    "            output = model(0, x_train)\n",
    "            loss = loss_fn(output, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_output = model(0, dataset[\"val_img\"].to(device))\n",
    "            val_loss = loss_fn(val_output, dataset[\"val_label\"].to(device)).item()\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_epoch = step\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "        pbar.set_description(f\"| Train: {train_loss:.4e} | Val: {val_loss:.4e} |\")\n",
    "    \n",
    "    best_loss = math.sqrt(best_loss)\n",
    "    print(f\"✅ Best validation loss: {best_loss:.4e} at {best_epoch} epoch\")\n",
    "    return best_model_state, results, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5abb1ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_custom_cnn_model(cnn_blocks, dense_layers, imgs_shape, device='cuda'):\n",
    "    class CustomCNNModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CustomCNNModel, self).__init__()\n",
    "            self.device = device\n",
    "\n",
    "            cnn_layers = []\n",
    "            in_channels = imgs_shape[0]\n",
    "            out_channels = 16\n",
    "            cnn_blocks_list = [22, 13, 8]\n",
    "            size_layer_norm = cnn_blocks_list[cnn_blocks-1]\n",
    "            \n",
    "            f_layer_size = 10 - cnn_blocks*2\n",
    "            for i in range(cnn_blocks):\n",
    "                cnn_layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=2))\n",
    "                \n",
    "                if i < cnn_blocks - 1:\n",
    "                    cnn_layers.append(nn.BatchNorm2d(out_channels))\n",
    "                    cnn_layers.append(nn.ReLU())\n",
    "                    cnn_layers.append(nn.MaxPool2d(2))\n",
    "                else:\n",
    "                    # Last block: LayerNorm + Sigmoid + Flatten\n",
    "                    cnn_layers.append(nn.LayerNorm([out_channels, size_layer_norm, size_layer_norm]))\n",
    "                    cnn_layers.append(nn.Sigmoid())\n",
    "                    cnn_layers.append(nn.Flatten())\n",
    "                in_channels = out_channels\n",
    "                out_channels *= 2\n",
    "\n",
    "            self.cnn_branch = nn.Sequential(*cnn_layers).to(device)\n",
    "            self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "            # Dense (FC) layers\n",
    "            fc_layers = []\n",
    "            input_dim = self.flat_size\n",
    "            for i in range(dense_layers - 1):\n",
    "                fc_layers.append(nn.Linear(int(input_dim), int(input_dim // 2)))\n",
    "                fc_layers.append(nn.ReLU())\n",
    "                input_dim = input_dim // 2\n",
    "            fc_layers.append(nn.Linear(int(input_dim), 1))\n",
    "\n",
    "            self.fc = nn.Sequential(*fc_layers).to(device)\n",
    "\n",
    "        def _get_flat_size(self, imgs_shape):\n",
    "            dummy_input = torch.zeros(1, *imgs_shape, device=self.device)\n",
    "            x = self.cnn_branch(dummy_input)\n",
    "            return x.shape[1]\n",
    "\n",
    "        def forward(self, num_input, img_input):\n",
    "            img_input = img_input.to(self.device)\n",
    "            features = self.cnn_branch(img_input)\n",
    "            output = self.fc(features)\n",
    "            return output\n",
    "\n",
    "    return CustomCNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5dc5e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_cnn(dataset, steps=110, opt='LBFGS', batch=-1, filename=None):\n",
    "    cnn_blocks_options = list(range(1, 4))\n",
    "    dense_layers_options = list(range(1, 4))\n",
    "\n",
    "    top_models = []  # list of dicts: each will have 'loss', 'config', 'state_dict'\n",
    "\n",
    "    for cnn_blocks in cnn_blocks_options:\n",
    "        for dense_layers in dense_layers_options:\n",
    "            print(f\"Testing cnn_blocks={cnn_blocks}, dense_layers={dense_layers}\")\n",
    "            model = build_custom_cnn_model(cnn_blocks, dense_layers, imgs_shape)\n",
    "            \n",
    "            # Custom training\n",
    "            model_state, _, best_epoch = fit_cnn_only_model(model, dataset, steps=steps, opt=opt, batch=batch)\n",
    "            model.load_state_dict(model_state)\n",
    "\n",
    "            # Evaluate\n",
    "            avg_loss = average_rmse(model(0, dataset['test_img']), dataset['test_label'])\n",
    "\n",
    "            # Save this model info\n",
    "            top_models.append({\n",
    "                \"loss\": avg_loss,\n",
    "                \"config\": {\"cnn_blocks\": cnn_blocks, \"dense_layers\": dense_layers},\n",
    "                #\"state_dict\": copy.deepcopy(model_state),\n",
    "                \"best_epoch\": best_epoch\n",
    "            })\n",
    "            append_row_to_csv(filename, cnn_blocks, dense_layers, '_', avg_loss, best_epoch)\n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            # Keep only top 5\n",
    "            top_models = sorted(top_models, key=lambda x: x[\"loss\"])[:5]\n",
    "\n",
    "    print(\"\\n✅ Top 5 Configurations:\")\n",
    "    for i, entry in enumerate(top_models):\n",
    "        cfg = entry[\"config\"]\n",
    "        print(f\"{i+1}. cnn_blocks={cfg['cnn_blocks']}, dense_layers={cfg['dense_layers']} | best_epoch={entry['best_epoch']} | loss={entry['loss']:.5f}\")\n",
    "\n",
    "    #return top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85028d5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hybrid Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "766c41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_hybrid_dataloaders(model, \n",
    "                           dataset,\n",
    "                           opt=\"AdamW\", \n",
    "                           steps=100, \n",
    "                           log=1, \n",
    "                           lamb=0., \n",
    "                           lamb_l1=1., \n",
    "                           lamb_entropy=2., \n",
    "                           lamb_coef=0., \n",
    "                           lamb_coefdiff=0., \n",
    "                           update_grid=True, \n",
    "                           grid_update_num=10, \n",
    "                           loss_fn=None, \n",
    "                           lr=1., \n",
    "                           start_grid_update_step=-1, \n",
    "                           stop_grid_update_step=50, \n",
    "                           batch=-1,\n",
    "                           metrics=None, \n",
    "                           save_fig=False, \n",
    "                           in_vars=None, \n",
    "                           out_vars=None, \n",
    "                           beta=3, \n",
    "                           save_fig_freq=1, \n",
    "                           img_folder='./video', \n",
    "                           singularity_avoiding=False, \n",
    "                           y_th=1000., \n",
    "                           reg_metric='edge_forward_spline_n', \n",
    "                           display_metrics=None,\n",
    "                           sum_f_reg=True):\n",
    "    \"\"\"\n",
    "    Trains the hybrid model (with a KAN branch and a CNN branch) using a steps-based loop\n",
    "    adapted from KAN.fit(), with grid updates and regularization.\n",
    "    \n",
    "    Instead of a single dataset dict, this function accepts three DataLoaders:\n",
    "        - train_loader: provides (mlp, img, target) for training\n",
    "        - val_loader: provides (mlp, img, target) for evaluation during training\n",
    "        - test_loader: provides (mlp, img, target) for validation\n",
    "\n",
    "    Internally, the function combines each loader into a dataset dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        results: dictionary containing training loss, evaluation loss, regularization values,\n",
    "                 and any additional metrics recorded during training.\n",
    "    \"\"\"\n",
    "    #device = next(model.parameters()).device\n",
    "\n",
    "    # Warn if regularization is requested but model's internal flag isn't enabled.\n",
    "    if lamb > 0. and not getattr(model.m_kan, \"save_act\", False):\n",
    "        print(\"setting lamb=0. If you want to set lamb > 0, set model.m_kan.save_act=True\")\n",
    "    \n",
    "    # Disable symbolic processing for training if applicable (KAN internal logic)\n",
    "    if hasattr(model.m_kan, \"disable_symbolic_in_fit\"):\n",
    "        old_save_act, old_symbolic_enabled = model.m_kan.disable_symbolic_in_fit(lamb)\n",
    "        f_old_save_act, f_old_symbolic_enabled = model.final_kan.disable_symbolic_in_fit(lamb)\n",
    "    else:\n",
    "        old_save_act, old_symbolic_enabled = None, None\n",
    "\n",
    "    pbar = tqdm(range(steps), desc='Training', ncols=100)\n",
    "\n",
    "    # Default loss function (mean squared error) if not provided\n",
    "    if loss_fn is None:\n",
    "        loss_fn = lambda x, y: torch.mean((x - y) ** 2)\n",
    "\n",
    "    # Determine grid update frequency\n",
    "    grid_update_freq = int(stop_grid_update_step / grid_update_num) if grid_update_num > 0 else 1\n",
    "\n",
    "    # Determine total number of training examples\n",
    "    n_train = dataset[\"train_input\"].shape[0]\n",
    "    n_eval  = dataset[\"val_input\"].shape[0]  # using val set for evaluation during training\n",
    "    batch_size = n_train if batch == -1 or batch > n_train else batch\n",
    "\n",
    "    # Set up optimizer: choose between Adam and LBFGS (removed tolerance_ys)\n",
    "    if opt == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    elif opt == \"LBFGS\":        \n",
    "        optimizer = LBFGS(model.parameters(), lr=lr, history_size=10, \n",
    "                          line_search_fn=\"strong_wolfe\", \n",
    "                          tolerance_grad=1e-32, \n",
    "                          tolerance_change=1e-32, \n",
    "                          tolerance_ys=1e-32)\n",
    "    else:\n",
    "        raise ValueError(\"Optimizer not recognized. Use 'Adam' or 'LBFGS'.\")\n",
    "\n",
    "    # Prepare results dictionary.\n",
    "    results = {'train_loss': [], 'eval_loss': [], 'reg': []}\n",
    "    \n",
    "    if metrics is not None:\n",
    "        for metric in metrics:\n",
    "            results[metric.__name__] = []\n",
    "\n",
    "    best_model_state = None\n",
    "    best_epoch = -1\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for step in pbar:\n",
    "        # Randomly sample indices for a mini-batch from the training set.\n",
    "        train_indices = np.random.choice(n_train, batch_size, replace=False)\n",
    "        # Use full evaluation set for evaluation; you can also sample if desired.\n",
    "        eval_indices = np.arange(n_eval)\n",
    "        \n",
    "        cached_loss = {}\n",
    "        # Closure for LBFGS\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            mlp_batch = dataset[\"train_input\"][train_indices]\n",
    "            img_batch = dataset[\"train_img\"][train_indices]\n",
    "            target_batch = dataset[\"train_label\"][train_indices]\n",
    "            outputs = model(mlp_batch, img_batch)\n",
    "            train_loss = loss_fn(outputs, target_batch)\n",
    "            # Compute regularization term if enabled.\n",
    "            if hasattr(model.m_kan, \"save_act\") and model.m_kan.save_act:\n",
    "                if reg_metric == 'edge_backward':\n",
    "                    model.m_kan.attribute()\n",
    "                    model.final_kan.attribute()\n",
    "                if reg_metric == 'node_backward':\n",
    "                    model.m_kan.node_attribute()\n",
    "                    model.final_kan.node_attribute()\n",
    "                reg_val_inner = model.m_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "                if sum_f_reg:\n",
    "                    reg_val_inner += model.final_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            else:\n",
    "                reg_val_inner = torch.tensor(0., device=device)\n",
    "            loss_val_inner = train_loss + lamb * reg_val_inner\n",
    "            loss_val_inner.backward()\n",
    "            cached_loss['loss'] = loss_val_inner.detach()\n",
    "            cached_loss['reg'] = reg_val_inner.detach()\n",
    "            return loss_val_inner\n",
    "\n",
    "        # Perform grid update if applicable.\n",
    "        if (step % grid_update_freq == 0 and step < stop_grid_update_step \n",
    "            and update_grid and step >= start_grid_update_step):\n",
    "            \n",
    "            mlp_batch = dataset['train_input'][train_indices]\n",
    "            cnn_batch = dataset['train_img'][train_indices]\n",
    "            \n",
    "            model.m_kan.update_grid(mlp_batch)\n",
    "            #cnn_output = model.cnn_branch(cnn_batch)  # Process image input\n",
    "            concatenated = model.get_concat_output(mlp_batch, cnn_batch)\n",
    "\n",
    "            model.final_kan.update_grid(concatenated)\n",
    "\n",
    "        # Perform an optimizer step.\n",
    "        if opt == \"LBFGS\":\n",
    "            optimizer.step(closure)\n",
    "            loss_val = cached_loss['loss']\n",
    "            reg_val = cached_loss['reg']\n",
    "        else:  # AdamW branch\n",
    "            optimizer.zero_grad()\n",
    "            mlp_batch = dataset[\"train_input\"][train_indices]\n",
    "            img_batch = dataset[\"train_img\"][train_indices]\n",
    "            target_batch = dataset[\"train_label\"][train_indices]\n",
    "            outputs = model(mlp_batch, img_batch)\n",
    "            train_loss = loss_fn(outputs, target_batch)\n",
    "            if hasattr(model.m_kan, \"save_act\") and model.m_kan.save_act:\n",
    "                if reg_metric == 'edge_backward':\n",
    "                    model.m_kan.attribute()\n",
    "                    model.final_kan.attribute()\n",
    "                if reg_metric == 'node_backward':\n",
    "                    model.m_kan.node_attribute()\n",
    "                    model.final_kan.node_attribute()\n",
    "                reg_val = model.m_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "                if sum_f_reg:\n",
    "                    reg_val = reg_val + model.final_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            else:\n",
    "                reg_val = torch.tensor(0., device=device)\n",
    "            loss_val = train_loss + lamb * reg_val\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mlp_eval = dataset[\"val_input\"][eval_indices]\n",
    "            img_eval = dataset[\"val_img\"][eval_indices]\n",
    "            target_eval = dataset[\"val_label\"][eval_indices]\n",
    "            eval_loss = loss_fn(model(mlp_eval, img_eval), target_eval)\n",
    "\n",
    "        # Record results (using square-root of loss similar to KAN.fit)\n",
    "        results['train_loss'].append(torch.sqrt(loss_val.detach()).item())\n",
    "        results['eval_loss'].append(torch.sqrt(eval_loss.detach()).item())\n",
    "        results['reg'].append(reg_val.detach().item())\n",
    "\n",
    "        if metrics is not None:\n",
    "            for metric in metrics:\n",
    "                # Here, we assume each metric returns a tensor.\n",
    "                results[metric.__name__].append(metric().item())\n",
    "\n",
    "        if eval_loss < best_loss:\n",
    "            best_epoch = step\n",
    "            best_loss = eval_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Update progress bar.\n",
    "        if display_metrics is None:\n",
    "            pbar.set_description(\"| train_loss: %.2e | eval_loss: %.2e | reg: %.2e |\" %\n",
    "                                 (torch.sqrt(loss_val.detach()).item(),\n",
    "                                  torch.sqrt(eval_loss.detach()).item(),\n",
    "                                  reg_val.detach().item()))\n",
    "        else:\n",
    "            string = ''\n",
    "            data = ()\n",
    "            for metric in display_metrics:\n",
    "                string += f' {metric}: %.2e |'\n",
    "                try:\n",
    "                    results[metric]\n",
    "                except:\n",
    "                    raise Exception(f'{metric} not recognized')\n",
    "                data += (results[metric][-1],)\n",
    "            pbar.set_description(string % data)\n",
    "        # Optionally save a figure snapshot.\n",
    "        if save_fig and step % save_fig_freq == 0:\n",
    "            save_act_backup = getattr(model.m_kan, \"save_act\", False)\n",
    "            model.m_kan.save_act = True\n",
    "            model.plot(folder=img_folder, in_vars=in_vars, out_vars=out_vars, title=f\"Step {step}\", beta=beta)\n",
    "            plt.savefig(os.path.join(img_folder, f\"{step}.jpg\"), bbox_inches='tight', dpi=200)\n",
    "            plt.close()\n",
    "            model.m_kan.save_act = save_act_backup\n",
    "\n",
    "    # Restore original settings if applicable.\n",
    "    if old_symbolic_enabled is not None:\n",
    "        model.m_kan.symbolic_enabled = old_symbolic_enabled\n",
    "    if hasattr(model.m_kan, \"log_history\"):\n",
    "        model.m_kan.log_history('fit')\n",
    "    best_loss = torch.sqrt(best_loss)\n",
    "    print(f\"✅ Best validation loss: {best_loss:.4e} at {best_epoch} epoch\")\n",
    "    return best_model_state, results, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd3a6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_create_model(model_class, attributes, imgs_shape, kan_neurons, kan_grid):\n",
    "    try:\n",
    "        model = model_class(attributes, imgs_shape, kan_neurons, kan_grid)\n",
    "        \n",
    "        # Test the model with a sample input\n",
    "        num_input = torch.randn(4, attributes)\n",
    "        img_input = torch.randn(4, *imgs_shape)\n",
    "        output = model(num_input, img_input)\n",
    "        \n",
    "        print(f\"Successfully created and tested {model_class.__name__}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating or testing {model_class.__name__}:\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14c1082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_kan_hybrid(dataset, model_class, max_steps=30, filename=None):\n",
    "    hidden_neuron_options = list(range(1, attributes // 2 + 1))\n",
    "    grid_options = list(range(3, 7))\n",
    "    lamb_options = [0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "\n",
    "    top_models = []  # list of dicts: each will have 'loss', 'config', 'state_dict'\n",
    "\n",
    "    for hidden in hidden_neuron_options:\n",
    "        for grid in grid_options:\n",
    "            for lamb in lamb_options:\n",
    "                print(f\"Testing {hidden} hidden neurons, lamb={lamb}, grid={grid}\")\n",
    "                model = try_create_model(model_class, attributes, imgs_shape, kan_neurons=hidden, kan_grid=grid)  # Attempt to create Model3\n",
    "                \n",
    "                # Custom training\n",
    "                #_, model_state, _ = custom_fit(model, dataset, opt=\"LBFGS\", steps=40, lamb=lamb)\n",
    "                model_state, _, best_epoch = fit_hybrid_dataloaders(model, dataset, opt=\"LBFGS\", steps=max_steps, lamb=lamb)\n",
    "                model.load_state_dict(model_state)\n",
    "\n",
    "                # Evaluate\n",
    "                avg_loss = average_rmse(model(dataset['test_input'], dataset['test_img']), dataset['test_label'])\n",
    "\n",
    "                # Save this model info\n",
    "                top_models.append({\n",
    "                    \"loss\": avg_loss,\n",
    "                    \"config\": {\"hidden\": hidden, \"grid\": grid, \"lamb\": lamb},\n",
    "                    #\"state_dict\": copy.deepcopy(model_state),\n",
    "                    \"best_epoch\": best_epoch\n",
    "                })\n",
    "                append_row_to_csv(filename, hidden, grid, lamb, avg_loss, best_epoch)\n",
    "                del model\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        # Keep only top 5\n",
    "        top_models = sorted(top_models, key=lambda x: x[\"loss\"])[:5]\n",
    "\n",
    "    print(\"\\n✅ Top 5 Configurations:\")\n",
    "    for i, entry in enumerate(top_models):\n",
    "        cfg = entry[\"config\"]\n",
    "        print(f\"{i+1}. width=[{attributes}, {cfg['hidden']}, 1], grid={cfg['grid']}, lamb={cfg['lamb']}| best_epoch={entry['best_epoch']} | loss={entry['loss']:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28459f6e-ce5c-4d75-93de-ceb76e49252b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hybrid Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96a92a-0d27-4997-a984-89247929719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, kan_neurons, kan_grid, device=device):\n",
    "        super(Model2, self).__init__()\n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], 16, kernel_size=3, padding=2),     # out: 16 x 9 x 9\n",
    "            nn.LayerNorm([16, 22, 22]),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "        # Final KAN layers\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, kan_neurons],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Calculate the size of the flattened output\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        # Final MLP layers\n",
    "        self.final_kan = KAN(\n",
    "            width=[self.flat_size + kan_neurons, 1],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        # Forward pass with dummy input to calculate flat size\n",
    "        dummy_input = torch.zeros(4, *imgs_shape, device=device)\n",
    "        x = self.cnn_branch(dummy_input)\n",
    "        return x.size(1)\n",
    "\n",
    "    def get_concat_output(self, mlp_input, cnn_input):\n",
    "        # Ensure inputs are moved to the correct device\n",
    "        kan_input = mlp_input.to(self.device)\n",
    "        cnn_input = cnn_input.to(self.device)\n",
    "        \n",
    "        cnn_output = self.cnn_branch(cnn_input)  # Process image input\n",
    "        kan_output = self.m_kan(kan_input)  # Process numerical input\n",
    "        \n",
    "        return torch.cat((kan_output, cnn_output), dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self, mlp_input, cnn_input):\n",
    "        concat_output = self.get_concat_output(mlp_input, cnn_input)\n",
    "        return self.final_kan(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd9b9689-bc59-4c58-9838-854ac0adfafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, kan_neurons, kan_grid, device=device):\n",
    "        super(Model3, self).__init__()\n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], 16, kernel_size=3, padding=2),     # out: 16 x 9 x 9\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                            # out: 16 x 4 x 4\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=2),                # out: 32 x 5 x 5\n",
    "            nn.LayerNorm([32, 13, 13]),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "        # Final KAN layers\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, kan_neurons],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Calculate the size of the flattened output\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        # Final MLP layers\n",
    "        self.final_kan = KAN(\n",
    "            width=[self.flat_size + kan_neurons, 1],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        # Forward pass with dummy input to calculate flat size\n",
    "        dummy_input = torch.zeros(4, *imgs_shape, device=device)\n",
    "        x = self.cnn_branch(dummy_input)\n",
    "        return x.size(1)\n",
    "\n",
    "    def get_concat_output(self, mlp_input, cnn_input):\n",
    "        # Ensure inputs are moved to the correct device\n",
    "        kan_input = mlp_input.to(self.device)\n",
    "        cnn_input = cnn_input.to(self.device)\n",
    "        \n",
    "        cnn_output = self.cnn_branch(cnn_input)  # Process image input\n",
    "        kan_output = self.m_kan(kan_input)  # Process numerical input\n",
    "        \n",
    "        return torch.cat((kan_output, cnn_output), dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self, mlp_input, cnn_input):\n",
    "        concat_output = self.get_concat_output(mlp_input, cnn_input)\n",
    "        return self.final_kan(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a068700c-e48a-4253-bdce-9ec230676e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, kan_neurons, kan_grid, device=device):\n",
    "        super(Model4, self).__init__()\n",
    "\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], 16, kernel_size=3, padding=2),     # out: 16 x 9 x 9\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                            # out: 16 x 4 x 4\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=2),                # out: 32 x 5 x 5\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                            # 32 x 2 x 2\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=2),\n",
    "            nn.LayerNorm([64, 8, 8]),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "\n",
    "        # Middle KAN layers\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, kan_neurons],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "        # Calculate the size of the flattened output\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "        # Final MLP layers\n",
    "        self.final_kan = KAN(\n",
    "            width=[self.flat_size + kan_neurons, 1],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        # Forward pass with dummy input to calculate flat size\n",
    "        dummy_input = torch.zeros(4, *imgs_shape, device=device)\n",
    "        x = self.cnn_branch(dummy_input)\n",
    "        return x.size(1)\n",
    "\n",
    "    def get_concat_output(self, mlp_input, cnn_input):\n",
    "        # Ensure inputs are moved to the correct device\n",
    "        kan_input = mlp_input.to(self.device)\n",
    "        cnn_input = cnn_input.to(self.device)\n",
    "        \n",
    "        cnn_output = self.cnn_branch(cnn_input)  # Process image input\n",
    "        kan_output = self.m_kan(kan_input)  # Process numerical input\n",
    "        \n",
    "        return torch.cat((kan_output, cnn_output), dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self, mlp_input, cnn_input):\n",
    "        concat_output = self.get_concat_output(mlp_input, cnn_input)\n",
    "        return self.final_kan(concat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a371e-8b0f-44cc-835b-562ddeb71d01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Set Files Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b0d4199-5f62-4df3-8934-2521b493c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_with_header(filename, header=['kan_neurons', 'kan_grid', 'lamb', 'RMSE', 'Best_Epoch']):\n",
    "    \"\"\"Creates a CSV file with a given header.\"\"\"\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25b84a0f-c6f6-4e70-8996-d9b5572160f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_row_to_csv(filename, kan_neurons, kan_grid, lamb, rmse, best_epoch):\n",
    "    row = [kan_neurons, kan_grid, lamb, rmse, best_epoch]\n",
    "    \"\"\"Appends a single row to an existing CSV file.\"\"\"\n",
    "    if not os.path.isfile(filename):\n",
    "        raise FileNotFoundError(f\"{filename} does not exist. Please create the file first with a header.\")\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e503d65b-47e8-4c00-a84f-cec61e51ba9d",
   "metadata": {},
   "source": [
    "# Load Dataset and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0d46e15-1287-4f1e-ba6a-cc05b3c8915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_and_clean('N_train.npy', 'y_train.npy',x_col, target_col)\n",
    "X_test, y_test   = load_and_clean('N_test.npy',  'y_test.npy', x_col, target_col)\n",
    "X_val, y_val     = load_and_clean('N_val.npy',   'y_val.npy', x_col, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92613d4f-1cd2-49f8-82a9-620fda68b565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the dataframe\n",
    "num_columns = X_train.shape[1]\n",
    "\n",
    "# Calculate number of columns - 1\n",
    "columns_minus_one = num_columns - 1\n",
    "\n",
    "# Calculate the square root for image size\n",
    "image_size = math.ceil(math.sqrt(columns_minus_one))\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a28853f7-dcee-47de-91eb-8f35177b8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'treasury'\n",
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "pixel=20\n",
    "image_model = TINTO(problem=problem_type, blur=False, pixels=pixel, random_seed=SEED)\n",
    "name = f\"TINTO\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a01e98f2-1bea-4364-b0bc-3d0412ebba5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images are already generated\n",
      "The images are already generated\n",
      "The images are already generated\n",
      "Images shape:  (3, 20, 20)\n",
      "Attributes:  8\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape = load_and_preprocess_data(\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val,\n",
    "    image_model=image_model,\n",
    "    problem_type=problem_type,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af7bc776-0d49-4456-ba5b-861c4005fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_loader(loader):\n",
    "    \"\"\"\n",
    "    Combines all batches from a DataLoader into three tensors.\n",
    "    Assumes each batch is a tuple: (mlp_tensor, img_tensor, target_tensor)\n",
    "    \"\"\"\n",
    "    mlp_list, img_list, target_list = [], [], []\n",
    "    for mlp, img, target in loader:\n",
    "        mlp_list.append(mlp)\n",
    "        img_list.append(img)\n",
    "        target_list.append(target)\n",
    "    return torch.cat(mlp_list, dim=0), torch.cat(img_list, dim=0), torch.cat(target_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c84fd70-94ac-4546-b907-00fe548676ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine dataloaders into tensors.\n",
    "train_mlp, train_img, train_target = combine_loader(train_loader)\n",
    "val_mlp, val_img, val_target = combine_loader(val_loader)\n",
    "test_mlp, test_img, test_target = combine_loader(test_loader)\n",
    "\n",
    "dataset = {\n",
    "    \"train_input\": train_mlp.to(device),\n",
    "    \"train_img\": train_img.to(device),\n",
    "    \"train_label\": train_target.to(device),\n",
    "    \"val_input\": val_mlp.to(device),\n",
    "    \"val_img\": val_img.to(device),\n",
    "    \"val_label\": val_target.to(device),\n",
    "    \"test_input\": test_mlp.to(device),\n",
    "    \"test_img\": test_img.to(device),\n",
    "    \"test_label\": test_target.to(device),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8382d81-df3d-4828-90a0-a472f0fb312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([5242, 8])\n",
      "Train target shape: torch.Size([5242, 1])\n",
      "Test data shape: torch.Size([1639, 8])\n",
      "Test target shape: torch.Size([1639, 1])\n",
      "Validation data shape: torch.Size([1311, 8])\n",
      "Validation target shape: torch.Size([1311, 1])\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the tensors\n",
    "print(\"Train data shape:\", dataset['train_input'].shape)\n",
    "print(\"Train target shape:\", dataset['train_label'].shape)\n",
    "print(\"Test data shape:\", dataset['test_input'].shape)\n",
    "print(\"Test target shape:\", dataset['test_label'].shape)\n",
    "print(\"Validation data shape:\", dataset['val_input'].shape)\n",
    "print(\"Validation target shape:\", dataset['val_label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c347db2-9099-4f3c-8786-e7db470441c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_1=f'{dataset_name}_tinto_grid_search_cnn.csv'\n",
    "filename_2=f'{dataset_name}_tinto_grid_search_hybrid2.csv'\n",
    "filename_3=f'{dataset_name}_tinto_grid_search_hybrid3.csv'\n",
    "filename_4=f'{dataset_name}_tinto_grid_search_hybrid4.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16745534-695d-44df-a61b-187a86e86579",
   "metadata": {},
   "source": [
    "# Grid Search for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e942f11d-3375-4810-9ebe-016637b105a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_with_header(filename_1, header=['cnn_blocks', 'dense_layers', '_', 'RMSE', 'Best_Epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd8fbd74-7bc9-4cff-b7fb-7e0b11b849ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing cnn_blocks=1, dense_layers=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 1.8966e+01 | Val: 1.9253e+01 |: 100%|██████████████████████| 10/10 [00:07<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best validation loss: 1.9162e+01 at 7 epoch\n",
      "Testing cnn_blocks=1, dense_layers=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 1.9759e+01 | Val: 2.0469e+01 |: 100%|██████████████████████| 10/10 [00:44<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best validation loss: 2.0442e+01 at 8 epoch\n",
      "Testing cnn_blocks=1, dense_layers=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 1.9402e+01 | Val: 2.0243e+01 |: 100%|██████████████████████| 10/10 [00:54<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best validation loss: 2.0243e+01 at 9 epoch\n",
      "Testing cnn_blocks=2, dense_layers=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 1.2607e+01 | Val: 1.3239e+01 |: 100%|██████████████████████| 10/10 [00:11<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best validation loss: 1.3239e+01 at 8 epoch\n",
      "Testing cnn_blocks=2, dense_layers=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 1.7985e+01 | Val: 1.8389e+01 |:  30%|██████▉                | 3/10 [00:11<00:27,  3.91s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m grid_search_cnn(dataset, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, filename\u001b[38;5;241m=\u001b[39mfilename_2)\n",
      "Cell \u001b[1;32mIn[45], line 13\u001b[0m, in \u001b[0;36mgrid_search_cnn\u001b[1;34m(dataset, steps, opt, batch, filename)\u001b[0m\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m build_custom_cnn_model(cnn_blocks, dense_layers, imgs_shape)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Custom training\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model_state, _, best_epoch \u001b[38;5;241m=\u001b[39m fit_cnn_only_model(model, dataset, steps\u001b[38;5;241m=\u001b[39msteps, opt\u001b[38;5;241m=\u001b[39mopt, batch\u001b[38;5;241m=\u001b[39mbatch)\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(model_state)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 55\u001b[0m, in \u001b[0;36mfit_cnn_only_model\u001b[1;34m(model, dataset, steps, lr, loss_fn, batch, opt)\u001b[0m\n\u001b[0;32m     53\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m---> 55\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n\u001b[0;32m     56\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m closure()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# AdamW\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_torch_12\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_torch_12\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\Master Data Science\\Thesis\\pykan\\kan\\LBFGS.py:401\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_old \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    400\u001b[0m     al[i] \u001b[38;5;241m=\u001b[39m old_stps[i]\u001b[38;5;241m.\u001b[39mdot(q) \u001b[38;5;241m*\u001b[39m ro[i]\n\u001b[1;32m--> 401\u001b[0m     q\u001b[38;5;241m.\u001b[39madd_(old_dirs[i], alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mal[i])\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# multiply by initial Hessian\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;66;03m# r/d is the final direction\u001b[39;00m\n\u001b[0;32m    405\u001b[0m d \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmul(q, H_diag)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_search_cnn(dataset, steps=150, filename=filename_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6de250fe-da38-4db0-954a-bddb0b43137a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing cnn_blocks=1, dense_layers=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 8.9012e+00 | Val: 1.0304e+01 |: 100%|████████████████████| 110/110 [00:41<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best validation loss: 9.5108e+00 at 18 epoch\n",
      "Testing cnn_blocks=1, dense_layers=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 9.0829e+00 | Val: 9.9281e+00 |: 100%|████████████████████| 110/110 [00:46<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best validation loss: 9.6624e+00 at 26 epoch\n",
      "Testing cnn_blocks=1, dense_layers=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 9.8750e+00 | Val: 9.7805e+00 |: 100%|████████████████████| 110/110 [00:38<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best validation loss: 9.7607e+00 at 78 epoch\n",
      "Testing cnn_blocks=2, dense_layers=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 7.7997e+00 | Val: 1.1511e+01 |: 100%|████████████████████| 110/110 [00:26<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best validation loss: 9.7255e+00 at 7 epoch\n",
      "Testing cnn_blocks=2, dense_layers=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 5.2842e+00 | Val: 1.4116e+01 |: 100%|████████████████████| 110/110 [00:52<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best validation loss: 9.8174e+00 at 8 epoch\n",
      "Testing cnn_blocks=2, dense_layers=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 9.4683e+00 | Val: 9.7571e+00 |: 100%|████████████████████| 110/110 [00:40<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best validation loss: 9.7571e+00 at 81 epoch\n",
      "Testing cnn_blocks=3, dense_layers=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 4.1550e+00 | Val: 1.4427e+01 |: 100%|████████████████████| 110/110 [00:36<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best validation loss: 9.5568e+00 at 4 epoch\n",
      "Testing cnn_blocks=3, dense_layers=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 4.5311e+00 | Val: 1.4665e+01 |: 100%|████████████████████| 110/110 [00:45<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best validation loss: 9.7126e+00 at 3 epoch\n",
      "Testing cnn_blocks=3, dense_layers=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Train: 1.0363e+01 | Val: 1.0629e+01 |: 100%|████████████████████| 110/110 [00:36<00:00,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best validation loss: 1.0629e+01 at 15 epoch\n",
      "\n",
      "✅ Top 5 Configurations:\n",
      "1. cnn_blocks=1, dense_layers=1 | best_epoch=18 | loss=3.23118\n",
      "2. cnn_blocks=2, dense_layers=1 | best_epoch=7 | loss=3.25628\n",
      "3. cnn_blocks=3, dense_layers=2 | best_epoch=3 | loss=3.25922\n",
      "4. cnn_blocks=2, dense_layers=2 | best_epoch=8 | loss=3.26547\n",
      "5. cnn_blocks=1, dense_layers=2 | best_epoch=26 | loss=3.26898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "grid_search_cnn(dataset, steps=150, filename=filename_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62a122-4f8a-4f03-8819-1fc60c1514f7",
   "metadata": {},
   "source": [
    "# Grid Search for Hibrid Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c68c1b-0d39-4690-b627-24b183a79b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_with_header(filename_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c36bf29-5d84-4f8a-9296-05378fb4de1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_kan_hybrid(dataset, Model2, max_steps=60, filename=filename_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9db396-7fe4-4043-97ce-b8f5983224af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Grid Search for Hibrid Model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a05978a-06ac-408c-8d91-c0c1abe332f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_with_header(filename_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1cb5d881-c64b-40b2-bce3-f00d50871fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 hidden neurons, lamb=0.1, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                               | 0/2 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 930.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 12.31 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m grid_search_kan_hybrid(dataset, Model3, max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, filename\u001b[38;5;241m=\u001b[39mfilename_3)\n",
      "Cell \u001b[1;32mIn[51], line 16\u001b[0m, in \u001b[0;36mgrid_search_kan_hybrid\u001b[1;34m(dataset, model_class, max_steps, filename)\u001b[0m\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m try_create_model(model_class, attributes, imgs_shape, kan_neurons\u001b[38;5;241m=\u001b[39mhidden, kan_grid\u001b[38;5;241m=\u001b[39mgrid)  \u001b[38;5;66;03m# Attempt to create Model3\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Custom training\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#_, model_state, _ = custom_fit(model, dataset, opt=\"LBFGS\", steps=40, lamb=lamb)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m model_state, _, best_epoch \u001b[38;5;241m=\u001b[39m fit_hybrid_dataloaders(model, dataset, opt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLBFGS\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39mmax_steps, lamb\u001b[38;5;241m=\u001b[39mlamb)\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(model_state)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 140\u001b[0m, in \u001b[0;36mfit_hybrid_dataloaders\u001b[1;34m(model, dataset, opt, steps, log, lamb, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff, update_grid, grid_update_num, loss_fn, lr, start_grid_update_step, stop_grid_update_step, batch, metrics, save_fig, in_vars, out_vars, beta, save_fig_freq, img_folder, singularity_avoiding, y_th, reg_metric, display_metrics, sum_f_reg)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m#cnn_output = model.cnn_branch(cnn_batch)  # Process image input\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     concatenated \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_concat_output(mlp_batch, cnn_batch)\n\u001b[1;32m--> 140\u001b[0m     model\u001b[38;5;241m.\u001b[39mfinal_kan\u001b[38;5;241m.\u001b[39mupdate_grid(concatenated)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Perform an optimizer step.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLBFGS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\Master Data Science\\Thesis\\pykan\\kan\\MultKAN.py:723\u001b[0m, in \u001b[0;36mMultKAN.update_grid\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_grid\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    720\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;124;03m    call update_grid_from_samples. This seems unnecessary but we retain it for the sake of classes that might inherit from MultKAN\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 723\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_grid_from_samples(x)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\Master Data Science\\Thesis\\pykan\\kan\\MultKAN.py:716\u001b[0m, in \u001b[0;36mMultKAN.update_grid_from_samples\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03mupdate grid from samples\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;124;03m>>> print(model.act_fun[0].grid)\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m \n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth):\n\u001b[1;32m--> 716\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_act(x)\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fun[l]\u001b[38;5;241m.\u001b[39mupdate_grid_from_samples(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macts[l])\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\Master Data Science\\Thesis\\pykan\\kan\\MultKAN.py:2701\u001b[0m, in \u001b[0;36mMultKAN.get_act\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   2699\u001b[0m save_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_act\n\u001b[0;32m   2700\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 2701\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m   2702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_act \u001b[38;5;241m=\u001b[39m save_act\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\Master Data Science\\Thesis\\pykan\\kan\\MultKAN.py:809\u001b[0m, in \u001b[0;36mMultKAN.forward\u001b[1;34m(self, x, singularity_avoiding, y_th)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macts\u001b[38;5;241m.\u001b[39mappend(x)  \u001b[38;5;66;03m# acts shape: (batch, width[l])\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth):\n\u001b[1;32m--> 809\u001b[0m     x_numerical, preacts, postacts_numerical, postspline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fun[l](x)\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;66;03m#print(preacts, postacts_numerical, postspline)\u001b[39;00m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msymbolic_enabled \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_torch_12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\env_torch_12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\Master Data Science\\Thesis\\pykan\\kan\\KANLayer.py:163\u001b[0m, in \u001b[0;36mKANLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    161\u001b[0m base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_fun(x) \u001b[38;5;66;03m# (batch, in_dim)\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m#Spline Evaluation (y and postspline)\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m y \u001b[38;5;241m=\u001b[39m coef2curve(x_eval\u001b[38;5;241m=\u001b[39mx, grid\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid, coef\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk)\n\u001b[0;32m    165\u001b[0m postspline \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Combining Base and Spline Contributions\u001b[39;00m\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\Master Data Science\\Thesis\\pykan\\kan\\spline.py:75\u001b[0m, in \u001b[0;36mcoef2curve\u001b[1;34m(x_eval, grid, coef, k, device)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcoef2curve\u001b[39m(x_eval, grid, coef, k, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m    converting B-spline coefficients to B-spline curves. Evaluate x on B-spline curves (summing up B_batch results over B-spline basis).\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m        \u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     b_splines \u001b[38;5;241m=\u001b[39m B_batch(x_eval, grid, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m     76\u001b[0m     y_eval \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mijk,jlk->ijl\u001b[39m\u001b[38;5;124m'\u001b[39m, b_splines, coef\u001b[38;5;241m.\u001b[39mto(b_splines\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_eval\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\Master Data Science\\Thesis\\pykan\\kan\\spline.py:42\u001b[0m, in \u001b[0;36mB_batch\u001b[1;34m(x, grid, k, extend, device)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     B_km1 \u001b[38;5;241m=\u001b[39m B_batch(x[:,:,\u001b[38;5;241m0\u001b[39m], grid\u001b[38;5;241m=\u001b[39mgrid[\u001b[38;5;241m0\u001b[39m], k\u001b[38;5;241m=\u001b[39mk \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m     value \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m grid[:, :, :\u001b[38;5;241m-\u001b[39m(k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]) \u001b[38;5;241m/\u001b[39m (grid[:, :, k:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m grid[:, :, :\u001b[38;5;241m-\u001b[39m(k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]) \u001b[38;5;241m*\u001b[39m B_km1[:, :, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\n\u001b[0;32m     43\u001b[0m                 grid[:, :, k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m-\u001b[39m x) \u001b[38;5;241m/\u001b[39m (grid[:, :, k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m-\u001b[39m grid[:, :, \u001b[38;5;241m1\u001b[39m:(\u001b[38;5;241m-\u001b[39mk)]) \u001b[38;5;241m*\u001b[39m B_km1[:, :, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# in case grid is degenerate\u001b[39;00m\n\u001b[0;32m     46\u001b[0m value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnan_to_num(value)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 930.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 12.31 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "grid_search_kan_hybrid(dataset, Model3, max_steps=60, filename=filename_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cc2a2-794a-4fd7-8163-ffc086f811c0",
   "metadata": {},
   "source": [
    "# Grid Search for Hibrid Model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5fddc-684d-48f3-95dd-87687d1c2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_with_header(filename_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6797fb32-392a-4191-851d-501a47cc3498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 1 hidden neurons, lamb=1.0, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.63e+00 | eval_loss: 5.77e+00 | reg: 2.72e-01 |: 100%|█| 50/50 [03:11<00:00,  3.83s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3267e+01 at 11 epoch\n",
      "Testing 1 hidden neurons, lamb=0.1, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.35e+00 | eval_loss: 3.13e+00 | reg: 2.02e+01 |: 100%|█| 50/50 [03:17<00:00,  3.95s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7101e+00 at 8 epoch\n",
      "Testing 1 hidden neurons, lamb=0.01, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.35e+00 | eval_loss: 3.85e+00 | reg: 6.75e+01 |: 100%|█| 50/50 [03:13<00:00,  3.87s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.5744e+00 at 5 epoch\n",
      "Testing 1 hidden neurons, lamb=0.001, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.16e+00 | eval_loss: 4.10e+00 | reg: 3.96e+02 |: 100%|█| 50/50 [03:13<00:00,  3.88s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6269e+00 at 4 epoch\n",
      "Testing 1 hidden neurons, lamb=0.0001, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.09e+00 | eval_loss: 4.02e+00 | reg: 5.02e+02 |: 100%|█| 50/50 [03:16<00:00,  3.93s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6780e+00 at 3 epoch\n",
      "Testing 1 hidden neurons, lamb=1e-05, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.20e+00 | eval_loss: 3.92e+00 | reg: 5.81e+02 |: 100%|█| 50/50 [03:18<00:00,  3.97s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6478e+00 at 5 epoch\n",
      "Testing 1 hidden neurons, lamb=1e-06, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.07e+00 | eval_loss: 4.20e+00 | reg: 5.85e+02 |: 100%|█| 50/50 [03:18<00:00,  3.97s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6681e+00 at 4 epoch\n",
      "Testing 1 hidden neurons, lamb=1.0, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.63e+00 | eval_loss: 5.77e+00 | reg: 2.76e-01 |: 100%|█| 50/50 [03:27<00:00,  4.15s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3278e+01 at 30 epoch\n",
      "Testing 1 hidden neurons, lamb=0.1, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.35e+00 | eval_loss: 3.16e+00 | reg: 2.17e+01 |: 100%|█| 50/50 [03:13<00:00,  3.88s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.8340e+00 at 8 epoch\n",
      "Testing 1 hidden neurons, lamb=0.01, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.25e+00 | eval_loss: 3.84e+00 | reg: 9.14e+01 |: 100%|█| 50/50 [03:31<00:00,  4.23s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6101e+00 at 7 epoch\n",
      "Testing 1 hidden neurons, lamb=0.001, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.83e+00 | eval_loss: 4.35e+00 | reg: 4.32e+02 |: 100%|█| 50/50 [03:33<00:00,  4.28s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7355e+00 at 5 epoch\n",
      "Testing 1 hidden neurons, lamb=0.0001, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.07e+00 | eval_loss: 4.11e+00 | reg: 6.48e+02 |: 100%|█| 50/50 [03:33<00:00,  4.26s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6866e+00 at 4 epoch\n",
      "Testing 1 hidden neurons, lamb=1e-05, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.95e+00 | eval_loss: 4.18e+00 | reg: 7.97e+02 |: 100%|█| 50/50 [03:33<00:00,  4.26s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6459e+00 at 4 epoch\n",
      "Testing 1 hidden neurons, lamb=1e-06, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.03e+00 | eval_loss: 4.05e+00 | reg: 7.29e+02 |: 100%|█| 50/50 [03:32<00:00,  4.26s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7485e+00 at 4 epoch\n",
      "Testing 1 hidden neurons, lamb=1.0, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.63e+00 | eval_loss: 5.77e+00 | reg: 2.66e-01 |: 100%|█| 50/50 [03:59<00:00,  4.78s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3281e+01 at 39 epoch\n",
      "Testing 1 hidden neurons, lamb=0.1, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.29e+00 | eval_loss: 3.21e+00 | reg: 2.17e+01 |: 100%|█| 50/50 [04:05<00:00,  4.91s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7336e+00 at 11 epoch\n",
      "Testing 1 hidden neurons, lamb=0.01, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.89e+00 | eval_loss: 4.13e+00 | reg: 1.44e+02 |: 100%|█| 50/50 [03:59<00:00,  4.78s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6467e+00 at 6 epoch\n",
      "Testing 1 hidden neurons, lamb=0.001, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.46e+00 | eval_loss: 4.68e+00 | reg: 5.86e+02 |: 100%|█| 50/50 [05:41<00:00,  6.83s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6743e+00 at 4 epoch\n",
      "Testing 1 hidden neurons, lamb=0.0001, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.34e+00 | eval_loss: 4.76e+00 | reg: 6.36e+02 |: 100%|█| 50/50 [04:03<00:00,  4.86s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6665e+00 at 6 epoch\n",
      "Testing 1 hidden neurons, lamb=1e-05, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.25e+00 | eval_loss: 4.70e+00 | reg: 6.57e+02 |: 100%|█| 50/50 [04:27<00:00,  5.35s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6843e+00 at 5 epoch\n",
      "Testing 1 hidden neurons, lamb=1e-06, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.49e+00 | eval_loss: 4.59e+00 | reg: 6.08e+02 |: 100%|█| 50/50 [03:59<00:00,  4.78s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6463e+00 at 4 epoch\n",
      "Testing 1 hidden neurons, lamb=1.0, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.62e+00 | eval_loss: 5.77e+00 | reg: 2.45e-01 |: 100%|█| 50/50 [04:21<00:00,  5.23s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3276e+01 at 21 epoch\n",
      "Testing 1 hidden neurons, lamb=0.1, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.36e+00 | eval_loss: 3.17e+00 | reg: 2.28e+01 |: 100%|█| 50/50 [04:28<00:00,  5.38s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7770e+00 at 6 epoch\n",
      "Testing 1 hidden neurons, lamb=0.01, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.19e+00 | eval_loss: 3.86e+00 | reg: 1.42e+02 |: 100%|█| 50/50 [05:10<00:00,  6.21s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6416e+00 at 5 epoch\n",
      "Testing 1 hidden neurons, lamb=0.001, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.72e+00 | eval_loss: 4.31e+00 | reg: 6.17e+02 |: 100%|█| 50/50 [04:22<00:00,  5.24s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6683e+00 at 5 epoch\n",
      "Testing 1 hidden neurons, lamb=0.0001, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.70e+00 | eval_loss: 4.40e+00 | reg: 7.84e+02 |: 100%|█| 50/50 [04:23<00:00,  5.28s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6369e+00 at 5 epoch\n",
      "Testing 1 hidden neurons, lamb=1e-05, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.57e+00 | eval_loss: 4.37e+00 | reg: 8.56e+02 |: 100%|█| 50/50 [04:21<00:00,  5.22s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6301e+00 at 6 epoch\n",
      "Testing 1 hidden neurons, lamb=1e-06, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.64e+00 | eval_loss: 4.55e+00 | reg: 9.18e+02 |: 100%|█| 50/50 [04:20<00:00,  5.21s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6231e+00 at 3 epoch\n",
      "Testing 2 hidden neurons, lamb=1.0, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.63e+00 | eval_loss: 5.77e+00 | reg: 3.03e-01 |: 100%|█| 50/50 [03:10<00:00,  3.80s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3267e+01 at 19 epoch\n",
      "Testing 2 hidden neurons, lamb=0.1, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.26e+00 | eval_loss: 3.21e+00 | reg: 1.97e+01 |: 100%|█| 50/50 [03:10<00:00,  3.82s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7890e+00 at 10 epoch\n",
      "Testing 2 hidden neurons, lamb=0.01, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.47e+00 | eval_loss: 3.65e+00 | reg: 8.05e+01 |: 100%|█| 50/50 [03:11<00:00,  3.83s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7830e+00 at 6 epoch\n",
      "Testing 2 hidden neurons, lamb=0.001, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.84e+00 | eval_loss: 4.36e+00 | reg: 3.34e+02 |: 100%|█| 50/50 [03:18<00:00,  3.98s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6146e+00 at 7 epoch\n",
      "Testing 2 hidden neurons, lamb=0.0001, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.04e+00 | eval_loss: 4.10e+00 | reg: 5.84e+02 |: 100%|█| 50/50 [03:17<00:00,  3.94s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7621e+00 at 5 epoch\n",
      "Testing 2 hidden neurons, lamb=1e-05, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.37e+00 | eval_loss: 3.74e+00 | reg: 6.86e+02 |: 100%|█| 50/50 [03:21<00:00,  4.04s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.8639e+00 at 5 epoch\n",
      "Testing 2 hidden neurons, lamb=1e-06, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.09e+00 | eval_loss: 4.09e+00 | reg: 7.01e+02 |: 100%|█| 50/50 [03:21<00:00,  4.02s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7536e+00 at 5 epoch\n",
      "Testing 2 hidden neurons, lamb=1.0, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.63e+00 | eval_loss: 5.77e+00 | reg: 3.38e-01 |: 100%|█| 50/50 [03:26<00:00,  4.13s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3273e+01 at 17 epoch\n",
      "Testing 2 hidden neurons, lamb=0.1, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.23e+00 | eval_loss: 3.24e+00 | reg: 2.22e+01 |: 100%|█| 50/50 [03:25<00:00,  4.12s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7482e+00 at 7 epoch\n",
      "Testing 2 hidden neurons, lamb=0.01, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.91e+00 | eval_loss: 4.16e+00 | reg: 8.81e+01 |: 100%|█| 50/50 [03:29<00:00,  4.19s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6164e+00 at 4 epoch\n",
      "Testing 2 hidden neurons, lamb=0.001, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.54e+00 | eval_loss: 4.43e+00 | reg: 3.26e+02 |: 100%|█| 50/50 [03:33<00:00,  4.26s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7544e+00 at 5 epoch\n",
      "Testing 2 hidden neurons, lamb=0.0001, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.68e+00 | eval_loss: 4.39e+00 | reg: 5.17e+02 |: 100%|█| 50/50 [03:35<00:00,  4.31s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7339e+00 at 4 epoch\n",
      "Testing 2 hidden neurons, lamb=1e-05, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.66e+00 | eval_loss: 4.38e+00 | reg: 6.35e+02 |: 100%|█| 50/50 [03:39<00:00,  4.38s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6964e+00 at 4 epoch\n",
      "Testing 2 hidden neurons, lamb=1e-06, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.89e+00 | eval_loss: 4.09e+00 | reg: 6.98e+02 |: 100%|█| 50/50 [03:36<00:00,  4.33s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7595e+00 at 6 epoch\n",
      "Testing 2 hidden neurons, lamb=1.0, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.63e+00 | eval_loss: 5.77e+00 | reg: 2.85e-01 |: 100%|█| 50/50 [03:58<00:00,  4.78s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3270e+01 at 12 epoch\n",
      "Testing 2 hidden neurons, lamb=0.1, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.23e+00 | eval_loss: 3.24e+00 | reg: 2.13e+01 |: 100%|█| 50/50 [04:02<00:00,  4.85s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.8342e+00 at 21 epoch\n",
      "Testing 2 hidden neurons, lamb=0.01, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.06e+00 | eval_loss: 3.98e+00 | reg: 1.30e+02 |: 100%|█| 50/50 [03:58<00:00,  4.76s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6504e+00 at 5 epoch\n",
      "Testing 2 hidden neurons, lamb=0.001, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.66e+00 | eval_loss: 4.36e+00 | reg: 4.49e+02 |: 100%|█| 50/50 [04:02<00:00,  4.84s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6208e+00 at 7 epoch\n",
      "Testing 2 hidden neurons, lamb=0.0001, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.49e+00 | eval_loss: 4.50e+00 | reg: 6.33e+02 |: 100%|█| 50/50 [04:02<00:00,  4.86s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7010e+00 at 5 epoch\n",
      "Testing 2 hidden neurons, lamb=1e-05, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.67e+00 | eval_loss: 4.42e+00 | reg: 6.44e+02 |: 100%|█| 50/50 [04:03<00:00,  4.88s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7098e+00 at 3 epoch\n",
      "Testing 2 hidden neurons, lamb=1e-06, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.75e+00 | eval_loss: 4.37e+00 | reg: 6.57e+02 |: 100%|█| 50/50 [04:03<00:00,  4.87s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6796e+00 at 3 epoch\n",
      "Testing 2 hidden neurons, lamb=1.0, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.62e+00 | eval_loss: 5.77e+00 | reg: 2.34e-01 |: 100%|█| 50/50 [04:21<00:00,  5.23s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3271e+01 at 17 epoch\n",
      "Testing 2 hidden neurons, lamb=0.1, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.16e+00 | eval_loss: 3.35e+00 | reg: 2.41e+01 |: 100%|█| 50/50 [04:21<00:00,  5.23s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.8385e+00 at 12 epoch\n",
      "Testing 2 hidden neurons, lamb=0.01, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.99e+00 | eval_loss: 4.03e+00 | reg: 1.34e+02 |: 100%|█| 50/50 [04:25<00:00,  5.31s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.5532e+00 at 7 epoch\n",
      "Testing 2 hidden neurons, lamb=0.001, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.52e+00 | eval_loss: 4.50e+00 | reg: 4.78e+02 |: 100%|█| 50/50 [04:23<00:00,  5.26s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6893e+00 at 4 epoch\n",
      "Testing 2 hidden neurons, lamb=0.0001, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.29e+00 | eval_loss: 4.70e+00 | reg: 6.75e+02 |: 100%|█| 50/50 [04:24<00:00,  5.30s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7583e+00 at 2 epoch\n",
      "Testing 2 hidden neurons, lamb=1e-05, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.32e+00 | eval_loss: 4.57e+00 | reg: 7.16e+02 |: 100%|█| 50/50 [04:21<00:00,  5.23s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6716e+00 at 3 epoch\n",
      "Testing 2 hidden neurons, lamb=1e-06, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.22e+00 | eval_loss: 4.74e+00 | reg: 6.99e+02 |: 100%|█| 50/50 [04:27<00:00,  5.34s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7069e+00 at 4 epoch\n",
      "Testing 3 hidden neurons, lamb=1.0, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.65e+00 | eval_loss: 5.77e+00 | reg: 5.32e-01 |: 100%|█| 50/50 [03:10<00:00,  3.82s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3266e+01 at 25 epoch\n",
      "Testing 3 hidden neurons, lamb=0.1, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.28e+00 | eval_loss: 3.15e+00 | reg: 2.04e+01 |: 100%|█| 50/50 [03:09<00:00,  3.78s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6637e+00 at 23 epoch\n",
      "Testing 3 hidden neurons, lamb=0.01, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.20e+00 | eval_loss: 3.87e+00 | reg: 8.05e+01 |: 100%|█| 50/50 [03:17<00:00,  3.95s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6873e+00 at 5 epoch\n",
      "Testing 3 hidden neurons, lamb=0.001, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.92e+00 | eval_loss: 4.20e+00 | reg: 3.41e+02 |: 100%|█| 50/50 [03:17<00:00,  3.95s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7102e+00 at 5 epoch\n",
      "Testing 3 hidden neurons, lamb=0.0001, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.81e+00 | eval_loss: 4.14e+00 | reg: 4.00e+02 |: 100%|█| 50/50 [03:21<00:00,  4.03s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7315e+00 at 4 epoch\n",
      "Testing 3 hidden neurons, lamb=1e-05, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.85e+00 | eval_loss: 4.29e+00 | reg: 5.64e+02 |: 100%|█| 50/50 [03:21<00:00,  4.03s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.8402e+00 at 4 epoch\n",
      "Testing 3 hidden neurons, lamb=1e-06, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.88e+00 | eval_loss: 4.10e+00 | reg: 5.55e+02 |: 100%|█| 50/50 [03:24<00:00,  4.09s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7835e+00 at 4 epoch\n",
      "Testing 3 hidden neurons, lamb=1.0, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.64e+00 | eval_loss: 5.77e+00 | reg: 4.05e-01 |: 100%|█| 50/50 [03:28<00:00,  4.18s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3270e+01 at 10 epoch\n",
      "Testing 3 hidden neurons, lamb=0.1, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.16e+00 | eval_loss: 3.25e+00 | reg: 2.14e+01 |: 100%|█| 50/50 [03:28<00:00,  4.17s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6998e+00 at 8 epoch\n",
      "Testing 3 hidden neurons, lamb=0.01, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.98e+00 | eval_loss: 4.01e+00 | reg: 1.10e+02 |: 100%|█| 50/50 [03:30<00:00,  4.21s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7651e+00 at 5 epoch\n",
      "Testing 3 hidden neurons, lamb=0.001, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.87e+00 | eval_loss: 4.15e+00 | reg: 3.88e+02 |: 100%|█| 50/50 [03:31<00:00,  4.23s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7344e+00 at 5 epoch\n",
      "Testing 3 hidden neurons, lamb=0.0001, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.82e+00 | eval_loss: 4.22e+00 | reg: 6.88e+02 |: 100%|█| 50/50 [03:37<00:00,  4.35s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7362e+00 at 6 epoch\n",
      "Testing 3 hidden neurons, lamb=1e-05, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.88e+00 | eval_loss: 4.35e+00 | reg: 7.68e+02 |: 100%|█| 50/50 [03:37<00:00,  4.35s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.8614e+00 at 4 epoch\n",
      "Testing 3 hidden neurons, lamb=1e-06, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.07e+00 | eval_loss: 3.94e+00 | reg: 7.29e+02 |: 100%|█| 50/50 [03:33<00:00,  4.27s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.8371e+00 at 3 epoch\n",
      "Testing 3 hidden neurons, lamb=1.0, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.63e+00 | eval_loss: 5.77e+00 | reg: 3.19e-01 |: 100%|█| 50/50 [03:58<00:00,  4.76s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3268e+01 at 10 epoch\n",
      "Testing 3 hidden neurons, lamb=0.1, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.01e+00 | eval_loss: 3.46e+00 | reg: 2.41e+01 |: 100%|█| 50/50 [03:54<00:00,  4.69s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7539e+00 at 6 epoch\n",
      "Testing 3 hidden neurons, lamb=0.01, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.08e+00 | eval_loss: 4.17e+00 | reg: 1.16e+02 |: 100%|█| 50/50 [03:53<00:00,  4.67s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6332e+00 at 5 epoch\n",
      "Testing 3 hidden neurons, lamb=0.001, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.74e+00 | eval_loss: 4.40e+00 | reg: 4.82e+02 |: 100%|█| 50/50 [04:00<00:00,  4.80s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6447e+00 at 4 epoch\n",
      "Testing 3 hidden neurons, lamb=0.0001, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.50e+00 | eval_loss: 4.45e+00 | reg: 8.34e+02 |: 100%|█| 50/50 [04:05<00:00,  4.91s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.5847e+00 at 4 epoch\n",
      "Testing 3 hidden neurons, lamb=1e-05, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.50e+00 | eval_loss: 4.59e+00 | reg: 8.53e+02 |: 100%|█| 50/50 [03:59<00:00,  4.79s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6839e+00 at 5 epoch\n",
      "Testing 3 hidden neurons, lamb=1e-06, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.64e+00 | eval_loss: 4.49e+00 | reg: 8.83e+02 |: 100%|█| 50/50 [04:01<00:00,  4.83s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6244e+00 at 4 epoch\n",
      "Testing 3 hidden neurons, lamb=1.0, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.64e+00 | eval_loss: 5.77e+00 | reg: 4.16e-01 |: 100%|█| 50/50 [09:41<00:00, 11.63s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3266e+01 at 14 epoch\n",
      "Testing 3 hidden neurons, lamb=0.1, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.18e+00 | eval_loss: 3.30e+00 | reg: 2.36e+01 |: 100%|█| 50/50 [08:02<00:00,  9.64s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6894e+00 at 8 epoch\n",
      "Testing 3 hidden neurons, lamb=0.01, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.81e+00 | eval_loss: 4.23e+00 | reg: 1.26e+02 |: 100%|█| 50/50 [09:52<00:00, 11.84s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6264e+00 at 6 epoch\n",
      "Testing 3 hidden neurons, lamb=0.001, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.34e+00 | eval_loss: 4.65e+00 | reg: 4.94e+02 |: 100%|█| 50/50 [09:36<00:00, 11.53s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7481e+00 at 5 epoch\n",
      "Testing 3 hidden neurons, lamb=0.0001, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.19e+00 | eval_loss: 4.68e+00 | reg: 6.20e+02 |: 100%|█| 50/50 [07:34<00:00,  9.09s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7495e+00 at 3 epoch\n",
      "Testing 3 hidden neurons, lamb=1e-05, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.12e+00 | eval_loss: 4.75e+00 | reg: 6.43e+02 |: 100%|█| 50/50 [07:31<00:00,  9.03s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7641e+00 at 3 epoch\n",
      "Testing 3 hidden neurons, lamb=1e-06, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.07e+00 | eval_loss: 4.66e+00 | reg: 6.00e+02 |: 100%|█| 50/50 [07:34<00:00,  9.09s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6866e+00 at 3 epoch\n",
      "Testing 4 hidden neurons, lamb=1.0, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.64e+00 | eval_loss: 5.77e+00 | reg: 4.64e-01 |: 100%|█| 50/50 [03:09<00:00,  3.78s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3266e+01 at 12 epoch\n",
      "Testing 4 hidden neurons, lamb=0.1, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.25e+00 | eval_loss: 3.18e+00 | reg: 1.94e+01 |: 100%|█| 50/50 [03:16<00:00,  3.92s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7256e+00 at 6 epoch\n",
      "Testing 4 hidden neurons, lamb=0.01, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.39e+00 | eval_loss: 3.60e+00 | reg: 6.96e+01 |: 100%|█| 50/50 [03:11<00:00,  3.83s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6753e+00 at 3 epoch\n",
      "Testing 4 hidden neurons, lamb=0.001, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.47e+00 | eval_loss: 3.75e+00 | reg: 2.66e+02 |: 100%|█| 50/50 [03:32<00:00,  4.24s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6373e+00 at 6 epoch\n",
      "Testing 4 hidden neurons, lamb=0.0001, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.08e+00 | eval_loss: 3.96e+00 | reg: 5.39e+02 |: 100%|█| 50/50 [03:13<00:00,  3.87s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.8055e+00 at 5 epoch\n",
      "Testing 4 hidden neurons, lamb=1e-05, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.35e+00 | eval_loss: 3.74e+00 | reg: 6.80e+02 |: 100%|█| 50/50 [03:15<00:00,  3.91s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6467e+00 at 3 epoch\n",
      "Testing 4 hidden neurons, lamb=1e-06, grid=3\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.05e+00 | eval_loss: 4.06e+00 | reg: 5.41e+02 |: 100%|█| 50/50 [03:18<00:00,  3.96s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6114e+00 at 5 epoch\n",
      "Testing 4 hidden neurons, lamb=1.0, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.65e+00 | eval_loss: 5.77e+00 | reg: 4.87e-01 |: 100%|█| 50/50 [03:27<00:00,  4.16s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3266e+01 at 9 epoch\n",
      "Testing 4 hidden neurons, lamb=0.1, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.19e+00 | eval_loss: 3.24e+00 | reg: 2.24e+01 |: 100%|█| 50/50 [03:26<00:00,  4.13s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.8484e+00 at 7 epoch\n",
      "Testing 4 hidden neurons, lamb=0.01, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.15e+00 | eval_loss: 4.00e+00 | reg: 8.43e+01 |: 100%|█| 50/50 [03:30<00:00,  4.21s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6116e+00 at 4 epoch\n",
      "Testing 4 hidden neurons, lamb=0.001, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.69e+00 | eval_loss: 4.48e+00 | reg: 4.55e+02 |: 100%|█| 50/50 [03:35<00:00,  4.30s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.5530e+00 at 4 epoch\n",
      "Testing 4 hidden neurons, lamb=0.0001, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.88e+00 | eval_loss: 4.18e+00 | reg: 7.07e+02 |: 100%|█| 50/50 [03:36<00:00,  4.34s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6310e+00 at 4 epoch\n",
      "Testing 4 hidden neurons, lamb=1e-05, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.86e+00 | eval_loss: 4.31e+00 | reg: 7.76e+02 |: 100%|█| 50/50 [03:33<00:00,  4.27s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6020e+00 at 5 epoch\n",
      "Testing 4 hidden neurons, lamb=1e-06, grid=4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.71e+00 | eval_loss: 4.30e+00 | reg: 7.77e+02 |: 100%|█| 50/50 [03:36<00:00,  4.33s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6221e+00 at 5 epoch\n",
      "Testing 4 hidden neurons, lamb=1.0, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.64e+00 | eval_loss: 5.77e+00 | reg: 4.65e-01 |: 100%|█| 50/50 [03:59<00:00,  4.80s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3266e+01 at 12 epoch\n",
      "Testing 4 hidden neurons, lamb=0.1, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.31e+00 | eval_loss: 3.18e+00 | reg: 1.99e+01 |: 100%|█| 50/50 [03:50<00:00,  4.60s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6731e+00 at 7 epoch\n",
      "Testing 4 hidden neurons, lamb=0.01, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.82e+00 | eval_loss: 4.27e+00 | reg: 1.38e+02 |: 100%|█| 50/50 [04:01<00:00,  4.83s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6899e+00 at 7 epoch\n",
      "Testing 4 hidden neurons, lamb=0.001, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.52e+00 | eval_loss: 4.57e+00 | reg: 5.19e+02 |: 100%|█| 50/50 [04:04<00:00,  4.89s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7678e+00 at 5 epoch\n",
      "Testing 4 hidden neurons, lamb=0.0001, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.57e+00 | eval_loss: 4.39e+00 | reg: 7.73e+02 |: 100%|█| 50/50 [04:07<00:00,  4.95s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7931e+00 at 5 epoch\n",
      "Testing 4 hidden neurons, lamb=1e-05, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.51e+00 | eval_loss: 4.68e+00 | reg: 8.24e+02 |: 100%|█| 50/50 [04:07<00:00,  4.94s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.8021e+00 at 4 epoch\n",
      "Testing 4 hidden neurons, lamb=1e-06, grid=5\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.60e+00 | eval_loss: 4.37e+00 | reg: 7.97e+02 |: 100%|█| 50/50 [04:05<00:00,  4.90s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.8588e+00 at 3 epoch\n",
      "Testing 4 hidden neurons, lamb=1.0, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 5.64e+00 | eval_loss: 5.77e+00 | reg: 4.19e-01 |: 100%|█| 50/50 [09:49<00:00, 11.80s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 3.3266e+01 at 7 epoch\n",
      "Testing 4 hidden neurons, lamb=0.1, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.30e+00 | eval_loss: 3.20e+00 | reg: 1.98e+01 |: 100%|█| 50/50 [09:24<00:00, 11.28s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6646e+00 at 5 epoch\n",
      "Testing 4 hidden neurons, lamb=0.01, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.93e+00 | eval_loss: 4.12e+00 | reg: 1.19e+02 |: 100%|█| 50/50 [09:46<00:00, 11.73s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6140e+00 at 2 epoch\n",
      "Testing 4 hidden neurons, lamb=0.001, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.56e+00 | eval_loss: 4.22e+00 | reg: 4.44e+02 |: 100%|█| 50/50 [09:11<00:00, 11.03s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6622e+00 at 2 epoch\n",
      "Testing 4 hidden neurons, lamb=0.0001, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.11e+00 | eval_loss: 4.63e+00 | reg: 6.69e+02 |: 100%|█| 50/50 [09:04<00:00, 10.90s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6631e+00 at 2 epoch\n",
      "Testing 4 hidden neurons, lamb=1e-05, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.25e+00 | eval_loss: 4.65e+00 | reg: 6.74e+02 |: 100%|█| 50/50 [09:38<00:00, 11.58s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.6453e+00 at 5 epoch\n",
      "Testing 4 hidden neurons, lamb=1e-06, grid=6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.20e+00 | eval_loss: 4.74e+00 | reg: 5.81e+02 |: 100%|█| 50/50 [09:48<00:00, 11.77s/i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "✅ Best validation loss: 9.7050e+00 at 2 epoch\n",
      "\n",
      "✅ Top 5 Configurations:\n",
      "1. width=[8, 1, 1], grid=4, lamb=0.01| best_epoch=7 | loss=3.23119\n",
      "2. width=[8, 2, 1], grid=4, lamb=0.1| best_epoch=7 | loss=3.23543\n",
      "3. width=[8, 4, 1], grid=4, lamb=0.001| best_epoch=4 | loss=3.23555\n",
      "4. width=[8, 4, 1], grid=3, lamb=1e-06| best_epoch=5 | loss=3.23612\n",
      "5. width=[8, 2, 1], grid=5, lamb=0.01| best_epoch=5 | loss=3.23688\n"
     ]
    }
   ],
   "source": [
    "grid_search_kan_hybrid(dataset, Model4, max_steps=60, filename=filename_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcb62f9",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
