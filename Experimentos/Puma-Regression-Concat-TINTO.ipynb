{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d5668d4-717f-4e0a-a900-f1b70dc8ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the root directory\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Add the root directory to the Python path\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dace8b6-da36-4ed7-94f9-e7bfde57e588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from TINTOlib.tinto import TINTO\n",
    "from kan import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import traceback\n",
    "import time\n",
    "import gc\n",
    "import copy\n",
    "import traceback\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import csv\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787a631d-2f1e-4b05-845e-bb878447f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 381\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b71d2a5-d2ad-4b72-9869-8fdae3a98141",
   "metadata": {},
   "source": [
    "# BEST 3.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1580c162-4015-4cda-8492-b51350055b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=\"data/puma8NH\"\n",
    "x_col=[\"theta1\", \"theta2\", \"theta3\", \"thetad1\", \"thetad2\", \"thetad3\", \"tau1\",\"tau2\"]\n",
    "target_col=[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b03fb57-d803-49b8-8e18-3e3fb5bfe3d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5054e7cf-2a69-4b0a-97a5-02e10264038e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Dataset and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0d46e15-1287-4f1e-ba6a-cc05b3c8915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean(npy_filename, y_filename, x_col, target_col):\n",
    "    \"\"\"\n",
    "    Load the feature array (npy_filename) and label array (y_filename),\n",
    "    drop rows in the feature array that contain any NaNs, and apply\n",
    "    the same mask to the label array.\n",
    "    \"\"\"\n",
    "    # Load numpy arrays\n",
    "    X = np.load(os.path.join(folder, npy_filename))\n",
    "    y = np.load(os.path.join(folder, y_filename))\n",
    "    \n",
    "    # Ensure the number of rows matches between X and y\n",
    "    if X.shape[0] != y.shape[0]:\n",
    "        raise ValueError(\"The number of rows in {} and {} do not match.\".format(npy_filename, y_filename))\n",
    "    \n",
    "    # Create a boolean mask for rows that do NOT have any NaN values in X\n",
    "    valid_rows = ~np.isnan(X).any(axis=1)\n",
    "    #print(valid_rows)\n",
    "    # Filter both arrays using the valid_rows mask\n",
    "    X_clean = X[valid_rows]\n",
    "    y_clean = y[valid_rows]\n",
    "    \n",
    "    # Convert arrays to DataFrames\n",
    "    df_X = pd.DataFrame(X_clean)\n",
    "    df_y = pd.DataFrame(y_clean)\n",
    "    df_X.columns = x_col\n",
    "    df_y.columns = target_col\n",
    "    return df_X, df_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6443278-0647-432b-86d7-6883cf1aea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(X_train, y_train, X_test, y_test, X_val, y_val, image_model, problem_type, batch_size=32):\n",
    "    # Add target column to input for IGTD\n",
    "    X_train_full = X_train.copy()\n",
    "    X_train_full[\"target\"] = y_train.values\n",
    "\n",
    "    X_val_full = X_val.copy()\n",
    "    X_val_full[\"target\"] = y_val.values\n",
    "\n",
    "    X_test_full = X_test.copy()\n",
    "    X_test_full[\"target\"] = y_test.values\n",
    "\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(f'{images_folder}/train'):\n",
    "        image_model.fit_transform(X_train_full, f'{images_folder}/train')\n",
    "        image_model.saveHyperparameters(f'{images_folder}/model.pkl')\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    # Load image paths\n",
    "    imgs_train = pd.read_csv(os.path.join(f'{images_folder}/train', f'{problem_type}.csv'))\n",
    "    imgs_train[\"images\"] = images_folder + \"/train/\" + imgs_train[\"images\"]\n",
    "\n",
    "    if not os.path.exists(f'{images_folder}/val'):\n",
    "        image_model.transform(X_val_full, f'{images_folder}/val')\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    imgs_val = pd.read_csv(os.path.join(f'{images_folder}/val', f'{problem_type}.csv'))\n",
    "    imgs_val[\"images\"] = images_folder + \"/val/\" + imgs_val[\"images\"]\n",
    "\n",
    "    if not os.path.exists(f'{images_folder}/test'):\n",
    "        image_model.transform(X_test_full, f'{images_folder}/test')\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    imgs_test = pd.read_csv(os.path.join(f'{images_folder}/test', f'{problem_type}.csv'))\n",
    "    imgs_test[\"images\"] = images_folder + \"/test/\" + imgs_test[\"images\"]\n",
    "\n",
    "    # Image data\n",
    "    X_train_img = np.array([cv2.imread(img) for img in imgs_train[\"images\"]])\n",
    "    X_val_img = np.array([cv2.imread(img) for img in imgs_val[\"images\"]])\n",
    "    X_test_img = np.array([cv2.imread(img) for img in imgs_test[\"images\"]])\n",
    "\n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale numerical data\n",
    "    X_train_num = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_val_num = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n",
    "    X_test_num = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "    attributes = len(X_train_num.columns)\n",
    "    height, width, channels = X_train_img[0].shape\n",
    "    imgs_shape = (channels, height, width)\n",
    "\n",
    "    print(\"Images shape: \", imgs_shape)\n",
    "    print(\"Attributes: \", attributes)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_num_tensor = torch.as_tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor = torch.as_tensor(X_val_num.values, dtype=torch.float32)\n",
    "    X_test_num_tensor = torch.as_tensor(X_test_num.values, dtype=torch.float32)\n",
    "    X_train_img_tensor = torch.as_tensor(X_train_img, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n",
    "    X_val_img_tensor = torch.as_tensor(X_val_img, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n",
    "    X_test_img_tensor = torch.as_tensor(X_test_img, dtype=torch.float32).permute(0, 3, 1, 2) / 255.0\n",
    "    y_train_tensor = torch.as_tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_val_tensor = torch.as_tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_test_tensor = torch.as_tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_num_tensor, X_train_img_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_num_tensor, X_val_img_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_num_tensor, X_test_img_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes, imgs_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4c2d115-8c10-4670-afb9-88009569c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_coordinate_and_xcol(coordinate, x_col):\n",
    "    \"\"\"\n",
    "    Given a coordinate (tuple of row, col arrays) and x_col feature list,\n",
    "    return completed coordinate and x_col including empty positions.\n",
    "\n",
    "    Empty positions are filled with labels: 'Ex1', 'Ex2', ...\n",
    "    \"\"\"\n",
    "\n",
    "    row_coords, col_coords = coordinate\n",
    "    max_row = row_coords.max()\n",
    "    max_col = col_coords.max()\n",
    "    \n",
    "    # All possible coordinate slots\n",
    "    full_coords = set((r, c) for r in range(max_row + 1) for c in range(max_col + 1))\n",
    "    current_coords = set(zip(row_coords, col_coords))\n",
    "    missing_coords = sorted(full_coords - current_coords)\n",
    "\n",
    "    # Create updated coordinate arrays\n",
    "    new_row_coords = list(row_coords)\n",
    "    new_col_coords = list(col_coords)\n",
    "    new_x_col = list(x_col)\n",
    "\n",
    "    for idx, (r, c) in enumerate(missing_coords):\n",
    "        new_row_coords.append(r)\n",
    "        new_col_coords.append(c)\n",
    "        new_x_col.append(f\"Ex{idx+1}\")\n",
    "\n",
    "    completed_coordinate = (np.array(new_row_coords), np.array(new_col_coords))\n",
    "    return completed_coordinate, new_x_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5c7ac21-b8f8-4de9-b756-9da684567bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_mapping(x_col, coordinate, scale=(4,4)):\n",
    "    grid = np.full(scale, \"\", dtype=object)\n",
    "    rows, cols = coordinate\n",
    "    for i, (r, c) in enumerate(zip(rows, cols)):\n",
    "        if i < len(x_col):\n",
    "            grid[r, c] = x_col[i]\n",
    "        else:\n",
    "            grid[r, c] = \"?\"\n",
    "    \n",
    "    plt.figure(figsize=(scale[1] * 2, scale[0] * 2))\n",
    "    for i in range(scale[0]):\n",
    "        for j in range(scale[1]):\n",
    "            plt.text(j, i, grid[i, j], ha='center', va='center', fontsize=10,\n",
    "                     bbox=dict(facecolor='white', edgecolor='gray'))\n",
    "    \n",
    "    plt.xticks(np.arange(scale[1]))\n",
    "    plt.yticks(np.arange(scale[0]))\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Feature → Pixel Mapping\")\n",
    "    plt.gca().invert_yaxis()  # So row 0 is at the top\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6224cc89-a931-413b-a3f1-3bccd0d3e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_loader(loader):\n",
    "    \"\"\"\n",
    "    Combines all batches from a DataLoader into three tensors.\n",
    "    Assumes each batch is a tuple: (mlp_tensor, img_tensor, target_tensor)\n",
    "    \"\"\"\n",
    "    mlp_list, img_list, target_list = [], [], []\n",
    "    for mlp, img, target in loader:\n",
    "        mlp_list.append(mlp)\n",
    "        img_list.append(img)\n",
    "        target_list.append(target)\n",
    "    return torch.cat(mlp_list, dim=0), torch.cat(img_list, dim=0), torch.cat(target_list, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ab0b6-acee-45de-ad6d-7cb6982f79c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions for KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef657551-05a2-4430-8bbd-70d4bf24b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_rmse(output, val_target):\n",
    "    \"\"\"\n",
    "    Computes the root mean squared error (RMSE) between output and val_target.\n",
    "\n",
    "    Args:\n",
    "        output (torch.Tensor): The predicted output tensor.\n",
    "        val_target (torch.Tensor): The ground truth tensor.\n",
    "    \n",
    "    Returns:\n",
    "        float: The RMSE value.\n",
    "    \"\"\"\n",
    "    mse = torch.mean((output - val_target) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    return rmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "496af9fd-74ab-4fd0-9b77-944902aa3a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sorted_feature_importance(columns, importances, plot=False):\n",
    "    # Move to CPU and numpy if it's a GPU tensor\n",
    "    if isinstance(importances, torch.Tensor):\n",
    "        importances = importances.detach().cpu().numpy()\n",
    "\n",
    "    # Pair columns and importances and sort by importance descending\n",
    "    sorted_pairs = sorted(zip(columns, importances), key=lambda x: x[1], reverse=True)\n",
    "    print(sorted_pairs)\n",
    "    sorted_columns, sorted_importances = zip(*sorted_pairs)\n",
    "    if plot:\n",
    "        # Create the bar plot\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.barh(sorted_columns, sorted_importances, color='royalblue')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title('KAN Feature Importances')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    return sorted_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab5a49-d42a-4b73-8964-b0dce89f4dfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Grad CAM Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91437092-eb7c-464b-bed8-d608e690dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_to_feature_relevance(heatmap, coordinate, x_col, zoom=1):\n",
    "    \"\"\"\n",
    "    Map heatmap pixel intensities to their corresponding features using coordinate and zoom.\n",
    "    Returns a dictionary of {feature_name: relevance_score}.\n",
    "    \"\"\"\n",
    "    feature_scores = {}\n",
    "\n",
    "    for i, col in enumerate(x_col):\n",
    "        if i < len(coordinate[0]):\n",
    "            r, c = coordinate[0][i], coordinate[1][i]\n",
    "            ry = r * zoom + zoom // 2\n",
    "            cx = c * zoom + zoom // 2\n",
    "            if ry < heatmap.shape[0] and cx < heatmap.shape[1]:\n",
    "                feature_scores[col] = heatmap[ry, cx].item()\n",
    "\n",
    "    return feature_scores\n",
    "\n",
    "def plot_feature_relevance_bar(feature_scores, plot=False):\n",
    "    \"\"\"\n",
    "    Plots a horizontal bar chart of feature relevance from Grad-CAM heatmap.\n",
    "    \"\"\"\n",
    "    sorted_scores = sorted(feature_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    print(sorted_scores)\n",
    "    features, scores = zip(*sorted_scores)\n",
    "    if plot:\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.barh(features, scores, color='royalblue')\n",
    "        plt.xlabel(\"Grad-CAM Relevance\")\n",
    "        plt.title(\"Feature Relevance for Test\")\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    return sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9799c37d-4ad2-4e27-939b-2feffa63c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_feature_relevance_from_val(model, model_state, val_inputs, val_imgs, coordinate, x_col, zoom=1):\n",
    "    \"\"\"\n",
    "    Computes average Grad-CAM feature relevance over all validation instances with a tqdm progress bar.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        model_state: Trained weights to be loaded\n",
    "        val_inputs: List or tensor of numerical inputs\n",
    "        val_imgs: List or tensor of image inputs\n",
    "        coordinate: IGTD-style coordinate tuple (row array, col array)\n",
    "        x_col: List of feature names (including extras)\n",
    "        zoom: Zoom level used when generating the images\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of average feature relevance\n",
    "    \"\"\"\n",
    "    accumulated_scores = {feature: [] for feature in x_col}\n",
    "\n",
    "    for num_input, img_input in tqdm(zip(val_inputs, val_imgs), total=len(val_inputs), desc=\"Computing Grad-CAM\"):\n",
    "        heatmap = grad_cam_side_by_side(\n",
    "            model=model,\n",
    "            model_state=model_state,\n",
    "            num_input=num_input,\n",
    "            img_input=img_input,\n",
    "            coordinate=coordinate,\n",
    "            x_col=x_col,\n",
    "            zoom=zoom,\n",
    "            show=False\n",
    "        )\n",
    "        scores = heatmap_to_feature_relevance(heatmap, coordinate, x_col, zoom)\n",
    "        for feature, value in scores.items():\n",
    "            accumulated_scores[feature].append(value)\n",
    "\n",
    "    # Compute average\n",
    "    avg_scores = {feature: float(np.mean(values)) if values else 0.0\n",
    "                  for feature, values in accumulated_scores.items()}\n",
    "\n",
    "    return avg_scores\n",
    "\n",
    "\n",
    "def plot_avg_feature_relevance_from_val(model, val_inputs, val_imgs, coordinate, x_col, zoom=1):\n",
    "    \"\"\"\n",
    "    Combines all steps: compute average relevance and plot.\n",
    "    \"\"\"\n",
    "    avg_scores = compute_avg_feature_relevance_from_val(model, val_inputs, val_imgs, coordinate, x_col, zoom)\n",
    "    plot_feature_relevance_bar(avg_scores)\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbba9e-3360-4f05-b3c5-1b1cac214ffd",
   "metadata": {},
   "source": [
    "### CNN Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3b94d-b95b-44ad-a4dd-84781fdf0a7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hybrid Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e554e5a-af4d-4f84-bc39-a076c11b2389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mkan_vs_cnn_relevance(feature_scores, mkan_len):\n",
    "    if isinstance(feature_scores, torch.Tensor):\n",
    "        feature_scores = feature_scores.detach().cpu().numpy()\n",
    "\n",
    "    mkan_relevance = feature_scores[:mkan_len].sum()\n",
    "    cnn_relevance = feature_scores[mkan_len:].sum()\n",
    "    m_kan_relevance_perct = float(mkan_relevance/(mkan_relevance+cnn_relevance))\n",
    "    cnn_relevance_perct = float(cnn_relevance/(mkan_relevance+cnn_relevance))\n",
    "    print(f\"M_KAN Relevance: {m_kan_relevance_perct}\")\n",
    "    print(f\"CNN Relevance: {cnn_relevance_perct}\")\n",
    "    return m_kan_relevance_perct, cnn_relevance_perct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a73aa349-137e-43f2-9d51-d788b1585dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_hybrid_dataloaders(model, \n",
    "                           dataset,\n",
    "                           opt=\"AdamW\", \n",
    "                           steps=100, \n",
    "                           log=1, \n",
    "                           lamb=0., \n",
    "                           lamb_l1=1., \n",
    "                           lamb_entropy=2., \n",
    "                           lamb_coef=0., \n",
    "                           lamb_coefdiff=0., \n",
    "                           update_grid=True, \n",
    "                           grid_update_num=10, \n",
    "                           loss_fn=None, \n",
    "                           lr=1., \n",
    "                           start_grid_update_step=-1, \n",
    "                           stop_grid_update_step=50, \n",
    "                           batch=-1,\n",
    "                           metrics=None, \n",
    "                           save_fig=False, \n",
    "                           in_vars=None, \n",
    "                           out_vars=None, \n",
    "                           beta=3, \n",
    "                           save_fig_freq=1, \n",
    "                           img_folder='./video', \n",
    "                           singularity_avoiding=False, \n",
    "                           y_th=1000., \n",
    "                           reg_metric='edge_forward_spline_n', \n",
    "                           display_metrics=None,\n",
    "                           sum_f_reg=True):\n",
    "    \"\"\"\n",
    "    Trains the hybrid model (with a KAN branch and a CNN branch) using a steps-based loop\n",
    "    adapted from KAN.fit(), with grid updates and regularization.\n",
    "    \n",
    "    Instead of a single dataset dict, this function accepts three DataLoaders:\n",
    "        - train_loader: provides (mlp, img, target) for training\n",
    "        - val_loader: provides (mlp, img, target) for evaluation during training\n",
    "        - test_loader: provides (mlp, img, target) for validation\n",
    "\n",
    "    Internally, the function combines each loader into a dataset dictionary.\n",
    "    \n",
    "    Returns:\n",
    "        results: dictionary containing training loss, evaluation loss, regularization values,\n",
    "                 and any additional metrics recorded during training.\n",
    "    \"\"\"\n",
    "    # Warn if regularization is requested but model's internal flag isn't enabled.\n",
    "    if lamb > 0. and not getattr(model.m_kan, \"save_act\", False):\n",
    "        print(\"setting lamb=0. If you want to set lamb > 0, set model.m_kan.save_act=True\")\n",
    "    \n",
    "    # Disable symbolic processing for training if applicable (KAN internal logic)\n",
    "    if hasattr(model.m_kan, \"disable_symbolic_in_fit\"):\n",
    "        old_save_act, old_symbolic_enabled = model.m_kan.disable_symbolic_in_fit(lamb)\n",
    "        f_old_save_act, f_old_symbolic_enabled = model.final_kan.disable_symbolic_in_fit(lamb)\n",
    "    else:\n",
    "        old_save_act, old_symbolic_enabled = None, None\n",
    "\n",
    "    pbar = tqdm(range(steps), desc='Training', ncols=100)\n",
    "\n",
    "    # Default loss function (mean squared error) if not provided\n",
    "    if loss_fn is None:\n",
    "        loss_fn = lambda x, y: torch.mean((x - y) ** 2)\n",
    "\n",
    "    # Determine grid update frequency\n",
    "    grid_update_freq = int(stop_grid_update_step / grid_update_num) if grid_update_num > 0 else 1\n",
    "\n",
    "    # Determine total number of training examples\n",
    "    n_train = dataset[\"train_input\"].shape[0]\n",
    "    n_eval  = dataset[\"val_input\"].shape[0]  # using val set for evaluation during training\n",
    "    batch_size = n_train if batch == -1 or batch > n_train else batch\n",
    "\n",
    "    # Set up optimizer: choose between Adam and LBFGS (removed tolerance_ys)\n",
    "    if opt == \"AdamW\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    elif opt == \"LBFGS\":        \n",
    "        optimizer = LBFGS(model.parameters(), lr=lr, history_size=10, \n",
    "                          line_search_fn=\"strong_wolfe\", \n",
    "                          tolerance_grad=1e-32, \n",
    "                          tolerance_change=1e-32, \n",
    "                          tolerance_ys=1e-32)\n",
    "    else:\n",
    "        raise ValueError(\"Optimizer not recognized. Use 'Adam' or 'LBFGS'.\")\n",
    "\n",
    "    # Prepare results dictionary.\n",
    "    results = {'train_loss': [], 'eval_loss': [], 'reg': []}\n",
    "\n",
    "    \n",
    "    if metrics is not None:\n",
    "        for metric in metrics:\n",
    "            results[metric.__name__] = []\n",
    "\n",
    "    best_model_state = None\n",
    "    best_epoch = -1\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for step in pbar:\n",
    "        # Randomly sample indices for a mini-batch from the training set.\n",
    "        train_indices = np.random.choice(n_train, batch_size, replace=False)\n",
    "        # Use full evaluation set for evaluation; you can also sample if desired.\n",
    "        eval_indices = np.arange(n_eval)\n",
    "        cached_loss = {}\n",
    "        # Closure for LBFGS\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            mlp_batch = dataset[\"train_input\"][train_indices]\n",
    "            img_batch = dataset[\"train_img\"][train_indices]\n",
    "            target_batch = dataset[\"train_label\"][train_indices]\n",
    "            outputs = model(mlp_batch, img_batch)\n",
    "            train_loss = loss_fn(outputs, target_batch)\n",
    "            # Compute regularization term if enabled.\n",
    "            if hasattr(model.m_kan, \"save_act\") and model.m_kan.save_act:\n",
    "                if reg_metric == 'edge_backward':\n",
    "                    model.m_kan.attribute()\n",
    "                    model.final_kan.attribute()\n",
    "                if reg_metric == 'node_backward':\n",
    "                    model.m_kan.node_attribute()\n",
    "                    model.final_kan.node_attribute()\n",
    "                reg_val_inner = model.m_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "                if sum_f_reg:\n",
    "                    reg_val_inner += model.final_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            else:\n",
    "                reg_val_inner = torch.tensor(0., device=device)\n",
    "            loss_val_inner = train_loss + lamb * reg_val_inner\n",
    "            loss_val_inner.backward()\n",
    "\n",
    "            cached_loss['loss'] = loss_val_inner.detach()\n",
    "            cached_loss['reg'] = reg_val_inner.detach()\n",
    "            return loss_val_inner\n",
    "\n",
    "        # Perform grid update if applicable.\n",
    "        if (step % grid_update_freq == 0 and step < stop_grid_update_step \n",
    "            and update_grid and step >= start_grid_update_step):\n",
    "            \n",
    "            mlp_batch = dataset['train_input'][train_indices]\n",
    "            cnn_batch = dataset['train_img'][train_indices]\n",
    "            \n",
    "            model.m_kan.update_grid(mlp_batch)\n",
    "            #cnn_output = model.cnn_branch(cnn_batch)  # Process image input\n",
    "            concatenated = model.get_concat_output(mlp_batch, cnn_batch)\n",
    "            #concatenated = torch.cat((mlp_batch, cnn_output), dim=1)\n",
    "            model.final_kan.update_grid(concatenated)\n",
    "\n",
    "        # Perform an optimizer step.\n",
    "        if opt == \"LBFGS\":\n",
    "            optimizer.step(closure)\n",
    "            loss_val = cached_loss['loss']\n",
    "            reg_val = cached_loss['reg']\n",
    "        else:  # AdamW branch\n",
    "            optimizer.zero_grad()\n",
    "            mlp_batch = dataset[\"train_input\"][train_indices]\n",
    "            img_batch = dataset[\"train_img\"][train_indices]\n",
    "            target_batch = dataset[\"train_label\"][train_indices]\n",
    "            outputs = model(mlp_batch, img_batch)\n",
    "            train_loss = loss_fn(outputs, target_batch)\n",
    "            if hasattr(model.m_kan, \"save_act\") and model.m_kan.save_act:\n",
    "                if reg_metric == 'edge_backward':\n",
    "                    model.m_kan.attribute()\n",
    "                    model.final_kan.attribute()\n",
    "                if reg_metric == 'node_backward':\n",
    "                    model.m_kan.node_attribute()\n",
    "                    model.final_kan.node_attribute()\n",
    "                reg_val = model.m_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "                if sum_f_reg:\n",
    "                    reg_val += model.final_kan.get_reg(reg_metric, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff)\n",
    "            else:\n",
    "                reg_val = torch.tensor(0., device=device)\n",
    "            loss_val = train_loss + lamb * reg_val\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mlp_eval = dataset[\"val_input\"][eval_indices]\n",
    "            img_eval = dataset[\"val_img\"][eval_indices]\n",
    "            target_eval = dataset[\"val_label\"][eval_indices]\n",
    "            eval_loss = loss_fn(model(mlp_eval, img_eval), target_eval)\n",
    "\n",
    "        eval_loss_item = torch.sqrt(eval_loss.detach()).item()\n",
    "        # Record results (using square-root of loss similar to KAN.fit)\n",
    "        results['train_loss'].append(torch.sqrt(loss_val.detach()).item())\n",
    "        results['eval_loss'].append(eval_loss_item)\n",
    "        results['reg'].append(reg_val.detach().item())\n",
    "\n",
    "        if metrics is not None:\n",
    "            for metric in metrics:\n",
    "                # Here, we assume each metric returns a tensor.\n",
    "                results[metric.__name__].append(metric().item())\n",
    "\n",
    "        if eval_loss < best_loss:\n",
    "            best_epoch = step\n",
    "            best_loss = eval_loss\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # Update progress bar.\n",
    "        if display_metrics is None:\n",
    "            pbar.set_description(\"| train_loss: %.2e | eval_loss: %.2e | reg: %.2e |\" %\n",
    "                                 (torch.sqrt(loss_val.detach()).item(),\n",
    "                                  torch.sqrt(eval_loss.detach()).item(),\n",
    "                                  reg_val.detach().item()))\n",
    "        else:\n",
    "            desc = \"\"\n",
    "            data = []\n",
    "            for metric in display_metrics:\n",
    "                desc += f\" {metric}: %.2e |\"\n",
    "                data.append(results[metric.__name__][-1])\n",
    "            pbar.set_description(desc % tuple(data))\n",
    "\n",
    "        # Optionally save a figure snapshot.\n",
    "        if save_fig and step % save_fig_freq == 0:\n",
    "            save_act_backup = getattr(model.m_kan, \"save_act\", False)\n",
    "            model.m_kan.save_act = True\n",
    "            model.plot(folder=img_folder, in_vars=in_vars, out_vars=out_vars, title=f\"Step {step}\", beta=beta)\n",
    "            plt.savefig(os.path.join(img_folder, f\"{step}.jpg\"), bbox_inches='tight', dpi=200)\n",
    "            plt.close()\n",
    "            model.m_kan.save_act = save_act_backup\n",
    "                \n",
    "        if math.isnan(eval_loss_item):\n",
    "            break\n",
    "    # Restore original settings if applicable.\n",
    "    if old_symbolic_enabled is not None:\n",
    "        model.m_kan.symbolic_enabled = old_symbolic_enabled\n",
    "    if hasattr(model.m_kan, \"log_history\"):\n",
    "        model.m_kan.log_history('fit')\n",
    "    print(f\"Best epoch {best_epoch}\")\n",
    "    return best_model_state, results, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b2674e0-7282-46a1-aa61-f72459feb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def try_create_model(model_class, attributes, imgs_shape, kan_neurons, kan_grid, cnn_bottleneck_dim, alpha, hidden_dim, embed_dim, num_heads):\n",
    "    try:\n",
    "        model = model_class(attributes, imgs_shape, kan_neurons, kan_grid,\n",
    "                            cnn_bottleneck_dim=cnn_bottleneck_dim, alpha=alpha, hidden_dim=hidden_dim, embed_dim=embed_dim, num_heads=num_heads)\n",
    "\n",
    "        # Test the model with a sample input\n",
    "        num_input = torch.randn(4, attributes)\n",
    "        img_input = torch.randn(4, *imgs_shape)\n",
    "        output = model(num_input, img_input)\n",
    "        \n",
    "        print(f\"Successfully created and tested {model_class.__name__}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating or testing {model_class.__name__}:\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efc69566-0316-47ec-8c23-bc6d5cf685e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_branch_relevance(model, best_model_state):\n",
    "    avg_scores = compute_avg_feature_relevance_from_val(\n",
    "        model=model,\n",
    "        model_state=best_model_state,\n",
    "        val_inputs=dataset[\"test_input\"],\n",
    "        val_imgs=dataset[\"test_img\"],\n",
    "        coordinate=completed_coordinate,\n",
    "        x_col=completed_x_col,\n",
    "        zoom=2\n",
    "    )\n",
    "    return plot_feature_relevance_bar(avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb693e0d-e74d-4b67-b284-1b9d71e05295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_plot_relevance(model_class, kan_neurons, kan_grid, lamb, steps, cnn_bottleneck_dim=-1, alpha=-1, hidden_dim=-1, embed_dim=-1, num_heads=-1, n_kan_len=None, filename=None, opt_col_val=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model = try_create_model(model_class, attributes, imgs_shape, kan_neurons=kan_neurons, kan_grid=kan_grid,\n",
    "                             cnn_bottleneck_dim=cnn_bottleneck_dim, alpha=alpha, hidden_dim=hidden_dim, embed_dim=embed_dim, num_heads=num_heads)\n",
    "    best_model_state, metrics3, best_epoch = fit_hybrid_dataloaders(model, dataset, opt=\"LBFGS\", lamb=lamb, steps=steps)\n",
    "    model.load_state_dict(best_model_state)\n",
    "    rmse = average_rmse(model(dataset['test_input'], dataset['test_img']), dataset['test_label'])\n",
    "    print(rmse)\n",
    "    #plot_training_RMSE(metrics3['train_loss'], metrics3['eval_loss'])\n",
    "    if not n_kan_len:\n",
    "        n_kan_len = kan_neurons\n",
    "    k_rel, cnn_rel = print_mkan_vs_cnn_relevance(model.final_kan.feature_score, mkan_len=n_kan_len)\n",
    "    #plot_mkan_vs_cnn_relevance(model.final_kan.feature_score, mkan_len=kan_neurons)\n",
    "    kan_mrf = \"\"#plot_sorted_feature_importance(x_col, model.m_kan.feature_score)\n",
    "    \n",
    "    cnn_mrf = \"\" #cnn_branch_relevance(model, best_model_state)\n",
    "\n",
    "    append_row_to_csv(filename, kan_neurons, kan_grid, lamb, opt_col_val, rmse, best_epoch, k_rel, cnn_rel, kan_mrf, cnn_mrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a75e49-a28c-49b9-acb1-b46d51ca540d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Write metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b081106-5c06-4f60-919c-8bc71e830b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv_with_header(filename, columns_opt):\n",
    "    header=['kan_neurons', 'kan_grid', 'lamb', columns_opt, 'RMSE','Best_Epoch','KAN_Relevance','CNN_Relevance','KAN M.R.F.','CNN M.R.F.']\n",
    "    \"\"\"Creates a CSV file with a given header.\"\"\"\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1342965-5c7b-4a9e-a7c0-e18f5505531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_top_3(pairs):\n",
    "    return '\\n'.join(f\"{k}: {v:.2f}\" for k, v in pairs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9803124-29a9-4dc6-8d54-3dc8137bda01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_row_to_csv(filename, kan_neurons, kan_grid, lamb, opt_col_val, acc, best_epoch, k_rel, cnn_rel, kan_mrf, cnn_mrf):\n",
    "    row = [kan_neurons, kan_grid, lamb, opt_col_val, acc, best_epoch, k_rel, cnn_rel, format_top_3(kan_mrf), format_top_3(cnn_mrf)]\n",
    "    \"\"\"Appends a single row to an existing CSV file.\"\"\"\n",
    "    if not os.path.isfile(filename):\n",
    "        raise FileNotFoundError(f\"{filename} does not exist. Please create the file first with a header.\")\n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8c13c-88aa-4cbd-9819-bf1d37c8027a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Models Class Hybrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2c4625e-ed3d-43de-85d0-126f94916b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3_1(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, kan_neurons, kan_grid, cnn_bottleneck_dim=-1, alpha=-1, hidden_dim=-1, embed_dim=-1, num_heads=-1, device=device):\n",
    "        super(Model3_1, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], 16, kernel_size=3, padding=2),     # out: 16 x 9 x 9\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                            # out: 16 x 4 x 4\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=2),                # out: 32 x 5 x 5\n",
    "            nn.LayerNorm([32, 13, 13]),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "\n",
    "        # Dummy pass to get flattened size\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        # Bottleneck layer\n",
    "        self.cnn_bottleneck = nn.Linear(self.flat_size, cnn_bottleneck_dim).to(device)\n",
    "\n",
    "        # KAN branch\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, kan_neurons],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Final KAN layer\n",
    "        self.final_kan = KAN(\n",
    "            width=[cnn_bottleneck_dim + kan_neurons, 1],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        dummy_input = torch.zeros(1, *imgs_shape, device=self.device)\n",
    "        x = self.cnn_branch(dummy_input)\n",
    "        return x.shape[1]\n",
    "\n",
    "    def get_concat_output(self, mlp_input, cnn_input):\n",
    "        kan_input = mlp_input.to(self.device)\n",
    "        cnn_input = cnn_input.to(self.device)\n",
    "\n",
    "        conv_out = self.cnn_branch(cnn_input)\n",
    "        cnn_output = self.cnn_bottleneck(conv_out)\n",
    "\n",
    "        kan_output = self.m_kan(kan_input)\n",
    "\n",
    "        return torch.cat((kan_output, cnn_output), dim=1)\n",
    "\n",
    "    def forward(self, mlp_input, cnn_input):\n",
    "        concat_output = self.get_concat_output(mlp_input, cnn_input)\n",
    "        return self.final_kan(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0116754e-6c80-4271-99db-c698d41381fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3_2(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, kan_neurons, kan_grid, cnn_bottleneck_dim=-1, alpha=-1, hidden_dim=-1, embed_dim=-1, num_heads=-1, device=device):\n",
    "        super(Model3_2, self).__init__()\n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], 16, kernel_size=3, padding=2),     # out: 16 x 9 x 9\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                            # out: 16 x 4 x 4\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=2),                # out: 32 x 5 x 5\n",
    "            nn.LayerNorm([32, 13, 13]),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "        \n",
    "        # Final KAN layers\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, kan_neurons],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Calculate the size of the flattened output\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        # Final MLP layers\n",
    "        self.final_kan = KAN(\n",
    "            width=[self.flat_size + kan_neurons, 1],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "        self.alpha = alpha\n",
    "\n",
    "\n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        # Forward pass with dummy input to calculate flat size\n",
    "        dummy_input = torch.zeros(4, *imgs_shape, device=device)\n",
    "        x = self.cnn_branch(dummy_input)\n",
    "        return x.size(1)\n",
    "\n",
    "    def get_concat_output(self, mlp_input, cnn_input):\n",
    "        # Ensure inputs are moved to the correct device\n",
    "        kan_input = mlp_input.to(self.device)\n",
    "        cnn_input = cnn_input.to(self.device)\n",
    "        \n",
    "        cnn_output = self.cnn_branch(cnn_input)  # Process image input\n",
    "        cnn_output = cnn_output * self.alpha\n",
    "        kan_output = self.m_kan(kan_input)  # Process numerical input\n",
    "        \n",
    "        return torch.cat((kan_output, cnn_output), dim=1)\n",
    "\n",
    "    \n",
    "    def forward(self, mlp_input, cnn_input):\n",
    "        concat_output = self.get_concat_output(mlp_input, cnn_input)\n",
    "        return self.final_kan(concat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8787ef54-4d7a-4df6-9ac3-ecaf94035385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3_3(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, kan_neurons, kan_grid, cnn_bottleneck_dim=-1, alpha=-1, hidden_dim=-1, embed_dim=-1, num_heads=-1, device=device):\n",
    "        super(Model3_3, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], 16, kernel_size=3, padding=2),     # out: 16 x 9 x 9\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                            # out: 16 x 4 x 4\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=2),                # out: 32 x 5 x 5\n",
    "            nn.LayerNorm([32, 13, 13]),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        # KAN branch\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, kan_neurons],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Gating MLP: inputs are concatenated CNN + KAN representations\n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(self.flat_size + kan_neurons, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # Output ∈ [0,1]\n",
    "        ).to(device)\n",
    "\n",
    "        # Final regressor (KAN layer)\n",
    "        self.final_kan = KAN(\n",
    "            width=[kan_neurons + self.flat_size, 1],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        dummy_input = torch.zeros(4, *imgs_shape, device=self.device)\n",
    "        x = self.cnn_branch(dummy_input)\n",
    "        return x.size(1)\n",
    "\n",
    "    def get_concat_output(self, mlp_input, cnn_input):\n",
    "        mlp_input = mlp_input.to(self.device)\n",
    "        cnn_input = cnn_input.to(self.device)\n",
    "\n",
    "        kan_out = self.m_kan(mlp_input)                  # shape: (B, kan_neurons)\n",
    "        cnn_out = self.cnn_branch(cnn_input)             # shape: (B, cnn_flat)\n",
    "\n",
    "        concat = torch.cat((kan_out, cnn_out), dim=1)    # For gating\n",
    "        alpha = self.gate_net(concat)                    # shape: (B, 1)\n",
    "\n",
    "        gated_kan = (1 - alpha) * kan_out                # shape: (B, kan_neurons)\n",
    "        gated_cnn = alpha * cnn_out                      # shape: (B, cnn_flat)\n",
    "\n",
    "        return torch.cat((gated_kan, gated_cnn), dim=1)  # shape: (B, total)\n",
    "\n",
    "    def forward(self, mlp_input, cnn_input):\n",
    "        fused = self.get_concat_output(mlp_input, cnn_input)\n",
    "        return self.final_kan(fused)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c331018-1b5a-4485-9473-8f755ec9a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3_4(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, kan_neurons, kan_grid, cnn_bottleneck_dim=-1, alpha=-1, hidden_dim=-1, embed_dim=-1, num_heads=-1, device=device):\n",
    "        super(Model3_4, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # CNN branch\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(imgs_shape[0], 16, kernel_size=3, padding=2),     # out: 16 x 9 x 9\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                                            # out: 16 x 4 x 4\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=2),                # out: 32 x 5 x 5\n",
    "            nn.LayerNorm([32, 13, 13]),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Flatten()\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "        self.flat_size = self._get_flat_size(imgs_shape)\n",
    "\n",
    "        # KAN Branch\n",
    "        self.m_kan = KAN(\n",
    "            width=[attributes, kan_neurons],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.query_proj = nn.Linear(kan_neurons, embed_dim).to(device)\n",
    "        self.key_proj = nn.Linear(self.flat_size, embed_dim).to(device)\n",
    "        self.value_proj = nn.Linear(self.flat_size, embed_dim).to(device)\n",
    "\n",
    "        # Attention module\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True).to(device)\n",
    "\n",
    "        # Final regression layer (KAN again)\n",
    "        self.final_kan = KAN(\n",
    "            width=[embed_dim, 1],\n",
    "            grid=kan_grid,\n",
    "            k=3,\n",
    "            seed=SEED,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "\n",
    "    def _get_flat_size(self, imgs_shape):\n",
    "        dummy_input = torch.zeros(1, *imgs_shape, device=self.device)\n",
    "        return self.cnn_branch(dummy_input).shape[1]\n",
    "\n",
    "    \n",
    "    def get_concat_output(self, mlp_input, cnn_input):\n",
    "        # Get KAN and CNN outputs\n",
    "        kan_out = self.m_kan(mlp_input.to(self.device))  # [B, D_kan]\n",
    "        cnn_out = self.cnn_branch(cnn_input.to(self.device))  # [B, D_cnn]\n",
    "\n",
    "        # Project into Q, K, V space\n",
    "        Q = self.query_proj(kan_out).unsqueeze(1)  # [B, 1, E]\n",
    "        K = self.key_proj(cnn_out).unsqueeze(1)    # [B, 1, E]\n",
    "        V = self.value_proj(cnn_out).unsqueeze(1)  # [B, 1, E]\n",
    "\n",
    "        # Cross-attention: KAN attends to CNN\n",
    "        attn_out, _ = self.attn(Q, K, V)  # [B, 1, E]\n",
    "        attn_out = attn_out.squeeze(1)   # [B, E]\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "\n",
    "    def forward(self, mlp_input, cnn_input):\n",
    "        attn_out = self.get_concat_output(mlp_input, cnn_input)\n",
    "\n",
    "        return self.final_kan(attn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb7b42-d9ea-4e45-9b6b-da78e1bb43d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Dataset and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aee6586c-7ff1-40f9-9768-c6adccae02bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_and_clean('N_train.npy', 'y_train.npy',x_col, target_col)\n",
    "X_test, y_test   = load_and_clean('N_test.npy',  'y_test.npy', x_col, target_col)\n",
    "X_val, y_val     = load_and_clean('N_val.npy',   'y_val.npy', x_col, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92613d4f-1cd2-49f8-82a9-620fda68b565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the dataframe\n",
    "num_columns = X_train.shape[1]\n",
    "\n",
    "# Calculate number of columns - 1\n",
    "columns_minus_one = num_columns - 1\n",
    "\n",
    "# Calculate the square root for image size\n",
    "image_size = math.ceil(math.sqrt(columns_minus_one))\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a28853f7-dcee-47de-91eb-8f35177b8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'puma8NH'\n",
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "pixel=20\n",
    "image_model = TINTO(problem=problem_type, blur=False, pixels=pixel, random_seed=SEED)\n",
    "name = f\"TINTO\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a01e98f2-1bea-4364-b0bc-3d0412ebba5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images are already generated\n",
      "The images are already generated\n",
      "The images are already generated\n",
      "Images shape:  (3, 20, 20)\n",
      "Attributes:  8\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape = load_and_preprocess_data(\n",
    "    X_train, y_train, X_test, y_test, X_val, y_val,\n",
    "    image_model=image_model,\n",
    "    problem_type=problem_type,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c84fd70-94ac-4546-b907-00fe548676ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine dataloaders into tensors.\n",
    "train_mlp, train_img, train_target = combine_loader(train_loader)\n",
    "val_mlp, val_img, val_target = combine_loader(val_loader)\n",
    "test_mlp, test_img, test_target = combine_loader(test_loader)\n",
    "\n",
    "dataset = {\n",
    "    \"train_input\": train_mlp.to(device),\n",
    "    \"train_img\": train_img.to(device),\n",
    "    \"train_label\": train_target.to(device),\n",
    "    \"val_input\": val_mlp.to(device),\n",
    "    \"val_img\": val_img.to(device),\n",
    "    \"val_label\": val_target.to(device),\n",
    "    \"test_input\": test_mlp.to(device),\n",
    "    \"test_img\": test_img.to(device),\n",
    "    \"test_label\": test_target.to(device),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8382d81-df3d-4828-90a0-a472f0fb312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([5242, 8])\n",
      "Train target shape: torch.Size([5242, 1])\n",
      "Test data shape: torch.Size([1639, 8])\n",
      "Test target shape: torch.Size([1639, 1])\n",
      "Validation data shape: torch.Size([1311, 8])\n",
      "Validation target shape: torch.Size([1311, 1])\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the tensors\n",
    "print(\"Train data shape:\", dataset['train_input'].shape)\n",
    "print(\"Train target shape:\", dataset['train_label'].shape)\n",
    "print(\"Test data shape:\", dataset['test_input'].shape)\n",
    "print(\"Test target shape:\", dataset['test_label'].shape)\n",
    "print(\"Validation data shape:\", dataset['val_input'].shape)\n",
    "print(\"Validation target shape:\", dataset['val_label'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea18fe23-f001-4ccd-bf3c-52b3e9d6c16b",
   "metadata": {},
   "source": [
    "# Set Files Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff8479f3-3c89-45cd-ae58-37885ee353de",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_1=f'{dataset_name}_Tinto_Concat_Op1.csv'\n",
    "filename_2=f'{dataset_name}_Tinto_Concat_Op2.csv'\n",
    "filename_3=f'{dataset_name}_Tinto_Concat_Op3.csv'\n",
    "filename_4=f'{dataset_name}_Tinto_Concat_Op4.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b6db8d2-dd7c-4092-97bb-042f79f2c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_opt1 = 'cnn_bottleneck_dim'\n",
    "columns_opt2 = 'alpha'\n",
    "columns_opt3 = 'hidden_dim'\n",
    "columns_opt4 = 'embed_dim, num_heads'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d53e6-cabe-4cfa-9679-4f67f9113dda",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Option 1: Concat KAN with (CNN with dense layer to reduce output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffd21f64-4dd4-47f3-b866-9219999d5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_csv_with_header(filename_1, columns_opt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d67dd441-f2a9-4f8c-ba34-8b62c4e12e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"------------------------------ kan_neurons=2, kan_grid=4, lamb=1e-05 ------------------------------\")\n",
    "# for cnn_bottleneck_dim in [1, 2, 3, 4, 5, 6]:\n",
    "#     print(f\"cnn_bottleneck_dim: {cnn_bottleneck_dim}\")\n",
    "#     train_and_plot_relevance(Model3_1, kan_neurons=2, kan_grid=4, lamb=1e-05, steps=50, \n",
    "#                              cnn_bottleneck_dim=cnn_bottleneck_dim, filename=filename_1, opt_col_val=cnn_bottleneck_dim)\n",
    "\n",
    "# print(\"------------------------------ kan_neurons=3, kan_grid=5, lamb=0.0001 ------------------------------\")\n",
    "# for cnn_bottleneck_dim in [1,2, 3, 4, 5, 6, 7, 9]:\n",
    "#     print(f\"cnn_bottleneck_dim: {cnn_bottleneck_dim}\")\n",
    "#     train_and_plot_relevance(Model3_1, kan_neurons=3, kan_grid=5, lamb=0.0001, steps=50, \n",
    "#                              cnn_bottleneck_dim=cnn_bottleneck_dim, filename=filename_1, opt_col_val=cnn_bottleneck_dim)\n",
    "    \n",
    "# print(\"------------------------------ kan_neurons=3, kan_grid=4, lamb=1e-05 ------------------------------\")\n",
    "# for cnn_bottleneck_dim in [1,2, 3, 4, 5, 6, 7, 9]:\n",
    "#     print(f\"cnn_bottleneck_dim: {cnn_bottleneck_dim}\")\n",
    "#     train_and_plot_relevance(Model3_1, kan_neurons=3, kan_grid=4, lamb=1e-05, steps=50, \n",
    "#                              cnn_bottleneck_dim=cnn_bottleneck_dim, filename=filename_1, opt_col_val=cnn_bottleneck_dim)\n",
    "\n",
    "\n",
    "# print(\"------------------------------ kan_neurons=4, kan_grid=4, lamb=0.01 ------------------------------\")\n",
    "# for cnn_bottleneck_dim in [1, 2, 3, 4, 6, 7, 8, 10, 12]:\n",
    "#     print(f\"cnn_bottleneck_dim: {cnn_bottleneck_dim}\")\n",
    "#     train_and_plot_relevance(Model3_1, kan_neurons=4, kan_grid=4, lamb=0.01, steps=50, \n",
    "#                              cnn_bottleneck_dim=cnn_bottleneck_dim, filename=filename_1, opt_col_val=cnn_bottleneck_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d2af7-772b-4317-a7da-4446cbaf2990",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Option 2: Multiply CNN output by factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5f97596-263b-488c-b355-d9e404f6075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_csv_with_header(filename_2, columns_opt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0b767fc-0729-48c3-a08f-a4ad366b1969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"------------------------------ kan_neurons=2, kan_grid=4, lamb=1e-05 ------------------------------\")\n",
    "# for alpha in [0.9, .8, .75, .7, .6, .5, .4, .3, .2, .1, .05, .01]:\n",
    "#     print(f\"alpha: {alpha}\")\n",
    "#     train_and_plot_relevance(Model3_2, kan_neurons=2, kan_grid=4, lamb=1e-05, steps=60, \n",
    "#                              alpha=alpha, filename=filename_2, opt_col_val=alpha)\n",
    "\n",
    "# print(\"------------------------------ kan_neurons=3, kan_grid=5, lamb=0.0001 ------------------------------\")\n",
    "# for alpha in [0.9, .8, .75, .7, .6, .5, .4, .3, .2, .1, .05, .01]:\n",
    "#     print(f\"alpha: {alpha}\")\n",
    "#     train_and_plot_relevance(Model3_2, kan_neurons=3, kan_grid=5, lamb=0.0001, steps=60, \n",
    "#                              alpha=alpha, filename=filename_2, opt_col_val=alpha)\n",
    "\n",
    "# print(\"------------------------------ kan_neurons=3, kan_grid=4, lamb=1e-05 ------------------------------\")\n",
    "# for alpha in [0.9, .8, .75, .7, .6, .5, .4, .3, .2, .1, .05, .01]:\n",
    "#     print(f\"alpha: {alpha}\")\n",
    "#     train_and_plot_relevance(Model3_2, kan_neurons=3, kan_grid=4, lamb=1e-05, steps=60, \n",
    "#                              alpha=alpha, filename=filename_2, opt_col_val=alpha)\n",
    "\n",
    "# print(\"------------------------------ kan_neurons=4, kan_grid=4, lamb=0.01 ------------------------------\")\n",
    "# for alpha in [0.9, .8, .75, .7, .6, .5, .4, .3, .2, .1, .05, .01]:\n",
    "#     print(f\"alpha: {alpha}\")\n",
    "#     train_and_plot_relevance(Model3_2, kan_neurons=4, kan_grid=4, lamb=0.01, steps=60, \n",
    "#                              alpha=alpha, filename=filename_2, opt_col_val=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840dde9a-aead-4372-8c1c-a1f0445f0ca4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Option3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f84f3b7-eac8-43b5-8b71-5f8031830736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_csv_with_header(filename_3, columns_opt3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2daf4c4e-26cc-4004-bf08-aee355cd2d84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"------------------------------ kan_neurons=2, kan_grid=4, lamb=1e-05 ------------------------------\")\n",
    "# for hidden_dim in [128, 64, 32, 16, 8]:\n",
    "#     print(f\"hidden_dim: {hidden_dim}\")\n",
    "#     train_and_plot_relevance(Model3_3, kan_neurons=2, kan_grid=4, lamb=1e-05, steps=80, \n",
    "#                              hidden_dim=hidden_dim, filename=filename_3, opt_col_val=hidden_dim)\n",
    "\n",
    "# print(\"------------------------------ kan_neurons=3, kan_grid=5, lamb=0.0001 ------------------------------\")\n",
    "# for hidden_dim in [128, 64, 32, 16, 8]:\n",
    "#     print(f\"hidden_dim: {hidden_dim}\")\n",
    "#     train_and_plot_relevance(Model3_3, kan_neurons=3, kan_grid=5, lamb=0.0001, steps=80, \n",
    "#                              hidden_dim=hidden_dim, filename=filename_3, opt_col_val=hidden_dim)\n",
    "\n",
    "# print(\"------------------------------ kan_neurons=3, kan_grid=4, lamb=1e-05 ------------------------------\")\n",
    "# for hidden_dim in [128, 64, 32, 16, 8]:\n",
    "#     print(f\"hidden_dim: {hidden_dim}\")\n",
    "#     train_and_plot_relevance(Model3_3, kan_neurons=2, kan_grid=3, lamb=0.01, steps=80, \n",
    "#                              hidden_dim=hidden_dim, filename=filename_3, opt_col_val=hidden_dim)\n",
    "\n",
    "# print(\"------------------------------ kan_neurons=4, kan_grid=4, lamb=0.01 ------------------------------\")\n",
    "# for hidden_dim in [128, 64, 32, 16, 8]:\n",
    "#     print(f\"hidden_dim: {hidden_dim}\")\n",
    "#     train_and_plot_relevance(Model3_3, kan_neurons=4, kan_grid=4, lamb=0.01, steps=80, \n",
    "#                              hidden_dim=hidden_dim, filename=filename_3, opt_col_val=hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881409f-64ec-448e-9f30-a97b74e4c475",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Opt4: MultiHead Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f51759df-e18e-4f3c-8810-e96d91a0a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_csv_with_header(filename_4, columns_opt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ef0b5c5-5f40-409e-9f1d-4e00916fc8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------ kan_neurons=2, kan_grid=4, lamb=1e-05 ------------------------------\n",
      "embed_dim: 64, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.62e+00 | eval_loss: 3.68e+00 | reg: 2.73e+01 |: 100%|█| 130/130 [01:21<00:00,  1.59i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 100\n",
      "3.775926113128662\n",
      "M_KAN Relevance: 0.014159921556711197\n",
      "CNN Relevance: 0.9858400821685791\n",
      "embed_dim: 64, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.44e+00 | eval_loss: 3.73e+00 | reg: 2.24e+01 |: 100%|█| 130/130 [01:56<00:00,  1.12i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 32\n",
      "3.732860803604126\n",
      "M_KAN Relevance: 4.021665063191904e-06\n",
      "CNN Relevance: 0.9999960064888\n",
      "embed_dim: 64, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.44e+00 | eval_loss: 3.73e+00 | reg: 2.24e+01 |: 100%|█| 130/130 [01:56<00:00,  1.11i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 32\n",
      "3.732860803604126\n",
      "M_KAN Relevance: 4.021665063191904e-06\n",
      "CNN Relevance: 0.9999960064888\n",
      "embed_dim: 32, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.45e+00 | eval_loss: 3.73e+00 | reg: 1.95e+01 |: 100%|█| 130/130 [01:51<00:00,  1.16i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 26\n",
      "3.7470808029174805\n",
      "M_KAN Relevance: 0.1178840696811676\n",
      "CNN Relevance: 0.88211590051651\n",
      "embed_dim: 32, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.45e+00 | eval_loss: 3.73e+00 | reg: 1.95e+01 |: 100%|█| 130/130 [01:51<00:00,  1.16i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 26\n",
      "3.7470808029174805\n",
      "M_KAN Relevance: 0.1178840696811676\n",
      "CNN Relevance: 0.88211590051651\n",
      "embed_dim: 32, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.45e+00 | eval_loss: 3.73e+00 | reg: 1.95e+01 |: 100%|█| 130/130 [01:51<00:00,  1.16i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 26\n",
      "3.7470808029174805\n",
      "M_KAN Relevance: 0.1178840696811676\n",
      "CNN Relevance: 0.88211590051651\n",
      "embed_dim: 16, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.49e+00 | eval_loss: 3.67e+00 | reg: 1.74e+01 |: 100%|█| 130/130 [01:54<00:00,  1.14i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 46\n",
      "3.7330265045166016\n",
      "M_KAN Relevance: 0.5126510858535767\n",
      "CNN Relevance: 0.4873489439487457\n",
      "embed_dim: 16, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.54e+00 | eval_loss: 3.63e+00 | reg: 1.59e+01 |: 100%|█| 130/130 [01:19<00:00,  1.64i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 35\n",
      "3.7410175800323486\n",
      "M_KAN Relevance: 0.6010497212409973\n",
      "CNN Relevance: 0.3989502489566803\n",
      "embed_dim: 16, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.54e+00 | eval_loss: 3.63e+00 | reg: 1.59e+01 |: 100%|█| 130/130 [01:19<00:00,  1.63i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 35\n",
      "3.7410175800323486\n",
      "M_KAN Relevance: 0.6010497212409973\n",
      "CNN Relevance: 0.3989502489566803\n",
      "embed_dim: 48, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.44e+00 | eval_loss: 3.74e+00 | reg: 2.17e+01 |: 100%|█| 130/130 [01:52<00:00,  1.15i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 25\n",
      "3.753851890563965\n",
      "M_KAN Relevance: 0.06087382510304451\n",
      "CNN Relevance: 0.939126193523407\n",
      "embed_dim: 24, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.58e+00 | eval_loss: 3.69e+00 | reg: 1.59e+01 |: 100%|█| 130/130 [01:26<00:00,  1.50i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 103\n",
      "3.7727386951446533\n",
      "M_KAN Relevance: 0.16896185278892517\n",
      "CNN Relevance: 0.8310381770133972\n",
      "embed_dim: 12, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.45e+00 | eval_loss: 3.71e+00 | reg: 1.42e+01 |: 100%|█| 130/130 [01:54<00:00,  1.13i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 17\n",
      "3.7261929512023926\n",
      "M_KAN Relevance: 0.00018197229655925184\n",
      "CNN Relevance: 0.9998180270195007\n",
      "------------------------------ kan_neurons=3, kan_grid=5, lamb=0.0001 ------------------------------\n",
      "embed_dim: 64, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.77e+00 | eval_loss: 3.85e+00 | reg: 2.06e+01 |: 100%|█| 130/130 [01:43<00:00,  1.25i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 129\n",
      "3.9172306060791016\n",
      "M_KAN Relevance: 0.24410440027713776\n",
      "CNN Relevance: 0.7558956146240234\n",
      "embed_dim: 64, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.77e+00 | eval_loss: 3.85e+00 | reg: 2.06e+01 |: 100%|█| 130/130 [01:43<00:00,  1.25i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 129\n",
      "3.9172306060791016\n",
      "M_KAN Relevance: 0.24410440027713776\n",
      "CNN Relevance: 0.7558956146240234\n",
      "embed_dim: 64, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.77e+00 | eval_loss: 3.85e+00 | reg: 2.06e+01 |: 100%|█| 130/130 [01:43<00:00,  1.25i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 129\n",
      "3.9172306060791016\n",
      "M_KAN Relevance: 0.24410440027713776\n",
      "CNN Relevance: 0.7558956146240234\n",
      "embed_dim: 32, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.49e+00 | eval_loss: 3.69e+00 | reg: 1.78e+01 |: 100%|█| 130/130 [01:52<00:00,  1.15i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 52\n",
      "3.738222122192383\n",
      "M_KAN Relevance: 0.2284696102142334\n",
      "CNN Relevance: 0.7715303897857666\n",
      "embed_dim: 32, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.49e+00 | eval_loss: 3.69e+00 | reg: 1.78e+01 |: 100%|█| 130/130 [01:53<00:00,  1.14i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 52\n",
      "3.738222122192383\n",
      "M_KAN Relevance: 0.2284696102142334\n",
      "CNN Relevance: 0.7715303897857666\n",
      "embed_dim: 32, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.49e+00 | eval_loss: 3.69e+00 | reg: 1.78e+01 |: 100%|█| 130/130 [01:53<00:00,  1.15i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 52\n",
      "3.738222122192383\n",
      "M_KAN Relevance: 0.2284696102142334\n",
      "CNN Relevance: 0.7715303897857666\n",
      "embed_dim: 16, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.54e+00 | eval_loss: 3.66e+00 | reg: 1.65e+01 |: 100%|█| 130/130 [01:47<00:00,  1.21i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 120\n",
      "3.7498912811279297\n",
      "M_KAN Relevance: 0.4190428853034973\n",
      "CNN Relevance: 0.5809570550918579\n",
      "embed_dim: 16, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.54e+00 | eval_loss: 3.66e+00 | reg: 1.65e+01 |: 100%|█| 130/130 [01:48<00:00,  1.20i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 120\n",
      "3.7498912811279297\n",
      "M_KAN Relevance: 0.4190428853034973\n",
      "CNN Relevance: 0.5809570550918579\n",
      "embed_dim: 16, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.53e+00 | eval_loss: 3.65e+00 | reg: 1.58e+01 |: 100%|█| 130/130 [01:53<00:00,  1.15i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 90\n",
      "3.757819175720215\n",
      "M_KAN Relevance: 0.40652117133140564\n",
      "CNN Relevance: 0.5934788584709167\n",
      "embed_dim: 48, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.52e+00 | eval_loss: 3.65e+00 | reg: 2.04e+01 |: 100%|█| 130/130 [01:51<00:00,  1.16i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 111\n",
      "3.7439422607421875\n",
      "M_KAN Relevance: 0.14022360742092133\n",
      "CNN Relevance: 0.8597764372825623\n",
      "embed_dim: 24, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.91e+00 | eval_loss: 3.99e+00 | reg: 2.80e+01 |: 100%|█| 130/130 [01:11<00:00,  1.83i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 104\n",
      "4.021620750427246\n",
      "M_KAN Relevance: 0.09087295085191727\n",
      "CNN Relevance: 0.9091270565986633\n",
      "embed_dim: 12, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.43e+00 | eval_loss: 3.73e+00 | reg: 1.63e+01 |: 100%|█| 130/130 [01:52<00:00,  1.16i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 56\n",
      "3.7586207389831543\n",
      "M_KAN Relevance: 0.06090916320681572\n",
      "CNN Relevance: 0.9390907883644104\n",
      "------------------------------ kan_neurons=3, kan_grid=4, lamb=1e-05 ------------------------------\n",
      "embed_dim: 64, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.55e+00 | eval_loss: 3.63e+00 | reg: 3.04e+01 |: 100%|█| 130/130 [01:22<00:00,  1.58i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 100\n",
      "3.725766658782959\n",
      "M_KAN Relevance: 0.028219910338521004\n",
      "CNN Relevance: 0.9717801213264465\n",
      "embed_dim: 64, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.55e+00 | eval_loss: 3.63e+00 | reg: 3.04e+01 |: 100%|█| 130/130 [01:22<00:00,  1.58i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 100\n",
      "3.725766658782959\n",
      "M_KAN Relevance: 0.028219910338521004\n",
      "CNN Relevance: 0.9717801213264465\n",
      "embed_dim: 64, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.55e+00 | eval_loss: 3.63e+00 | reg: 3.04e+01 |: 100%|█| 130/130 [01:22<00:00,  1.57i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 100\n",
      "3.725766658782959\n",
      "M_KAN Relevance: 0.028219910338521004\n",
      "CNN Relevance: 0.9717801213264465\n",
      "embed_dim: 32, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.45e+00 | eval_loss: 3.71e+00 | reg: 2.02e+01 |: 100%|█| 130/130 [01:52<00:00,  1.15i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 28\n",
      "3.7482035160064697\n",
      "M_KAN Relevance: 0.30607855319976807\n",
      "CNN Relevance: 0.6939213871955872\n",
      "embed_dim: 32, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.45e+00 | eval_loss: 3.71e+00 | reg: 2.02e+01 |: 100%|█| 130/130 [01:52<00:00,  1.15i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 28\n",
      "3.7482035160064697\n",
      "M_KAN Relevance: 0.30607855319976807\n",
      "CNN Relevance: 0.6939213871955872\n",
      "embed_dim: 32, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.45e+00 | eval_loss: 3.71e+00 | reg: 2.02e+01 |: 100%|█| 130/130 [01:52<00:00,  1.15i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 28\n",
      "3.7482035160064697\n",
      "M_KAN Relevance: 0.30607855319976807\n",
      "CNN Relevance: 0.6939213871955872\n",
      "embed_dim: 16, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.52e+00 | eval_loss: 3.64e+00 | reg: 1.93e+01 |: 100%|█| 130/130 [01:44<00:00,  1.25i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 86\n",
      "3.7143940925598145\n",
      "M_KAN Relevance: 0.13543155789375305\n",
      "CNN Relevance: 0.8645684719085693\n",
      "embed_dim: 16, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.52e+00 | eval_loss: 3.64e+00 | reg: 1.93e+01 |: 100%|█| 130/130 [01:44<00:00,  1.24i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 86\n",
      "3.7143940925598145\n",
      "M_KAN Relevance: 0.13543155789375305\n",
      "CNN Relevance: 0.8645684719085693\n",
      "embed_dim: 16, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.52e+00 | eval_loss: 3.64e+00 | reg: 1.93e+01 |: 100%|█| 130/130 [01:45<00:00,  1.23i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 86\n",
      "3.7143940925598145\n",
      "M_KAN Relevance: 0.13543155789375305\n",
      "CNN Relevance: 0.8645684719085693\n",
      "embed_dim: 48, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.54e+00 | eval_loss: 3.64e+00 | reg: 2.57e+01 |: 100%|█| 130/130 [01:32<00:00,  1.41i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 113\n",
      "3.7316043376922607\n",
      "M_KAN Relevance: 0.006798543967306614\n",
      "CNN Relevance: 0.9932014346122742\n",
      "embed_dim: 24, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.57e+00 | eval_loss: 3.64e+00 | reg: 2.59e+01 |: 100%|█| 130/130 [01:39<00:00,  1.30i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 93\n",
      "3.7372970581054688\n",
      "M_KAN Relevance: 0.3834154009819031\n",
      "CNN Relevance: 0.6165845990180969\n",
      "embed_dim: 12, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.39e+00 | eval_loss: 3.82e+00 | reg: 1.51e+01 |: 100%|█| 130/130 [01:51<00:00,  1.17i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 19\n",
      "3.7562928199768066\n",
      "M_KAN Relevance: 0.3765682578086853\n",
      "CNN Relevance: 0.6234317421913147\n",
      "------------------------------ kan_neurons=4, kan_grid=4, lamb=0.01 ------------------------------\n",
      "embed_dim: 64, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.50e+00 | eval_loss: 3.66e+00 | reg: 1.12e+01 |: 100%|█| 130/130 [01:59<00:00,  1.09i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 56\n",
      "3.7181315422058105\n",
      "M_KAN Relevance: 1.3386869795795064e-05\n",
      "CNN Relevance: 0.9999866485595703\n",
      "embed_dim: 64, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.50e+00 | eval_loss: 3.66e+00 | reg: 1.12e+01 |: 100%|█| 130/130 [01:58<00:00,  1.09i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 56\n",
      "3.7181315422058105\n",
      "M_KAN Relevance: 1.3386869795795064e-05\n",
      "CNN Relevance: 0.9999866485595703\n",
      "embed_dim: 64, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.50e+00 | eval_loss: 3.66e+00 | reg: 1.12e+01 |: 100%|█| 130/130 [01:59<00:00,  1.09i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 56\n",
      "3.7181315422058105\n",
      "M_KAN Relevance: 1.3386869795795064e-05\n",
      "CNN Relevance: 0.9999866485595703\n",
      "embed_dim: 32, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.47e+00 | eval_loss: 3.71e+00 | reg: 8.48e+00 |: 100%|█| 130/130 [01:53<00:00,  1.14i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 35\n",
      "3.739335298538208\n",
      "M_KAN Relevance: 0.02773342654109001\n",
      "CNN Relevance: 0.9722665548324585\n",
      "embed_dim: 32, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.47e+00 | eval_loss: 3.71e+00 | reg: 8.48e+00 |: 100%|█| 130/130 [01:53<00:00,  1.14i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 35\n",
      "3.739335298538208\n",
      "M_KAN Relevance: 0.02773342654109001\n",
      "CNN Relevance: 0.9722665548324585\n",
      "embed_dim: 32, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.39e+00 | eval_loss: 3.79e+00 | reg: 9.22e+00 |: 100%|█| 130/130 [01:50<00:00,  1.17i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 43\n",
      "3.7501659393310547\n",
      "M_KAN Relevance: 0.0026824232190847397\n",
      "CNN Relevance: 0.9973176121711731\n",
      "embed_dim: 16, num_head:2\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.42e+00 | eval_loss: 3.74e+00 | reg: 8.34e+00 |: 100%|█| 130/130 [01:54<00:00,  1.14i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 16\n",
      "3.7220332622528076\n",
      "M_KAN Relevance: 0.2069840133190155\n",
      "CNN Relevance: 0.7930159568786621\n",
      "embed_dim: 16, num_head:4\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.42e+00 | eval_loss: 3.74e+00 | reg: 8.34e+00 |: 100%|█| 130/130 [01:54<00:00,  1.14i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 16\n",
      "3.7220332622528076\n",
      "M_KAN Relevance: 0.2069840133190155\n",
      "CNN Relevance: 0.7930159568786621\n",
      "embed_dim: 16, num_head:8\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.42e+00 | eval_loss: 3.74e+00 | reg: 8.34e+00 |: 100%|█| 130/130 [01:54<00:00,  1.13i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 16\n",
      "3.7220332622528076\n",
      "M_KAN Relevance: 0.2069840133190155\n",
      "CNN Relevance: 0.7930159568786621\n",
      "embed_dim: 48, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.58e+00 | eval_loss: 3.63e+00 | reg: 2.09e+01 |: 100%|█| 130/130 [01:28<00:00,  1.47i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 129\n",
      "3.7480971813201904\n",
      "M_KAN Relevance: 0.014722379855811596\n",
      "CNN Relevance: 0.9852776527404785\n",
      "embed_dim: 24, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.39e+00 | eval_loss: 3.79e+00 | reg: 1.03e+01 |: 100%|█| 130/130 [01:50<00:00,  1.18i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 27\n",
      "3.7361900806427\n",
      "M_KAN Relevance: 0.10823393613100052\n",
      "CNN Relevance: 0.8917660713195801\n",
      "embed_dim: 12, num_head:6\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "Successfully created and tested Model3_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 3.45e+00 | eval_loss: 3.72e+00 | reg: 7.03e+00 |: 100%|█| 130/130 [01:52<00:00,  1.16i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "Best epoch 87\n",
      "3.763381004333496\n",
      "M_KAN Relevance: 0.9218504428863525\n",
      "CNN Relevance: 0.07814953476190567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------------------ kan_neurons=2, kan_grid=4, lamb=1e-05 ------------------------------\")\n",
    "for embed_dim in [64, 32, 16]:\n",
    "    for num_head in [2, 4 , 8]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model3_4, kan_neurons=2, kan_grid=4, lamb=1e-05, steps=130, \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "\n",
    "for embed_dim in [48, 24, 12]:\n",
    "    for num_head in [6]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model3_4, kan_neurons=2, kan_grid=4, lamb=1e-05, steps=130, \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "\n",
    "print(\"------------------------------ kan_neurons=3, kan_grid=5, lamb=0.0001 ------------------------------\")\n",
    "for embed_dim in [64, 32, 16]:\n",
    "    for num_head in [2, 4 , 8]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model3_4, kan_neurons=3, kan_grid=5, lamb=0.0001, steps=130, \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "\n",
    "for embed_dim in [48, 24, 12]:\n",
    "    for num_head in [6]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model3_4, kan_neurons=3, kan_grid=5, lamb=0.0001, steps=130, \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "\n",
    "\n",
    "print(\"------------------------------ kan_neurons=3, kan_grid=4, lamb=1e-05 ------------------------------\")\n",
    "for embed_dim in [64, 32, 16]:\n",
    "    for num_head in [2, 4 , 8]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model3_4, kan_neurons=3, kan_grid=4, lamb=1e-05, steps=130, \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "for embed_dim in [48, 24, 12]:\n",
    "    for num_head in [6]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model3_4, kan_neurons=3, kan_grid=4, lamb=1e-05, steps=130, \n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "\n",
    "\n",
    "\n",
    "print(\"------------------------------ kan_neurons=4, kan_grid=4, lamb=0.01 ------------------------------\")\n",
    "for embed_dim in [64, 32, 16]:\n",
    "    for num_head in [2, 4 , 8]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model3_4, kan_neurons=4, kan_grid=4, lamb=0.01, steps=130,\n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n",
    "\n",
    "for embed_dim in [48, 24, 12]:\n",
    "    for num_head in [6]:\n",
    "        print(f\"embed_dim: {embed_dim}, num_head:{num_head}\")\n",
    "        train_and_plot_relevance(Model3_4, kan_neurons=4, kan_grid=4, lamb=0.01, steps=130,\n",
    "                                 embed_dim=embed_dim, num_heads=num_head, filename=filename_4, opt_col_val=f'{embed_dim}, {num_head}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9857c5-e077-4d56-902b-aedba3b16cb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
